#!/usr/bin/env python3
"""
Ethical Crawlers ì™„ì „ ë°ëª¨ - ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë“œì˜ ëª¨ë“  ë‚´ìš© êµ¬í˜„
- Respect robots.txt
- Never degrade website's performance
- Identify creator with contact information
- Don't cause pain for system administrator
"""
import scrapy
from scrapy.crawler import CrawlerProcess
import requests
import subprocess
import os


def demo_robots_txt():
    """robots.txt ë°ëª¨"""
    print("ğŸ¤– robots.txt ë°ëª¨")
    print("=" * 50)

    print("ğŸ“‹ robots.txtì˜ 5ê°€ì§€ ì£¼ìš” ìš©ì–´:")
    print("â€¢ User-agent: ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì§€ì •")
    print("â€¢ Disallow: í¬ë¡¤ë§ ê¸ˆì§€ URL")
    print("â€¢ Allow: í¬ë¡¤ë§ í—ˆìš© URL")
    print("â€¢ Crawl-delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„")
    print("â€¢ Sitemap: ì‚¬ì´íŠ¸ë§µ ìœ„ì¹˜")
    print()

    print("ğŸŒ ì‹¤ì œ robots.txt í™•ì¸:")

    # ì‹¤ì œ robots.txt í™•ì¸
    sites = [
        "https://www.naver.com/robots.txt",
        "https://www.google.com/robots.txt",
        "https://quotes.toscrape.com/robots.txt",
    ]

    for site in sites:
        try:
            response = requests.get(site, timeout=5)
            if response.status_code == 200:
                print(f"âœ… {site}: ì ‘ê·¼ ê°€ëŠ¥")
                lines = response.text.split("\\n")[:10]  # ì²˜ìŒ 10ì¤„ë§Œ
                for line in lines:
                    if line.strip():
                        print(f"   {line}")
                print("   ...")
            else:
                print(f"âŒ {site}: HTTP {response.status_code}")
        except Exception as e:
            print(f"âŒ {site}: ì ‘ê·¼ ì‹¤íŒ¨ - {e}")
        print()


def demo_scrapy_settings():
    """Scrapy ìœ¤ë¦¬ì  ì„¤ì • ë°ëª¨"""
    print("âš™ï¸ Scrapy ìœ¤ë¦¬ì  ì„¤ì •")
    print("=" * 50)

    print("ğŸ“‹ settings.pyì˜ ì£¼ìš” ìœ¤ë¦¬ì  ì„¤ì •ë“¤:")
    print()

    print("1ï¸âƒ£ robots.txt ì¤€ìˆ˜:")
    print("   ROBOTSTXT_OBEY = True")
    print("   HTTPCACHE_ENABLED = True")
    print()

    print("2ï¸âƒ£ í¬ë¡¤ëŸ¬ ì‹ ì› í™•ì¸:")
    print("   USER_AGENT = 'MyCompany-MyCrawler (bot@mycompany.com)'")
    print()

    print("3ï¸âƒ£ ì›¹ì‚¬ì´íŠ¸ ì„±ëŠ¥ ë³´í˜¸:")
    print("   DOWNLOAD_DELAY = 5.0  # ê¸°ë³¸ê°’ì€ 0")
    print("   # ScrapyëŠ” [0.5 ~ 1.5] x DOWNLOAD_DELAY ë²”ìœ„ì˜ ëœë¤ ì§€ì—° ì‚¬ìš©")
    print("   # ì •í™•í•œ ì§€ì—° ì‹œê°„ ì‚¬ìš©í•˜ë ¤ë©´ RANDOMIZE_DOWNLOAD_DELAY ë¹„í™œì„±í™”")
    print()

    print("4ï¸âƒ£ ë™ì‹œ ìš”ì²­ ì œí•œ:")
    print("   CONCURRENT_REQUESTS_PER_DOMAIN = 1")
    print("   # ëª¨ë“  ìŠ¤íŒŒì´ë”ì˜ ë™ì‹œ ìš”ì²­ ìµœëŒ€ ê°œìˆ˜ ì„¤ì •")
    print()

    print("5ï¸âƒ£ AutoThrottleë¡œ ì§€ì—° ì‹œê°„ ìë™ ì¡°ì ˆ:")
    print("   AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0")
    print("   # ìš”ì²­ ê°„ ì§€ì—°ì„ ìë™ìœ¼ë¡œ ì¡°ì ˆ")


def demo_scrapy_shell_robotstxt():
    """Scrapy Shellì—ì„œ robots.txt í™•ì¸"""
    print("\nğŸš Scrapy Shellì—ì„œ robots.txt í™•ì¸")
    print("=" * 50)

    print("ğŸ“‹ ì´ë¯¸ì§€ì—ì„œ ë³´ì—¬ì¤€ ì‹œë‚˜ë¦¬ì˜¤:")
    print("â€¢ scrapy shell https://finance.naver.com/marketindex/")
    print("â€¢ By default, robots.txt blocks scraping!")
    print()

    print("ğŸ” ì‘ë‹µ í™•ì¸:")
    print("â€¢ response == None  (robots.txtì— ì˜í•´ ì°¨ë‹¨ë¨)")
    print("â€¢ 'Forbidden by robots.txt' ë©”ì‹œì§€ ì¶œë ¥")
    print()

    print("ğŸ’¡ í•´ê²° ë°©ë²•:")
    print("1ï¸âƒ£ í”„ë¡œì íŠ¸ ì „ì²´ ì„¤ì •:")
    print("   Set 'ROBOTSTXT_OBEY = True' at the settings.py")
    print()

    print("2ï¸âƒ£ Shell ì„¸ì…˜ë³„ ì„¤ì •:")
    print(
        "   scrapy shell https://finance.naver.com/marketindex/ -s ROBOTSTXT_OBEY=False"
    )
    print("   â† set additional option for this shell session!")
    print()

    print("3ï¸âƒ£ ìŠ¤íŒŒì´ë”ë³„ ì„¤ì •:")
    print("   scrapy crawl -s ROBOTSTXT_OBEY=False spider_name")
    print("   â† for this spider!")


def create_ethical_spider():
    """ìœ¤ë¦¬ì  í¬ë¡¤ë§ ìŠ¤íŒŒì´ë” ìƒì„±"""
    print("\nğŸ•·ï¸ ìœ¤ë¦¬ì  í¬ë¡¤ë§ ìŠ¤íŒŒì´ë” ì˜ˆì œ")
    print("=" * 50)

    spider_code = """
import scrapy

class EthicalSpider(scrapy.Spider):
    name = 'ethical_crawler'

    # ìœ¤ë¦¬ì  í¬ë¡¤ë§ ì„¤ì •
    custom_settings = {
        'ROBOTSTXT_OBEY': True,
        'USER_AGENT': 'MyCompany-EthicalCrawler (contact@mycompany.com)',
        'DOWNLOAD_DELAY': 3.0,
        'RANDOMIZE_DOWNLOAD_DELAY': True,  # [0.5*3 ~ 1.5*3] ì´ˆ ì‚¬ì´
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 60,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'HTTPCACHE_ENABLED': True,
    }

    start_urls = ['http://quotes.toscrape.com']

    def parse(self, response):
        self.logger.info(f"ìœ¤ë¦¬ì ìœ¼ë¡œ í¬ë¡¤ë§ ì¤‘: {response.url}")

        # robots.txt í™•ì¸
        if hasattr(response, 'meta') and response.meta.get('download_timeout'):
            self.logger.warning("ìš”ì²­ ì‹œê°„ ì´ˆê³¼ - ì„œë²„ ë¶€í•˜ë¥¼ ê³ ë ¤í•˜ì—¬ ëŒ€ê¸°")

        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }

        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™ (ì‹ ì¤‘í•˜ê²Œ)
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
"""

    print("ğŸ“„ ìœ¤ë¦¬ì  ìŠ¤íŒŒì´ë” ì½”ë“œ:")
    print(spider_code)

    # íŒŒì¼ë¡œ ì €ì¥
    with open(
        "/Users/juns/bigdata/tutorial/tutorial/spiders/ethical_spider.py",
        "w",
        encoding="utf-8",
    ) as f:
        f.write(spider_code.strip())

    print("ğŸ’¾ íŒŒì¼ ì €ì¥: tutorial/tutorial/spiders/ethical_spider.py")


if __name__ == "__main__":
    print("ğŸ¯ Ethical Crawlers ì™„ì „ ë°ëª¨")
    print("=" * 80)
    print("Web crawlers: an essential component for many web applications")
    print("â€¢ search engine, digital library, online marketing, web data mining, ...")
    print("â€¢ crawlers are highly automated & seldom regulated manually")
    print("â€¢ crawler-generated visits significantly increase site traffic")
    print("  and can affect log statistics to overestimate real user traffic")
    print()

    print("ğŸ­ Regular (Polite) Crawlers vs. Rogue (Nefarious) Crawlers")
    print("â€¢ regular crawl of web pages for general purpose indexing")
    print("â€¢ extraction of email & personal ID information, service attack")
    print()

    # 1. robots.txt ë°ëª¨
    demo_robots_txt()

    # 2. Scrapy ì„¤ì • ë°ëª¨
    demo_scrapy_settings()

    # 3. Scrapy Shell ë°ëª¨
    demo_scrapy_shell_robotstxt()

    # 4. ìœ¤ë¦¬ì  ìŠ¤íŒŒì´ë” ìƒì„±
    create_ethical_spider()

    print("\nğŸ¯ Crawling the Web Politely! 4ê°€ì§€ ì›ì¹™:")
    print("1ï¸âƒ£ Respect robots.txt - Follow Allows and Disallows")
    print("2ï¸âƒ£ Never degrade a website's performance - Follow Crawl-delay")
    print(
        "3ï¸âƒ£ Identify its creator with contact information - Use request's User-agent header"
    )
    print("4ï¸âƒ£ Don't cause pain for system administrator")
    print()

    print("âœ… Ethical Crawlers ë°ëª¨ ì™„ë£Œ!")
    print("ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” ìœ¤ë¦¬ì ì´ê³  ì •ì¤‘í•˜ê²Œ ì‘ë™í•´ì•¼ í•©ë‹ˆë‹¤! ğŸ¤")
