#!/usr/bin/env python3
"""
Scrapyì˜ DUPEFILTER_CLASS ì¤‘ë³µ í•„í„° ê¸°ëŠ¥ ë°ëª¨
ê¸°ë³¸ì ìœ¼ë¡œ ScrapyëŠ” ì´ë¯¸ ë°©ë¬¸í•œ URLì— ëŒ€í•œ ìš”ì²­ì„ í•„í„°ë§í•©ë‹ˆë‹¤.
"""
import scrapy
from scrapy.crawler import CrawlerProcess


class DupeFilterTestSpider(scrapy.Spider):
    name = "dupefilter_test"
    start_urls = [
        "http://quotes.toscrape.com/page/1/",
        "http://quotes.toscrape.com/page/1/",  # ì¤‘ë³µ URL
        "http://quotes.toscrape.com/page/2/",
        "http://quotes.toscrape.com/page/1/",  # ë˜ ë‹¤ë¥¸ ì¤‘ë³µ URL
    ]

    def parse(self, response):
        print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘ì¸ í˜ì´ì§€: {response.url}")

        quotes = response.css("div.quote")
        print(f"ğŸ” ë°œê²¬ëœ ëª…ì–¸ ìˆ˜: {len(quotes)}ê°œ")

        for q in quotes:
            yield {
                "text": q.css("span.text::text").get(),
                "author": q.css("small.author::text").get(),
                "tags": q.css("div.tags a.tag::text").getall(),
            }

        # ë™ì¼í•œ í˜ì´ì§€ì— ëŒ€í•œ ì¤‘ë³µ ìš”ì²­ ìƒì„±
        next_page = response.css("li.next a::attr(href)").get()
        if next_page:
            print(f"â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™: {next_page}")
            yield response.follow(next_page, callback=self.parse)

            # ì¤‘ë³µ ìš”ì²­ ì‹œë„ (í•„í„°ë§ë  ì˜ˆì •)
            yield response.follow(next_page, callback=self.parse)
            yield response.follow(next_page, callback=self.parse)


def test_with_dupefilter():
    """ì¤‘ë³µ í•„í„° í™œì„±í™”ëœ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ì¤‘ë³µ í•„í„° í™œì„±í™” í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    process = CrawlerProcess(
        {
            "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "ROBOTSTXT_OBEY": False,
            "DOWNLOAD_DELAY": 0.5,
            "DUPEFILTER_DEBUG": True,  # ì¤‘ë³µ í•„í„° ë””ë²„ê·¸ ë¡œê·¸ í™œì„±í™”
            "LOG_LEVEL": "INFO",
            "FEEDS": {
                "dupefilter_with_filter.json": {
                    "format": "json",
                    "encoding": "utf8",
                    "overwrite": True,
                },
            },
        }
    )
    process.crawl(DupeFilterTestSpider)
    process.start()


def test_without_dupefilter():
    """ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™”ëœ ìƒíƒœë¡œ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    process = CrawlerProcess(
        {
            "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "ROBOTSTXT_OBEY": False,
            "DOWNLOAD_DELAY": 0.5,
            "DUPEFILTER_CLASS": "scrapy.dupefilters.BaseDupeFilter",  # ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™”
            "LOG_LEVEL": "INFO",
            "FEEDS": {
                "dupefilter_without_filter.json": {
                    "format": "json",
                    "encoding": "utf8",
                    "overwrite": True,
                },
            },
        }
    )
    process.crawl(DupeFilterTestSpider)
    process.start()


if __name__ == "__main__":
    print("ğŸ“‹ Scrapy DUPEFILTER_CLASS ë°ëª¨")
    print("ğŸ“š ì¤‘ë³µ URL ìš”ì²­ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”")
    print()
    print("1. ì¤‘ë³µ í•„í„° í™œì„±í™” (ê¸°ë³¸ê°’)")
    print("2. ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™”")

    choice = input("\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()

    if choice == "1":
        test_with_dupefilter()
    elif choice == "2":
        test_without_dupefilter()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
