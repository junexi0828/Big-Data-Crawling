# ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ ë° ë¡œê·¸ì¸ ì²˜ë¦¬ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ë™ì  ì½˜í…ì¸  ì²˜ë¦¬](#ë™ì -ì½˜í…ì¸ -ì²˜ë¦¬)
2. [ë³µì¡í•œ HTTP ìš”ì²­](#ë³µì¡í•œ-http-ìš”ì²­)
3. [ë¡œê·¸ì¸ ì²˜ë¦¬](#ë¡œê·¸ì¸-ì²˜ë¦¬)
4. [ì‹¤ìŠµ ì˜ˆì œ](#ì‹¤ìŠµ-ì˜ˆì œ)
5. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ğŸŒ ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

### ê°œìš”

í˜„ëŒ€ ì›¹ì‚¬ì´íŠ¸ëŠ” JavaScriptë¥¼ í†µí•´ ì½˜í…ì¸ ë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. ScrapyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ JavaScriptë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ ì ‘ê·¼ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤.

### ì „ëµ 1: ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ í™œìš©

#### Network ë„êµ¬ ì‚¬ìš©ë²•

1. **ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ ì—´ê¸°** (F12)
2. **Network íƒ­ ì„ íƒ**
3. **Preserve log ì˜µì…˜ í™œì„±í™”** - ì´ì „ ë°©ë¬¸ ë¡œê·¸ ìœ ì§€
4. **í˜ì´ì§€ ìŠ¤í¬ë¡¤/ìƒí˜¸ì‘ìš©** - ë™ì  ë¡œë”© íŠ¸ë¦¬ê±°
5. **XHR/Fetch í•„í„°** - API ìš”ì²­ í™•ì¸

#### ì£¼ìš” ë°œê²¬ ì‚¬í•­

```javascript
// ì˜ˆ: ë¬´í•œ ìŠ¤í¬ë¡¤ ì›¹ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸
http://quotes.toscrape.com/api/quotes?page=1
// ì‘ë‹µ: JSON í˜•ì‹
{
  "has_next": true,
  "page": 1,
  "quotes": [...]
}
```

### ì „ëµ 2: JSON API í™œìš©

#### ScrollableSpider ì˜ˆì œ

```python
import scrapy
import json

class ScrollableSpider(scrapy.Spider):
    name = "scrollable_spider"
    allowed_domains = ["quotes.toscrape.com"]
    page = 1
    start_urls = ["http://quotes.toscrape.com/api/quotes?page=1"]

    def parse(self, response):
        # JSON ì‘ë‹µì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        data = json.loads(response.text)

        # quotes ë°ì´í„°ì—ì„œ ê° quote ì¶”ì¶œ
        for quote in data["quotes"]:
            yield {
                "quote": quote["text"],
                "author": quote["author"]["name"],
                "tags": quote["tags"],
            }

        # ë‹¤ìŒ í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
        if data["has_next"]:
            self.page += 1
            next_page = f"http://quotes.toscrape.com/api/quotes?page={self.page}"
            yield scrapy.Request(url=next_page, callback=self.parse)
```

#### ì‹¤í–‰

```bash
scrapy crawl scrollable_spider -O dynamicquote.jl
```

#### ê²°ê³¼

- âœ… 100ê°œ quotes ìˆ˜ì§‘
- âœ… ìë™ í˜ì´ì§€ë„¤ì´ì…˜
- âœ… HTML íŒŒì‹± ë¶ˆí•„ìš”

---

## ğŸ”„ ë³µì¡í•œ HTTP ìš”ì²­

### cURL to Scrapy Request ë³€í™˜

ë§ì€ ì›¹ì‚¬ì´íŠ¸ëŠ” ë³µì¡í•œ í—¤ë”ë‚˜ ì¿ í‚¤ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤. cURL ëª…ë ¹ì–´ë¥¼ Scrapy Requestë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ë°©ë²• 1: Request.from_curl()

```python
import scrapy

class ComplexRequestSpider(scrapy.Spider):
    name = "complex_request_spider"

    def start_requests(self):
        curl_command = """
        curl 'http://quotes.toscrape.com/api/quotes?page=1' \
        -H 'User-Agent: Mozilla/5.0' \
        -H 'Accept: application/json' \
        -H 'Referer: http://quotes.toscrape.com/scroll'
        """

        request = scrapy.Request.from_curl(
            curl_command.strip(),
            callback=self.parse
        )
        yield request

    def parse(self, response):
        import json
        data = json.loads(response.text)
        # ë°ì´í„° ì²˜ë¦¬...
```

### ë°©ë²• 2: curl_to_request_kwargs()

```python
from scrapy.utils.curl import curl_to_request_kwargs

curl_kwargs = curl_to_request_kwargs(
    curl_command.strip(),
    ignore_unknown_options=True
)
yield scrapy.Request(**curl_kwargs, callback=self.parse)
```

### ì‹¤í–‰ ê²°ê³¼

```bash
scrapy crawl complex_request_spider -O complex_requests_output.jl
```

- âœ… ë³µì¡í•œ í—¤ë” ìë™ ì²˜ë¦¬
- âœ… cURL ëª…ë ¹ì–´ ì¬ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ” ë¡œê·¸ì¸ ì²˜ë¦¬

### CSRF (Cross-Site Request Forgery) ì´í•´

#### CSRF ê³µê²©ì´ë€?

- ê³µê²©ìê°€ í”¼í•´ìë¡œ í•˜ì—¬ê¸ˆ ì˜ë„í•˜ì§€ ì•Šì€ í–‰ë™ì„ í•˜ê²Œ ë§Œë“œëŠ” ê³µê²©
- ë¡œê·¸ì¸ëœ ì„¸ì…˜ì„ ì•…ìš©í•˜ì—¬ ì•…ì˜ì ì¸ ìš”ì²­ ì „ì†¡

#### CSRF ë°©ì–´ ë©”ì»¤ë‹ˆì¦˜

1. **CSRF í† í°**: ê° í¼ì— ê³ ìœ í•œ í† í° í¬í•¨
2. **Referer/Origin ì²´í¬**: ìš”ì²­ ì¶œì²˜ í™•ì¸
3. **SameSite ì¿ í‚¤**: ì¿ í‚¤ ì „ì†¡ ì œí•œ
4. **CAPTCHA**: ë´‡ ë°©ì§€

### ë¡œê·¸ì¸ ì²˜ë¦¬ ë°©ë²•

#### 1-Pass ë¡œê·¸ì¸ (ê°„ë‹¨í•œ ë°©ë²•)

**ì‚¬ìš© ì‹œê¸°**: Hidden ë°ì´í„°ê°€ ë³µì¡í•˜ì§€ ì•Šì€ ê²½ìš°

```python
import scrapy
from scrapy.http import FormRequest

class SimpleLoginSpider(scrapy.Spider):
    name = "simple_login"
    login_url = "http://quotes.toscrape.com/login"

    custom_settings = {
        "DUPEFILTER_CLASS": "scrapy.dupefilters.BaseDupeFilter",
        "COOKIES_ENABLED": True,  # ì¤‘ìš”!
    }

    def start_requests(self):
        # ì§ì ‘ ë¡œê·¸ì¸ í¼ ë°ì´í„° ì œì¶œ
        return [
            FormRequest(
                self.login_url,
                formdata={
                    "username": "user",
                    "password": "secret"
                },
                callback=self.parse,
            )
        ]

    def parse(self, response):
        # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
        if "Logout" in response.text:
            self.logger.info("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            # ë°ì´í„° ìˆ˜ì§‘...
```

#### 2-Pass ë¡œê·¸ì¸ (ë³µì¡í•œ ë°©ë²•)

**ì‚¬ìš© ì‹œê¸°**: CSRF í† í° ë“± Hidden ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°

```python
import scrapy
from scrapy.http import Request, FormRequest

class ComplexLoginSpider(scrapy.Spider):
    name = "complex_login"
    login_url = "http://quotes.toscrape.com/login"

    custom_settings = {
        "DUPEFILTER_CLASS": "scrapy.dupefilters.BaseDupeFilter",
        "COOKIES_ENABLED": True,
    }

    def start_requests(self):
        # 1ë‹¨ê³„: ë¡œê·¸ì¸ í˜ì´ì§€ ìš”ì²­
        return [Request(self.login_url, callback=self.process_login)]

    def process_login(self, response):
        # 2ë‹¨ê³„: CSRF í† í° ì¶”ì¶œ
        csrf_token = response.xpath(
            '//input[@name="csrf_token"]/@value'
        ).extract_first()

        self.logger.info(f"ğŸ”‘ CSRF í† í°: {csrf_token}")

        # FormRequest.from_responseë¡œ ë¡œê·¸ì¸ í¼ ì œì¶œ
        yield FormRequest.from_response(
            response,
            formdata={
                "csrf_token": csrf_token,
                "username": "user",
                "password": "secret",
            },
            callback=self.parse,
        )

    def parse(self, response):
        # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
        if "Logout" in response.text:
            self.logger.info("âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
            # ë°ì´í„° ìˆ˜ì§‘...
```

### ì‹¤í–‰ ë° ê²°ê³¼

```bash
# ê°„ë‹¨í•œ ë¡œê·¸ì¸
scrapy crawl simple_login -O simple_login_output.jl

# ë³µì¡í•œ ë¡œê·¸ì¸
scrapy crawl complex_login -O complex_login_output.jl
```

**ê²°ê³¼ ë¹„êµ**

| ë°©ì‹   | ìš”ì²­ ìˆ˜ | ì¥ì            | ë‹¨ì                 |
| ------ | ------- | -------------- | ------------------- |
| 1-Pass | 3       | ë¹ ë¦„, ë‹¨ìˆœ     | CSRF í† í° ì²˜ë¦¬ ë¶ˆê°€ |
| 2-Pass | 4       | CSRF í† í° ì²˜ë¦¬ | ì¶”ê°€ ìš”ì²­ í•„ìš”      |

---

## ğŸ¯ ì‹¤ìŠµ ì˜ˆì œ

### ItemLoaderì™€ í•¨ê»˜ ì‚¬ìš©í•˜ê¸°

```python
from scrapy.loader import ItemLoader
from tutorial.items import QuotesItem

class LoginQuotesSpider(scrapy.Spider):
    name = "login_quotes"

    def parse(self, response):
        for q in response.css("div.quote"):
            loader = ItemLoader(item=QuotesItem(), selector=q)
            loader.add_css("quote_content", "span.text::text")
            loader.add_css("author_name", "small.author::text")
            loader.add_css("tags", "div.tags a.tag::text")

            yield loader.load_item()

        # ë‹¤ìŒ í˜ì´ì§€
        next_page = response.css("li.next a::attr(href)").get()
        if next_page:
            yield response.follow(next_page, callback=self.parse)
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### 1. ë¡œê·¸ì¸ì´ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°

#### ë¬¸ì œ: "Logout" ë§í¬ê°€ ë³´ì´ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•**:

```python
# ì¿ í‚¤ í™œì„±í™” í™•ì¸
custom_settings = {
    "COOKIES_ENABLED": True,  # ë°˜ë“œì‹œ True!
}

# ë¡œê·¸ì¸ ì‘ë‹µ ë””ë²„ê¹…
def after_login(self, response):
    self.logger.info(f"ì‘ë‹µ URL: {response.url}")
    self.logger.info(f"ì‘ë‹µ í…ìŠ¤íŠ¸ ì¼ë¶€: {response.text[:200]}")
```

#### ë¬¸ì œ: CSRF í† í° ì˜¤ë¥˜

**í•´ê²° ë°©ë²•**:

```python
# XPath ì„ íƒì í™•ì¸
csrf_token = response.xpath('//input[@name="csrf_token"]/@value').get()

# ë˜ëŠ” CSS ì„ íƒì ì‚¬ìš©
csrf_token = response.css('form input[name="csrf_token"]::attr(value)').get()

# í† í° ì¶œë ¥í•˜ì—¬ í™•ì¸
self.logger.info(f"CSRF í† í°: {csrf_token}")
```

### 2. ë™ì  ì½˜í…ì¸ ê°€ ë³´ì´ì§€ ì•ŠëŠ” ê²½ìš°

#### ë¬¸ì œ: JavaScriptë¡œ ë¡œë“œë˜ëŠ” ì½˜í…ì¸ 

**í•´ê²° ë°©ë²• 1: API ì—”ë“œí¬ì¸íŠ¸ ì°¾ê¸°**

```bash
# ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ > Network íƒ­
# XHR/Fetch í•„í„° ì ìš©
# ì‹¤ì œ API ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
```

**í•´ê²° ë°©ë²• 2: Scrapy-Splash ì‚¬ìš©**

```python
# JavaScript ë Œë”ë§ì´ í•„ìš”í•œ ê²½ìš°
# Scrapy-Splash ë˜ëŠ” Selenium ê³ ë ¤
```

### 3. XPath í‘œí˜„ì‹ ìµœì í™”

#### âŒ ë‚˜ìœ ì˜ˆ: ì ˆëŒ€ ê²½ë¡œ

```python
response.xpath('/html/body/div/div[2]/div[1]/div[1]/span[1]/text()')
```

#### âœ… ì¢‹ì€ ì˜ˆ: ìƒëŒ€ ê²½ë¡œ + ì†ì„±

```python
response.xpath('//span[has-class("text")]/text()')
# ë˜ëŠ”
response.css('span.text::text')
```

### 4. ì¤‘ë³µ í•„í„° ë¬¸ì œ

#### ë¬¸ì œ: ê°™ì€ URL ì¬ë°©ë¬¸ ë¶ˆê°€

**í•´ê²° ë°©ë²•**:

```python
# ì „ì—­ ì„¤ì • (settings.py)
DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'

# ë˜ëŠ” ìŠ¤íŒŒì´ë”ë³„ ì„¤ì •
custom_settings = {
    'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',
}

# ë˜ëŠ” ê°œë³„ ìš”ì²­ì— dont_filter ì‚¬ìš©
yield scrapy.Request(url, callback=self.parse, dont_filter=True)
```

---

## ğŸ“Š ì‹¤ìŠµ ê²°ê³¼ ìš”ì•½

### êµ¬í˜„ëœ ìŠ¤íŒŒì´ë”

1. **ScrollableSpider** (`scrollablespider.py`)

   - 100ê°œ quotes ìˆ˜ì§‘
   - JSON API í™œìš©
   - ìë™ í˜ì´ì§€ë„¤ì´ì…˜

2. **ComplexRequestSpider** (`complex_request_spider.py`)

   - cURL ëª…ë ¹ì–´ ë³€í™˜
   - ë³µì¡í•œ í—¤ë” ì²˜ë¦¬

3. **SimpleLoginSpider** (`simple_login_spider.py`)

   - 1-pass ë¡œê·¸ì¸
   - 5ê°œ quotes ìˆ˜ì§‘
   - ë¹ ë¥¸ ì²˜ë¦¬

4. **ComplexLoginSpider** (`complex_login_spider.py`)
   - 2-pass ë¡œê·¸ì¸
   - CSRF í† í° ì²˜ë¦¬
   - 5ê°œ quotes ìˆ˜ì§‘

### ì„±ëŠ¥ í†µê³„

| ìŠ¤íŒŒì´ë”             | ìš”ì²­ ìˆ˜ | ìˆ˜ì§‘ ì•„ì´í…œ | ì²˜ë¦¬ ì‹œê°„ | ì„±ê³µë¥  |
| -------------------- | ------- | ----------- | --------- | ------ |
| ScrollableSpider     | 11      | 100         | ~34ì´ˆ     | 100%   |
| ComplexRequestSpider | 2       | 3           | ~4ì´ˆ      | 100%   |
| SimpleLoginSpider    | 3       | 5           | ~9ì´ˆ      | 100%   |
| ComplexLoginSpider   | 4       | 5           | ~13ì´ˆ     | 100%   |

---

## ğŸ“ í•µì‹¬ í•™ìŠµ ë‚´ìš©

### ë™ì  ì½˜í…ì¸  ì²˜ë¦¬

- âœ… ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ í™œìš©ë²•
- âœ… Network íƒ­ìœ¼ë¡œ API ì—”ë“œí¬ì¸íŠ¸ ë°œê²¬
- âœ… JSON ì‘ë‹µ íŒŒì‹±
- âœ… XPath í‘œí˜„ì‹ ìµœì í™”

### ë³´ì•ˆ ë° ì¸ì¦

- âœ… CSRF ê³µê²© ì´í•´
- âœ… CSRF í† í° ì¶”ì¶œ ë° í™œìš©
- âœ… ì¿ í‚¤ ê´€ë¦¬
- âœ… ì„¸ì…˜ ìœ ì§€

### ì‹¤ë¬´ ìŠ¤í‚¬

- âœ… cURL ëª…ë ¹ì–´ ì¬ì‚¬ìš©
- âœ… ë³µì¡í•œ HTTP ìš”ì²­ ì²˜ë¦¬
- âœ… ë¡œê·¸ì¸ ì‹œìŠ¤í…œ ìš°íšŒ
- âœ… ì—ëŸ¬ í•¸ë“¤ë§ ë° ë””ë²„ê¹…

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### ê³µì‹ ë¬¸ì„œ

- [Scrapy FormRequest](https://docs.scrapy.org/en/latest/topics/request-response.html#formrequest-objects)
- [Scrapy Request.from_curl()](https://docs.scrapy.org/en/latest/topics/request-response.html#request-from-curl)
- [CSRF ë³´ì•ˆ](https://portswigger.net/web-security/csrf)

### ê´€ë ¨ ë„êµ¬

- [curl2scrapy](https://michael-shub.github.io/curl2scrapy/) - cURL to Scrapy ë³€í™˜ê¸°
- [Scrapy-Splash](https://github.com/scrapy-plugins/scrapy-splash) - JavaScript ë Œë”ë§
- [Selenium](https://www.selenium.dev/) - ë¸Œë¼ìš°ì € ìë™í™”

### ëª¨ë²” ì‚¬ë¡€

1. **ìœ¤ë¦¬ì  í¬ë¡¤ë§**: robots.txt ì¤€ìˆ˜
2. **ì§€ì—° ì‹œê°„ ì„¤ì •**: ì„œë²„ ë¶€í•˜ ìµœì†Œí™”
3. **User-Agent ì„¤ì •**: ì ì ˆí•œ ì‹ë³„
4. **ì—ëŸ¬ ì²˜ë¦¬**: ê²¬ê³ í•œ ì˜ˆì™¸ ì²˜ë¦¬
5. **ë¡œê·¸ ë ˆë²¨ ì¡°ì •**: ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ì„¤ì¹˜ ê°€ì´ë“œ](INSTALLATION.md)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](PROJECT_STRUCTURE.md)
- [ë°°í¬ ê°€ì´ë“œ](DEPLOYMENT_GUIDE.md)
- [ì •ê·œí™” ë°ì´í„°ë² ì´ìŠ¤ ê°€ì´ë“œ](NORMALIZED_DB_GUIDE.md)

---

**ì‘ì„±ì¼**: 2025-10-13
**ë²„ì „**: 1.0
**ì‘ì„±ì**: Big Data Scrapy Tutorial
