#!/bin/bash

# ğŸš€ Scrapy ê³ ê¸‰ ê¸°ëŠ¥ í”„ë¡œì íŠ¸ í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
# ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œì íŠ¸ í™˜ê²½ì„ ìë™ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.

echo "ğŸ•·ï¸ Scrapy ê³ ê¸‰ ê¸°ëŠ¥ í”„ë¡œì íŠ¸ í™˜ê²½ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤..."

# í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
PROJECT_ROOT=$(pwd)
echo "ğŸ“‚ í”„ë¡œì íŠ¸ ë£¨íŠ¸: $PROJECT_ROOT"

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
if [[ "$VIRTUAL_ENV" != "" ]]; then
    echo "âœ… ê°€ìƒí™˜ê²½ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤: $VIRTUAL_ENV"
else
    echo "âš ï¸ ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # scrapy_env ë””ë ‰í† ë¦¬ê°€ ìˆëŠ”ì§€ í™•ì¸
    if [ -d "scrapy_env" ]; then
        echo "ğŸ”„ ê¸°ì¡´ ê°€ìƒí™˜ê²½ì„ í™œì„±í™”í•©ë‹ˆë‹¤..."
        source scrapy_env/bin/activate
        echo "âœ… ê°€ìƒí™˜ê²½ í™œì„±í™” ì™„ë£Œ!"
    else
        echo "âŒ scrapy_env ê°€ìƒí™˜ê²½ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        echo "ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ì„¸ìš”:"
        echo "   python3 -m venv scrapy_env"
        echo "   source scrapy_env/bin/activate"
        echo "   pip install -r requirements/requirements.txt"
        exit 1
    fi
fi

# Scrapy ì„¤ì¹˜ í™•ì¸
echo "ğŸ” Scrapy ì„¤ì¹˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤..."
if command -v scrapy &> /dev/null; then
    SCRAPY_VERSION=$(scrapy version)
    echo "âœ… Scrapyê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤: $SCRAPY_VERSION"
else
    echo "âŒ Scrapyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    echo "ğŸ“¦ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤..."
    pip install -r requirements/requirements.txt
fi

# í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
echo "ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤..."

REQUIRED_DIRS=("scrapy_project" "demos" "docs" "requirements" "scripts")
for dir in "${REQUIRED_DIRS[@]}"; do
    if [ -d "$dir" ]; then
        echo "âœ… $dir ë””ë ‰í† ë¦¬ ì¡´ì¬"
    else
        echo "âŒ $dir ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        mkdir -p "$dir"
        echo "ğŸ“ $dir ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
    fi
done

# Scrapy í”„ë¡œì íŠ¸ ì„¤ì • í™•ì¸
echo "ğŸ•·ï¸ Scrapy í”„ë¡œì íŠ¸ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤..."
cd scrapy_project

if [ -f "scrapy.cfg" ]; then
    echo "âœ… scrapy.cfg íŒŒì¼ ì¡´ì¬"
else
    echo "âŒ scrapy.cfg íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."
    exit 1
fi

# ìŠ¤íŒŒì´ë” ëª©ë¡ í™•ì¸
echo "ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íŒŒì´ë”ë¥¼ í™•ì¸í•©ë‹ˆë‹¤..."
SPIDERS=$(scrapy list 2>/dev/null)
if [ $? -eq 0 ]; then
    echo "âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íŒŒì´ë”ë“¤:"
    echo "$SPIDERS" | sed 's/^/   â€¢ /'
else
    echo "âŒ ìŠ¤íŒŒì´ë” ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
fi

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
echo "ğŸ“Š ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤..."
OUTPUT_DIRS=("outputs/json" "outputs/csv" "outputs/databases")
for dir in "${OUTPUT_DIRS[@]}"; do
    if [ ! -d "$dir" ]; then
        mkdir -p "$dir"
        echo "ğŸ“ $dir ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤."
    else
        echo "âœ… $dir ë””ë ‰í† ë¦¬ ì¡´ì¬"
    fi
done

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export SCRAPY_SETTINGS_MODULE=tutorial.settings
echo "âš™ï¸ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ: SCRAPY_SETTINGS_MODULE=$SCRAPY_SETTINGS_MODULE"

# ì„¤ì • ì™„ë£Œ ë©”ì‹œì§€
echo ""
echo "ğŸ‰ í™˜ê²½ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸš€ ë‹¤ìŒ ëª…ë ¹ì–´ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
echo ""
echo "   ğŸ“‹ ê¸°ë³¸ í¬ë¡¤ë§:"
echo "   scrapy crawl quotes_spider -o outputs/json/basic_quotes.json"
echo ""
echo "   ğŸ”„ ItemLoader ì‚¬ìš©:"
echo "   scrapy crawl complex_quotes -o outputs/json/complex_quotes.json"
echo ""
echo "   ğŸ•µï¸ User-Agent íšŒì „:"
echo "   scrapy crawl useragent_spider -o outputs/json/useragent_test.json"
echo ""
echo "   ğŸ›¡ï¸ ìœ¤ë¦¬ì  í¬ë¡¤ë§:"
echo "   scrapy crawl ethical_spider -o outputs/json/ethical_crawling.json"
echo ""
echo "   ğŸ® ë°ëª¨ ì‹¤í–‰:"
echo "   python ../demos/advanced_features/useragent_demo.py"
echo ""
echo "Happy Scraping! ğŸ•·ï¸âœ¨"

cd "$PROJECT_ROOT"
