#!/usr/bin/env python3
"""
ğŸ•·ï¸ ëª¨ë“  ìŠ¤íŒŒì´ë” ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ìŠ¤íŒŒì´ë”ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³ 
ê²°ê³¼ë¥¼ ì •ë¦¬ëœ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.
"""

import subprocess
import os
import sys
import json
from datetime import datetime
from pathlib import Path


class SpiderRunner:
    """ìŠ¤íŒŒì´ë” ì‹¤í–‰ ë° ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.scrapy_project = self.project_root / "scrapy_project"
        self.outputs_dir = self.scrapy_project / "outputs"

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        (self.outputs_dir / "json").mkdir(parents=True, exist_ok=True)
        (self.outputs_dir / "csv").mkdir(parents=True, exist_ok=True)
        (self.outputs_dir / "databases").mkdir(parents=True, exist_ok=True)

        # ì‹¤í–‰ ê²°ê³¼ ì €ì¥
        self.results = []

    def get_available_spiders(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íŒŒì´ë” ëª©ë¡ ì¡°íšŒ"""
        try:
            os.chdir(self.scrapy_project)
            result = subprocess.run(
                ["scrapy", "list"], capture_output=True, text=True, check=True
            )
            spiders = result.stdout.strip().split("\n")
            return [spider.strip() for spider in spiders if spider.strip()]
        except subprocess.CalledProcessError as e:
            print(f"âŒ ìŠ¤íŒŒì´ë” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def run_spider(self, spider_name, output_format="json", delay=1):
        """ê°œë³„ ìŠ¤íŒŒì´ë” ì‹¤í–‰"""
        print(f"\nğŸ•·ï¸ {spider_name} ìŠ¤íŒŒì´ë”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")

        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"{spider_name}_{timestamp}.{output_format}"
        output_path = self.outputs_dir / output_format / output_file

        # Scrapy ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            "scrapy",
            "crawl",
            spider_name,
            "-o",
            str(output_path),
            "-s",
            f"DOWNLOAD_DELAY={delay}",
            "-L",
            "INFO",
        ]

        try:
            # ì‹¤í–‰ ì‹œì‘ ì‹œê°„
            start_time = datetime.now()

            # ìŠ¤íŒŒì´ë” ì‹¤í–‰
            result = subprocess.run(
                cmd, capture_output=True, text=True, check=True, cwd=self.scrapy_project
            )

            # ì‹¤í–‰ ì™„ë£Œ ì‹œê°„
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # ê²°ê³¼ íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = output_path.stat().st_size if output_path.exists() else 0

            # ìˆ˜ì§‘ëœ ì•„ì´í…œ ìˆ˜ ê³„ì‚° (JSON íŒŒì¼ì¸ ê²½ìš°)
            item_count = 0
            if output_format == "json" and output_path.exists():
                try:
                    with open(output_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        item_count = len(data) if isinstance(data, list) else 1
                except:
                    pass

            # ê²°ê³¼ ì €ì¥
            spider_result = {
                "spider": spider_name,
                "status": "success",
                "duration": round(duration, 2),
                "output_file": str(output_path),
                "file_size": file_size,
                "item_count": item_count,
                "timestamp": start_time.isoformat(),
            }

            self.results.append(spider_result)

            print(f"âœ… {spider_name} ì™„ë£Œ!")
            print(f"   ğŸ“Š ìˆ˜ì§‘ ì•„ì´í…œ: {item_count}ê°œ")
            print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
            print(f"   ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")

            return True

        except subprocess.CalledProcessError as e:
            print(f"âŒ {spider_name} ì‹¤í–‰ ì‹¤íŒ¨:")
            print(f"   ì˜¤ë¥˜: {e}")

            spider_result = {
                "spider": spider_name,
                "status": "failed",
                "error": str(e),
                "timestamp": datetime.now().isoformat(),
            }

            self.results.append(spider_result)
            return False

    def run_all_spiders(self):
        """ëª¨ë“  ìŠ¤íŒŒì´ë” ì‹¤í–‰"""
        print("ğŸš€ ëª¨ë“  ìŠ¤íŒŒì´ë” ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("=" * 60)

        spiders = self.get_available_spiders()

        if not spiders:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íŒŒì´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ“‹ ì‹¤í–‰í•  ìŠ¤íŒŒì´ë”: {len(spiders)}ê°œ")
        for spider in spiders:
            print(f"   â€¢ {spider}")

        # ê° ìŠ¤íŒŒì´ë”ë³„ ì„¤ì •
        spider_configs = {
            "quotes_spider": {"delay": 1, "format": "json"},
            "complex_quotes": {"delay": 1, "format": "json"},
            "useragent_spider": {"delay": 1, "format": "json"},
            "ethical_spider": {
                "delay": 2,
                "format": "json",
            },  # ìœ¤ë¦¬ì  í¬ë¡¤ë§ì€ ë” ëŠë¦¬ê²Œ
        }

        # ìŠ¤íŒŒì´ë” ì‹¤í–‰
        successful = 0
        failed = 0

        for spider in spiders:
            config = spider_configs.get(spider, {"delay": 1, "format": "json"})

            if self.run_spider(spider, config["format"], config["delay"]):
                successful += 1
            else:
                failed += 1

        # ê²°ê³¼ ìš”ì•½
        self.print_summary(successful, failed)
        self.save_report()

    def print_summary(self, successful, failed):
        """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)

        total = successful + failed
        print(f"ğŸ¯ ì „ì²´ ìŠ¤íŒŒì´ë”: {total}ê°œ")
        print(f"âœ… ì„±ê³µ: {successful}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")

        if self.results:
            print("\nğŸ“‹ ê°œë³„ ê²°ê³¼:")
            for result in self.results:
                status_icon = "âœ…" if result["status"] == "success" else "âŒ"
                spider_name = result["spider"]

                if result["status"] == "success":
                    print(
                        f"   {status_icon} {spider_name}: {result['item_count']}ê°œ ì•„ì´í…œ, {result['duration']}ì´ˆ"
                    )
                else:
                    print(f"   {status_icon} {spider_name}: ì‹¤íŒ¨")

    def save_report(self):
        """ì‹¤í–‰ ë³´ê³ ì„œ ì €ì¥"""
        report_file = self.outputs_dir / "execution_report.json"

        report = {
            "timestamp": datetime.now().isoformat(),
            "total_spiders": len(self.results),
            "successful": len([r for r in self.results if r["status"] == "success"]),
            "failed": len([r for r in self.results if r["status"] == "failed"]),
            "results": self.results,
        }

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ ì‹¤í–‰ ë³´ê³ ì„œ ì €ì¥: {report_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ•·ï¸ Scrapy ì „ì²´ ìŠ¤íŒŒì´ë” ì‹¤í–‰ê¸°")
    print("=" * 60)

    runner = SpiderRunner()

    if len(sys.argv) > 1:
        # íŠ¹ì • ìŠ¤íŒŒì´ë”ë§Œ ì‹¤í–‰
        spider_name = sys.argv[1]
        print(f"ğŸ¯ íŠ¹ì • ìŠ¤íŒŒì´ë” ì‹¤í–‰: {spider_name}")
        runner.run_spider(spider_name)
    else:
        # ëª¨ë“  ìŠ¤íŒŒì´ë” ì‹¤í–‰
        runner.run_all_spiders()

    print("\nğŸ‰ ìŠ¤íŒŒì´ë” ì‹¤í–‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
