#!/usr/bin/env python3
"""
ìœ¤ë¦¬ì  í¬ë¡¤ë§ ë° USER-AGENT ì‚¬ìš©ë²• ë°ëª¨
"""
import scrapy
from scrapy.crawler import CrawlerProcess
import random
import time


class EthicalCrawlerSpider(scrapy.Spider):
    name = "ethical_crawler"

    # ë‹¤ì–‘í•œ USER-AGENT ëª©ë¡
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    ]

    def start_requests(self):
        urls = [
            "http://quotes.toscrape.com/",
            "http://quotes.toscrape.com/page/2/",
        ]

        for url in urls:
            # ëœë¤ USER-AGENT ì„ íƒ
            user_agent = random.choice(self.user_agents)

            yield scrapy.Request(
                url=url,
                callback=self.parse,
                headers={"User-Agent": user_agent},
                meta={"user_agent": user_agent},
            )

    def parse(self, response):
        user_agent = response.meta.get("user_agent", "Unknown")
        print(f"\nğŸŒ í˜ì´ì§€ ì²˜ë¦¬: {response.url}")
        print(f"ğŸ­ ì‚¬ìš©ëœ USER-AGENT: {user_agent[:50]}...")
        print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status}")

        # robots.txt ì¤€ìˆ˜ ì—¬ë¶€ í™•ì¸
        print(f"ğŸ¤– robots.txt ì¤€ìˆ˜: {self.settings.get('ROBOTSTXT_OBEY')}")

        # ë‹¤ìš´ë¡œë“œ ì§€ì—° ì •ë³´
        delay = self.settings.get("DOWNLOAD_DELAY", 0)
        print(f"â° ë‹¤ìš´ë¡œë“œ ì§€ì—°: {delay}ì´ˆ")

        quotes = response.css("div.quote")
        print(f"ğŸ’¬ ë°œê²¬ëœ ëª…ì–¸: {len(quotes)}ê°œ")

        for i, quote in enumerate(quotes[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì²˜ë¦¬
            quote_text = quote.css("span.text::text").get()
            author = quote.css("small.author::text").get()

            yield {
                "quote": quote_text,
                "author": author,
                "url": response.url,
                "user_agent": user_agent[:50] + "...",
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            }

            print(f"  ğŸ“ ëª…ì–¸ {i}: {author} - {quote_text[:30]}...")


def test_ethical_crawling():
    """ìœ¤ë¦¬ì  í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ìœ¤ë¦¬ì  í¬ë¡¤ë§ ë°ëª¨ ì‹œì‘!")
    print("=" * 60)

    process = CrawlerProcess(
        {
            # ìœ¤ë¦¬ì  í¬ë¡¤ë§ ì„¤ì •
            "USER_AGENT": "EthicalBot/1.0 (+http://example.com/bot-info)",
            "ROBOTSTXT_OBEY": True,  # robots.txt ì¤€ìˆ˜
            "DOWNLOAD_DELAY": 2,  # 2ì´ˆ ì§€ì—°
            "RANDOMIZE_DOWNLOAD_DELAY": True,  # ì§€ì—° ì‹œê°„ ëœë¤í™”
            "CONCURRENT_REQUESTS_PER_DOMAIN": 1,  # ë„ë©”ì¸ë‹¹ ë™ì‹œ ìš”ì²­ 1ê°œ
            # AutoThrottle ì„¤ì •
            "AUTOTHROTTLE_ENABLED": True,
            "AUTOTHROTTLE_START_DELAY": 1,
            "AUTOTHROTTLE_MAX_DELAY": 10,
            "AUTOTHROTTLE_TARGET_CONCURRENCY": 1.0,
            "AUTOTHROTTLE_DEBUG": True,
            # HTTP ìºì‹œ í™œì„±í™”
            "HTTPCACHE_ENABLED": True,
            "HTTPCACHE_EXPIRATION_SECS": 300,
            # ë¡œê·¸ ì„¤ì •
            "LOG_LEVEL": "INFO",
            # ê²°ê³¼ ì €ì¥
            "FEEDS": {
                "ethical_crawling_result.json": {
                    "format": "json",
                    "encoding": "utf8",
                    "overwrite": True,
                },
            },
        }
    )

    process.crawl(EthicalCrawlerSpider)
    process.start()


def test_robots_txt_check():
    """robots.txt í™•ì¸ ë°ëª¨"""
    print("\nğŸ¤– robots.txt í™•ì¸ ë°ëª¨")
    print("=" * 40)

    import urllib.robotparser

    sites = [
        "http://quotes.toscrape.com",
        "https://finance.naver.com",
        "https://www.google.com",
    ]

    for site in sites:
        print(f"\nğŸŒ ì‚¬ì´íŠ¸: {site}")
        try:
            rp = urllib.robotparser.RobotFileParser()
            rp.set_url(f"{site}/robots.txt")
            rp.read()

            # ì¼ë°˜ì ì¸ í¬ë¡¤ëŸ¬ í™•ì¸
            user_agents = ["*", "Googlebot", "Scrapy"]

            for ua in user_agents:
                can_fetch = rp.can_fetch(ua, site + "/")
                status = "âœ… í—ˆìš©" if can_fetch else "âŒ ì°¨ë‹¨"
                print(f"  ğŸ­ {ua}: {status}")

        except Exception as e:
            print(f"  âš ï¸ robots.txt í™•ì¸ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    print("ğŸ“‹ ìœ¤ë¦¬ì  í¬ë¡¤ë§ ë° USER-AGENT ë°ëª¨")
    print("ğŸ¯ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì¡´ì¤‘í•˜ë©° í¬ë¡¤ë§í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤")
    print()
    print("1. ìœ¤ë¦¬ì  í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("2. robots.txt í™•ì¸")
    print("3. ëª¨ë‘ ì‹¤í–‰")

    choice = input("\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-3): ").strip()

    if choice == "1":
        test_ethical_crawling()
    elif choice == "2":
        test_robots_txt_check()
    elif choice == "3":
        test_robots_txt_check()
        print("\n" + "=" * 60)
        test_ethical_crawling()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
