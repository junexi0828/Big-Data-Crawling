"""
ë³µì¡í•œ HTTP ìš”ì²­ ì²˜ë¦¬ ë°ëª¨
- cURL í˜•ì‹ì„ Scrapy Requestë¡œ ë³€í™˜
- Request.from_curl() ë©”ì„œë“œ ì‚¬ìš©
- scrapy.utils.curl.curl_to_request_kwargs() ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
"""

import scrapy
from scrapy.utils.curl import curl_to_request_kwargs


class ComplexRequestSpider(scrapy.Spider):
    name = "complex_request_spider"
    allowed_domains = ["quotes.toscrape.com"]

    def start_requests(self):
        # ë°©ë²• 1: Request.from_curl() ë©”ì„œë“œ ì‚¬ìš©
        # ì´ë¯¸ì§€ì—ì„œ ë³´ì—¬ì¤€ cURL ëª…ë ¹ì–´ë¥¼ Scrapy Requestë¡œ ë³€í™˜
        curl_command = """
        curl 'http://quotes.toscrape.com/api/quotes?page=1' -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0' -H 'Accept: */*' -H 'Accept-Language: ca,en-US;q=0.7,en;q=0.3' --compressed -H 'X-Requested-With: XMLHttpRequest' -H 'Proxy-Authorization: Basic QFRLLTzEwZTAxLTkSMWUtNDFiNCIZWRmLTJjNGI4N2ZiNDEtOTkxZS00Mw' -H 'Connection: keep-alive' -H 'Referer: http://quotes.toscrape.com/scroll' -H 'Cache-Control: max-age=0'
        """

        request = scrapy.Request.from_curl(
            curl_command.strip(), callback=self.parse_curl_response
        )
        yield request

        # ë°©ë²• 2: curl_to_request_kwargs() ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
        curl_kwargs = curl_to_request_kwargs(
            curl_command.strip(), ignore_unknown_options=True
        )
        yield scrapy.Request(**curl_kwargs, callback=self.parse_kwargs_response)

    def parse_curl_response(self, response):
        """from_curl() ë©”ì„œë“œë¡œ ë§Œë“  ìš”ì²­ì˜ ì‘ë‹µ ì²˜ë¦¬"""
        self.logger.info(f"âœ… from_curl() ë°©ë²•ìœ¼ë¡œ ë°›ì€ ì‘ë‹µ: {response.status}")
        self.logger.info(
            f"Content-Type: {response.headers.get('Content-Type', b'').decode()}"
        )

        # JSON ì‘ë‹µ íŒŒì‹±
        import json

        try:
            data = json.loads(response.text)
            self.logger.info(f"ğŸ“Š quotes ê°œìˆ˜: {len(data.get('quotes', []))}")

            for quote in data.get("quotes", [])[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                yield {
                    "method": "from_curl",
                    "quote": quote["text"][:50] + "...",
                    "author": quote["author"]["name"],
                    "tags": quote["tags"],
                }
        except json.JSONDecodeError:
            self.logger.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨")

    def parse_kwargs_response(self, response):
        """curl_to_request_kwargs() ìœ í‹¸ë¦¬í‹°ë¡œ ë§Œë“  ìš”ì²­ì˜ ì‘ë‹µ ì²˜ë¦¬"""
        self.logger.info(
            f"âœ… curl_to_request_kwargs() ë°©ë²•ìœ¼ë¡œ ë°›ì€ ì‘ë‹µ: {response.status}"
        )

        import json

        try:
            data = json.loads(response.text)
            self.logger.info(f"ğŸ“Š quotes ê°œìˆ˜: {len(data.get('quotes', []))}")

            for quote in data.get("quotes", [])[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                yield {
                    "method": "curl_to_request_kwargs",
                    "quote": quote["text"][:50] + "...",
                    "author": quote["author"]["name"],
                    "tags": quote["tags"],
                }
        except json.JSONDecodeError:
            self.logger.error("âŒ JSON íŒŒì‹± ì‹¤íŒ¨")
