# ğŸ•·ï¸ Scrapy ê³ ê¸‰ ê¸°ëŠ¥ ì™„ì „ ì‹¤ìŠµ í”„ë¡œì íŠ¸

ì´ í”„ë¡œì íŠ¸ëŠ” **Scrapyì˜ ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥**ì„ í•™ìŠµí•˜ê³  ì‹¤ìŠµí•  ìˆ˜ ìˆëŠ” **ì™„ì „í•œ ê°€ì´ë“œ**ì…ë‹ˆë‹¤.

## ğŸ¯ **í”„ë¡œì íŠ¸ ê°œìš”**

ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ì€ Scrapy ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤:

- âœ… **Spider Arguments** - ëª…ë ¹ì¤„ ì¸ìë¥¼ í†µí•œ ë™ì  í¬ë¡¤ë§
- âœ… **ItemLoader** - ë°ì´í„° ì „ì²˜ë¦¬ ë° ê²€ì¦
- âœ… **Item Pipeline** - ë°ì´í„° ì €ì¥ ë° í›„ì²˜ë¦¬
- âœ… **Duplication Filter** - ì¤‘ë³µ ë°ì´í„° ì œê±°
- âœ… **Ethical Crawling** - ìœ¤ë¦¬ì  í¬ë¡¤ë§ ì›ì¹™ ì¤€ìˆ˜
- âœ… **User-Agent íšŒì „** - ì°¨ë‹¨ ìš°íšŒ ê¸°ìˆ 

## ğŸ“ **í”„ë¡œì íŠ¸ êµ¬ì¡°**

```
ğŸ“ scrapy-advanced-tutorial/
â”œâ”€â”€ ğŸ“ scrapy_project/                 # ğŸ•·ï¸ ë©”ì¸ Scrapy í”„ë¡œì íŠ¸ (ì •ë¦¬ëœ ë²„ì „)
â”‚   â”œâ”€â”€ scrapy.cfg                     # Scrapy ì„¤ì •
â”‚   â”œâ”€â”€ tutorial/                      # ë©”ì¸ íŒ¨í‚¤ì§€
â”‚   â”‚   â”œâ”€â”€ settings.py               # ìœ¤ë¦¬ì  í¬ë¡¤ë§ ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ items.py                  # ItemLoader ì ìš© ì•„ì´í…œ
â”‚   â”‚   â”œâ”€â”€ itemloaders.py            # ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
â”‚   â”‚   â”œâ”€â”€ pipelines.py              # ë°ì´í„° íŒŒì´í”„ë¼ì¸
â”‚   â”‚   â””â”€â”€ spiders/                  # ğŸ•·ï¸ ëª¨ë“  ìŠ¤íŒŒì´ë”
â”‚   â”‚       â”œâ”€â”€ quotes_spider.py      # ê¸°ë³¸ í¬ë¡¤ë§
â”‚   â”‚       â”œâ”€â”€ complex_quotes.py     # ItemLoader + ì¤‘ë³µí•„í„°
â”‚   â”‚       â”œâ”€â”€ useragent_spider.py   # User-Agent íšŒì „
â”‚   â”‚       â””â”€â”€ ethical_spider.py     # ìœ¤ë¦¬ì  í¬ë¡¤ë§
â”‚   â””â”€â”€ outputs/                      # ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼
â”‚       â”œâ”€â”€ json/                     # JSON ê²°ê³¼
â”‚       â”œâ”€â”€ csv/                      # CSV ê²°ê³¼
â”‚       â””â”€â”€ databases/                # SQLite ë°ì´í„°ë² ì´ìŠ¤
â”‚
â”œâ”€â”€ ğŸ“ tutorial/                       # ğŸ“š ì›ë³¸ ê°œë°œ ê³¼ì • (í•™ìŠµ ì°¸ê³ ìš©)
â”‚   â”œâ”€â”€ scrapy.cfg                     # ì›ë³¸ Scrapy ì„¤ì •
â”‚   â””â”€â”€ tutorial/                      # ê°œë°œ ê³¼ì •ì˜ ëª¨ë“  íŒŒì¼ë“¤
â”‚
â”œâ”€â”€ ğŸ“ demos/                          # ğŸ® í•™ìŠµìš© ë°ëª¨
â”‚   â”œâ”€â”€ basic_features/               # ê¸°ë³¸ ê¸°ëŠ¥ ë°ëª¨ (pagination, follow ë“±)
â”‚   â”œâ”€â”€ advanced_features/            # ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨ (ItemLoader, User-Agent ë“±)
â”‚   â””â”€â”€ scrapy_shell/                 # Shell ëª…ë ¹ì–´ ë°ëª¨
â”‚
â”œâ”€â”€ ğŸ“ docs/                          # ğŸ“– í”„ë¡œì íŠ¸ ë¬¸ì„œ
â”‚   â”œâ”€â”€ README.md                     # ìƒì„¸ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ INSTALLATION.md               # ì„¤ì¹˜ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ DEPLOYMENT_GUIDE.md          # ë°°í¬ ê°€ì´ë“œ
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md         # êµ¬ì¡° ì„¤ëª…
â”‚
â”œâ”€â”€ ğŸ“ scripts/                       # ğŸ”§ ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup_environment.sh         # í™˜ê²½ ì„¤ì • ìë™í™”
â”‚   â”œâ”€â”€ run_all_spiders.py           # ëª¨ë“  ìŠ¤íŒŒì´ë” ì‹¤í–‰
â”‚   â””â”€â”€ clean_outputs.py             # ê²°ê³¼ íŒŒì¼ ì •ë¦¬
â”‚
â”œâ”€â”€ ğŸ“ requirements/                  # ğŸ“¦ ì˜ì¡´ì„± ê´€ë¦¬
â”‚   â”œâ”€â”€ requirements.txt             # ê¸°ë³¸ íŒ¨í‚¤ì§€
â”‚   â””â”€â”€ requirements-dev.txt         # ê°œë°œìš© íŒ¨í‚¤ì§€
â”‚
â”œâ”€â”€ scrapy_env/                      # ğŸ Python ê°€ìƒí™˜ê²½
â”œâ”€â”€ index.html                       # ğŸŒ í”„ë¡œì íŠ¸ ì›¹ ì¸í„°í˜ì´ìŠ¤
â””â”€â”€ README.md                        # ğŸ“‹ í”„ë¡œì íŠ¸ ë©”ì¸ ê°€ì´ë“œ
```

## ğŸš€ **ë¹ ë¥¸ ì‹œì‘**

### 1. ê°€ìƒí™˜ê²½ í™œì„±í™”
```bash
source scrapy_env/bin/activate
```

### 2. í™˜ê²½ ì„¤ì • (ìë™)
```bash
./scripts/setup_environment.sh
```

### 3. ê¸°ë³¸ í¬ë¡¤ë§ ì‹¤í–‰
```bash
cd scrapy_project
scrapy crawl quotes -o outputs/json/basic_quotes.json
```

### 4. ê³ ê¸‰ ê¸°ëŠ¥ ì‹¤í–‰
```bash
# ItemLoader ì‚¬ìš©
scrapy crawl complex_quotes -o outputs/json/complex_quotes.json

# User-Agent íšŒì „
scrapy crawl useragent_spider -o outputs/json/useragent_test.json

# ìœ¤ë¦¬ì  í¬ë¡¤ë§
scrapy crawl ethical_crawler -o outputs/json/ethical_crawling.json
```

### 5. ëª¨ë“  ìŠ¤íŒŒì´ë” í•œë²ˆì— ì‹¤í–‰
```bash
python scripts/run_all_spiders.py
```

## ğŸ® **ë°ëª¨ ì‹¤í–‰**

### ê¸°ë³¸ ê¸°ëŠ¥ ë°ëª¨

```bash
python demos/basic_features/tutorial_explanations/follow_explanation.py
```

### ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨

```bash
# ItemLoader ë°ëª¨
python demos/advanced_features/itemloader_demo.py

# ì¤‘ë³µ í•„í„° ë°ëª¨
python demos/advanced_features/duplication_filter_demo.py

# User-Agent ë°ëª¨
python demos/advanced_features/useragent_demo.py

# ìœ¤ë¦¬ì  í¬ë¡¤ë§ ë°ëª¨
python demos/advanced_features/ethical_crawling_complete_demo.py
```

## ğŸ“– **í•™ìŠµ ê°€ì´ë“œ**

1. **ê¸°ì´ˆ í•™ìŠµ**: `demos/basic_features/` ì—ì„œ ì‹œì‘
2. **ê³ ê¸‰ ê¸°ëŠ¥**: `demos/advanced_features/` ë¡œ ì§„í–‰
3. **ì‹¤ì „ ì ìš©**: `scrapy_project/` ì—ì„œ ì‹¤ìŠµ
4. **ì‹¬í™” í•™ìŠµ**: `docs/` ì—ì„œ ìƒì„¸ ê°€ì´ë“œ í™•ì¸

## ğŸ›¡ï¸ **ìœ¤ë¦¬ì  í¬ë¡¤ë§**

ë³¸ í”„ë¡œì íŠ¸ëŠ” **ìœ¤ë¦¬ì  í¬ë¡¤ë§ 4ì›ì¹™**ì„ ì¤€ìˆ˜í•©ë‹ˆë‹¤:

1. âœ… **robots.txt ì¤€ìˆ˜** - `ROBOTSTXT_OBEY = True`
2. âœ… **ì„±ëŠ¥ ì €í•˜ ë°©ì§€** - `DOWNLOAD_DELAY`, `CONCURRENT_REQUESTS` ì œí•œ
3. âœ… **ì‹ ì› í™•ì¸** - ì ì ˆí•œ `USER_AGENT` ì„¤ì •
4. âœ… **ê´€ë¦¬ì ë°°ë ¤** - AutoThrottle, HTTP ìºì‹œ í™œìš©

## ğŸ“Š **ê²°ê³¼ í™•ì¸**

í¬ë¡¤ë§ ê²°ê³¼ëŠ” `scrapy_project/outputs/` ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **JSON**: `outputs/json/*.json`
- **CSV**: `outputs/csv/*.csv`
- **SQLite**: `outputs/databases/*.db`

## ğŸ”— **ìœ ìš©í•œ ë§í¬**

- [Scrapy ê³µì‹ ë¬¸ì„œ](https://docs.scrapy.org/)
- [í”„ë¡œì íŠ¸ ê¹ƒí—ˆë¸Œ](https://github.com/junexi0828/Big-Data-Crawling)
- [ì„¤ì¹˜ ê°€ì´ë“œ](docs/INSTALLATION.md)
- [ë°°í¬ ê°€ì´ë“œ](docs/DEPLOYMENT_GUIDE.md)

---

**Happy Scraping! ğŸ‰**
