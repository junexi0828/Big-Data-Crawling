#!/usr/bin/env python3
"""
Duplication Filter ì™„ì „ ë°ëª¨ - ì´ë¯¸ì§€ ìŠ¬ë¼ì´ë“œì˜ 3ê°€ì§€ ë°©ë²• ëª¨ë‘ êµ¬í˜„
50 quotes out of 100 quotes ìƒí™©ì„ ì¬í˜„í•˜ê³  í•´ê²°ì±… ì œì‹œ
"""
import subprocess
import os


def demo_duplication_filter():
    """Duplication Filter 3ê°€ì§€ ë°©ë²• ë°ëª¨"""
    print(
        "ğŸ¯ Duplication Filter ë°ëª¨ - Spider complex_quotes scraped only 50 quotes out of 100 quotes!"
    )
    print("=" * 90)
    print("Some were not populated due to duplication filtering of author urls.")
    print()

    print("ğŸ“‹ ì´ë¯¸ì§€ì—ì„œ ë³´ì—¬ì¤€ 3ê°€ì§€ í•´ê²° ë°©ë²•:")
    print()

    # 1. Global setting ë°©ë²•
    print("1ï¸âƒ£ Global setting: change settings.py file and run again")
    print(
        "   â€¢ Disable the duplicate filter altogether! â† be careful about crawling loop!"
    )
    print("   â€¢ íŒŒì¼: tutorial/tutorial/settings.py")
    print("   â€¢ ì½”ë“œ: DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'")
    print()

    # 2. Command line option ë°©ë²•
    print("2ï¸âƒ£ Use command line option â† spider-wise (revoke global setting before!)")
    print("   â€¢ Explicitly override setting using the -s or --set command line option")
    print("   â€¢ ëª…ë ¹ì–´:")
    print("     scrapy crawl complex_quotes --set")
    print("     DUPEFILTER_CLASS=scrapy.dupefilters.BaseDupeFilter")
    print()

    # 3. custom_settings ë°©ë²•
    print("3ï¸âƒ£ Add custom_settings block to your spider â† spider-wise!")
    print("   â€¢ Modify your spider and run")
    print("   â€¢ ì½”ë“œ:")
    print("     custom_settings = {")
    print("         'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',")
    print("     }")
    print()


def test_duplication_filter_methods():
    """ê° ë°©ë²•ë“¤ì„ ì‹¤ì œë¡œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    print("=" * 50)

    # í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
    base_dir = "/Users/juns/bigdata"
    tutorial_dir = os.path.join(base_dir, "tutorial")

    if not os.path.exists(tutorial_dir):
        print("âŒ tutorial ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print("1ï¸âƒ£ ê¸°ë³¸ ìƒíƒœì—ì„œ complex_quotes ì‹¤í–‰ (ì¤‘ë³µ í•„í„° í™œì„±í™”)")
    print("   ëª…ë ¹ì–´: scrapy crawl complex_quotes -o normal_result.json")
    print("   ì˜ˆìƒ ê²°ê³¼: 50ê°œ ì •ë„ ìˆ˜ì§‘ (ì¤‘ë³µ URL í•„í„°ë§ìœ¼ë¡œ ì¸í•´)")
    print()

    print("2ï¸âƒ£ Command line ì˜µì…˜ìœ¼ë¡œ ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™”")
    print(
        "   ëª…ë ¹ì–´: scrapy crawl complex_quotes --set DUPEFILTER_CLASS=scrapy.dupefilters.BaseDupeFilter -o no_filter_result.json"
    )
    print("   ì˜ˆìƒ ê²°ê³¼: 100ê°œ ìˆ˜ì§‘ (ëª¨ë“  ëª…ì–¸ ìˆ˜ì§‘)")
    print()

    print("3ï¸âƒ£ custom_settingsê°€ ì ìš©ëœ ìƒíƒœì—ì„œ ì‹¤í–‰")
    print("   ëª…ë ¹ì–´: scrapy crawl complex_quotes -o custom_settings_result.json")
    print("   ì˜ˆìƒ ê²°ê³¼: 100ê°œ ìˆ˜ì§‘ (custom_settings ë•ë¶„ì—)")
    print()

    print("ğŸ’¡ ì‹¤ì œ ì‹¤í–‰í•´ë³´ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë“¤ì„ ì‚¬ìš©í•˜ì„¸ìš”:")
    print(f"   cd {tutorial_dir}")
    print("   scrapy crawl complex_quotes -o dupefilter_test.json")
    print(
        "   scrapy crawl complex_quotes --set DUPEFILTER_CLASS=scrapy.dupefilters.BaseDupeFilter -o no_dupefilter_test.json"
    )


def explain_duplication_filtering():
    """ì¤‘ë³µ í•„í„°ë§ì´ ì™œ ë°œìƒí•˜ëŠ”ì§€ ì„¤ëª…"""
    print("\nğŸ” ì™œ ì¤‘ë³µ í•„í„°ë§ì´ ë°œìƒí•˜ë‚˜ìš”?")
    print("=" * 50)
    print("â€¢ ì—¬ëŸ¬ ëª…ì–¸ì´ ê°™ì€ ì‘ê°€ì˜ ê²ƒì¼ ë•Œ")
    print("â€¢ ì‘ê°€ ìƒì„¸ í˜ì´ì§€ URLì´ ë™ì¼í•¨")
    print("â€¢ Scrapyì˜ ê¸°ë³¸ ì¤‘ë³µ í•„í„°ê°€ ê°™ì€ URL ì¬ë°©ë¬¸ì„ ì°¨ë‹¨")
    print("â€¢ ê²°ê³¼ì ìœ¼ë¡œ ì¼ë¶€ ì‘ê°€ ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì§€ ì•ŠìŒ")
    print()

    print("ğŸ“Š ì´ë¯¸ì§€ì—ì„œ ë³´ì—¬ì¤€ ìƒí™©:")
    print("â€¢ complex_quotes ìŠ¤íŒŒì´ë”ê°€ 50ê°œë§Œ ìˆ˜ì§‘ (100ê°œ ì¤‘)")
    print("â€¢ 'item_scraped_count': 50 (ë¡œê·¸ì—ì„œ í™•ì¸ ê°€ëŠ¥)")
    print("â€¢ 111 requests ì „ì†¡í–ˆì§€ë§Œ ì¼ë¶€ëŠ” ì¤‘ë³µìœ¼ë¡œ ì°¨ë‹¨ë¨")
    print()

    print("âš ï¸ ì£¼ì˜ì‚¬í•­:")
    print("â€¢ ì¤‘ë³µ í•„í„°ë¥¼ ë¹„í™œì„±í™”í•˜ë©´ ë¬´í•œ ë£¨í”„ ìœ„í—˜")
    print("â€¢ crawling loopì— ì£¼ì˜í•´ì•¼ í•¨")
    print("â€¢ ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©í•´ì•¼ í•¨")


if __name__ == "__main__":
    print("ğŸ¯ Scrapy Duplication Filter ì™„ì „ ë°ëª¨")
    print("=" * 80)

    # 1. ê¸°ë³¸ ì„¤ëª…
    demo_duplication_filter()

    # 2. í…ŒìŠ¤íŠ¸ ë°©ë²•
    test_duplication_filter_methods()

    # 3. ì›ë¦¬ ì„¤ëª…
    explain_duplication_filtering()

    print("\nâœ… Duplication Filter ë°ëª¨ ì™„ë£Œ!")
    print("ì‹¤ì œ í…ŒìŠ¤íŠ¸ëŠ” tutorial ë””ë ‰í† ë¦¬ì—ì„œ ìœ„ì˜ ëª…ë ¹ì–´ë“¤ì„ ì‹¤í–‰í•´ë³´ì„¸ìš”!")
