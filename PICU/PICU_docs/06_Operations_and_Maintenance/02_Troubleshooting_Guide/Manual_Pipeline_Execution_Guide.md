# ìˆ˜ë™ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê°€ì´ë“œ

GUI ì—†ì´ ê° íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  í…ŒìŠ¤íŠ¸í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)
2. [Hadoop/HDFS ì‹œì‘](#2-hadoophdfs-ì‹œì‘)
3. [Kafka ìƒíƒœ í™•ì¸](#3-kafka-ìƒíƒœ-í™•ì¸)
4. [ë°ì´í„° ìˆ˜ì§‘ (Scrapy) ì‹¤í–‰](#4-ë°ì´í„°-ìˆ˜ì§‘-scrapy-ì‹¤í–‰)
5. [Kafka ë°ì´í„° í™•ì¸](#5-kafka-ë°ì´í„°-í™•ì¸)
6. [HDFS ì €ì¥ í™•ì¸](#6-hdfs-ì €ì¥-í™•ì¸)
7. [Backend API ì‹¤í–‰](#7-backend-api-ì‹¤í–‰)
8. [ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰](#8-ì „ì²´-íŒŒì´í”„ë¼ì¸-í†µí•©-ì‹¤í–‰)

---

## 1. í™˜ê²½ ì„¤ì •

### 1.1 í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
```bash
cd /Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker
```

### 1.2 Python ê°€ìƒí™˜ê²½ í™œì„±í™”
```bash
source venv/bin/activate
```

### 1.3 PYTHONPATH ì„¤ì •
```bash
export PYTHONPATH=/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker:/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker/worker-nodes:$PYTHONPATH
```

### 1.4 Hadoop í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
```bash
export HADOOP_HOME=/Users/juns/code/personal/notion/pknu_workspace/bigdata/hadoop_project/hadoop-3.4.1
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export JAVA_HOME=$(/usr/libexec/java_home)
```

### 1.5 í™˜ê²½ ë³€ìˆ˜ í™•ì¸
```bash
echo "PYTHONPATH: $PYTHONPATH"
echo "HADOOP_HOME: $HADOOP_HOME"
echo "JAVA_HOME: $JAVA_HOME"
```

---

## 2. Hadoop/HDFS ì‹œì‘

### 2.1 í˜„ì¬ Java í”„ë¡œì„¸ìŠ¤ í™•ì¸
```bash
jps
```

**ì˜ˆìƒ ì¶œë ¥:**
```
74313 Kafka
81258 org.eclipse.equinox.launcher_1.7.100.v20251111-0406.jar
3793 Jps
```

### 2.2 HDFS ì‹œì‘
```bash
start-dfs.sh
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [OS.local]
```

### 2.3 HDFS í”„ë¡œì„¸ìŠ¤ í™•ì¸ (5ì´ˆ ëŒ€ê¸° í›„)
```bash
sleep 5
jps
```

**ì˜ˆìƒ ì¶œë ¥:**
```
3490 DataNode
3646 SecondaryNameNode
3374 NameNode
74313 Kafka
3793 Jps
```

### 2.4 HDFS í¬íŠ¸ í™•ì¸
```bash
nc -zv localhost 9000
nc -zv localhost 9870
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Connection to localhost port 9000 [tcp/cslistener] succeeded!
Connection to localhost port 9870 [tcp/*] succeeded!
```

### 2.5 HDFS íŒŒì¼ ì‹œìŠ¤í…œ í™•ì¸
```bash
hdfs dfs -ls /
```

**ì˜ˆìƒ ì¶œë ¥:** HDFS ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ì´ í‘œì‹œë©ë‹ˆë‹¤.

---

## 3. Kafka ìƒíƒœ í™•ì¸

### 3.1 Kafka í¬íŠ¸ í™•ì¸
```bash
nc -zv localhost 9092
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Connection to localhost port 9092 [tcp/XmlIpcRegSvc] succeeded!
```

### 3.2 Kafka í† í”½ ëª©ë¡ í™•ì¸
```bash
python -c "
from kafka.admin import KafkaAdminClient
try:
    admin_client = KafkaAdminClient(bootstrap_servers=['localhost:9092'])
    topics = admin_client.list_topics()
    print('Kafka Topics:')
    for topic in sorted(topics):
        print(f'  - {topic}')
    admin_client.close()
except Exception as e:
    print(f'Error: {e}')
"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Kafka Topics:
  - __consumer_offsets
  - analytics
  - bigdata
  - cointicker.raw.perplexity
  - cointicker.raw.saveticker
  - cointicker.raw.upbit_trends
  - datascience
  - test
```

---

## 4. ë°ì´í„° ìˆ˜ì§‘ (Scrapy) ì‹¤í–‰

### 4.1 Scrapy í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
```bash
cd worker-nodes/cointicker
```

### 4.2 ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤íŒŒì´ë” ëª©ë¡ í™•ì¸
```bash
scrapy list
```

**ì˜ˆìƒ ì¶œë ¥:**
```
cnn_fear_greed
coinness
perplexity
saveticker
upbit_trends
```

### 4.3 upbit_trends ìŠ¤íŒŒì´ë” ì‹¤í–‰ (5ê°œ ì•„ì´í…œë§Œ ìˆ˜ì§‘)
```bash
scrapy crawl upbit_trends -s CLOSESPIDER_ITEMCOUNT=5
```

**ì˜ˆìƒ ì¶œë ¥ (ì¤‘ìš” ë¶€ë¶„):**
```
2025-12-06 19:25:38 - cointicker.pipelines - INFO - HDFS Pipeline initialized for upbit_trends
2025-12-06 19:25:38 - cointicker.pipelines.kafka_pipeline - INFO - Kafka Pipeline initialized for upbit_trends
2025-12-06 19:25:38 - shared.kafka_client - INFO - Kafka Producer connected to localhost:9092 (compression=gzip, linger_ms=100)
2025-12-06 19:25:38 - shared.kafka_client - DEBUG - Message sent to topic=cointicker.raw.upbit_trends, partition=0, offset=387
2025-12-06 19:25:38 - shared.kafka_client - DEBUG - Message sent to topic=cointicker.raw.upbit_trends, partition=0, offset=388
...
2025-12-06 19:25:39 - cointicker.pipelines.kafka_pipeline - INFO - Sent 9/9 items to Kafka topic: cointicker.raw.upbit_trends
2025-12-06 19:25:46 - cointicker.pipelines - INFO - Saved 9 items to HDFS: /raw/upbit/20251206/upbit_20251206_192539.json
```

### 4.4 í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ëŒì•„ê°€ê¸°
```bash
cd ../..
```

---

## 5. Kafka ë°ì´í„° í™•ì¸

### 5.1 Kafka Consumerë¡œ ë©”ì‹œì§€ í™•ì¸ (ìµœì‹  ë©”ì‹œì§€ í™•ì¸)
```bash
python -c "
from shared.kafka_client import KafkaConsumerClient

# Kafka Consumer ì—°ê²°
consumer = KafkaConsumerClient(
    bootstrap_servers=['localhost:9092'],
    group_id='manual-test-consumer',
    auto_offset_reset='latest'
)

topics = ['cointicker.raw.upbit_trends']
if consumer.connect(topics):
    print('âœ… Kafka Consumer ì—°ê²° ì„±ê³µ')
    print(f'êµ¬ë… í† í”½: {topics}')
    print('ë©”ì‹œì§€ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘... (Ctrl+Cë¡œ ì¢…ë£Œ)')

    # ì°¸ê³ : consumer_timeout_ms ë²„ê·¸ë¡œ ì¸í•´ ë¬´í•œ ëŒ€ê¸°í•  ìˆ˜ ìˆìŒ
    # ìƒˆ ë©”ì‹œì§€ê°€ ë“¤ì–´ì˜¤ë©´ í‘œì‹œë¨

    consumer.close()
else:
    print('âŒ Kafka Consumer ì—°ê²° ì‹¤íŒ¨')
"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
âœ… Kafka Consumer ì—°ê²° ì„±ê³µ
êµ¬ë… í† í”½: ['cointicker.raw.upbit_trends']
```

---

## 6. HDFS ì €ì¥ í™•ì¸

### 6.1 HDFS upbit ë””ë ‰í† ë¦¬ í™•ì¸
```bash
hdfs dfs -ls /raw/upbit/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Found 1 items
drwxr-xr-x   - juns supergroup          0 2025-12-06 19:25 /raw/upbit/20251206
```

### 6.2 ì˜¤ëŠ˜ ë‚ ì§œ ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ í™•ì¸
```bash
hdfs dfs -ls /raw/upbit/20251206/
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Found 1 items
-rw-r--r--   1 juns supergroup       1969 2025-12-06 19:25 /raw/upbit/20251206/upbit_20251206_192539.json
```

### 6.3 HDFS íŒŒì¼ ë‚´ìš© í™•ì¸
```bash
hdfs dfs -cat /raw/upbit/20251206/upbit_20251206_192539.json 2>&1 | grep -v "WARN" | head -30
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
[
  {
    "source": "upbit",
    "symbol": "DOGE",
    "price": 209.0,
    "volume_24h": 198572882.33571494,
    "change_24h": 0.0,
    "market_cap": 41793058613.087906,
    "timestamp": "2025-12-06T19:25:38.710213"
  },
  {
    "source": "upbit",
    "symbol": "ETH",
    "price": 4540000.0,
    "volume_24h": 46409.00485314,
    "change_24h": 0.33,
    "market_cap": 212675905264.9422,
    "timestamp": "2025-12-06T19:25:38.710862"
  },
  ...
]
```

### 6.4 HDFSì˜ ëª¨ë“  raw ë°ì´í„° í™•ì¸
```bash
hdfs dfs -ls -R /raw/
```

---

## 7. Backend API ì‹¤í–‰

### 7.1 Backend ë””ë ‰í† ë¦¬ë¡œ ì´ë™
```bash
cd backend
```

### 7.2 Backend API ì„œë²„ ì‹œì‘ (í¬ê·¸ë¼ìš´ë“œ ì‹¤í–‰)
```bash
python -c "import uvicorn; uvicorn.run('app:app', host='0.0.0.0', port=5001, log_level='info')"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
INFO:     Started server process [7031]
INFO:     Waiting for application startup.
âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê³„ì† ì§„í–‰): (pymysql.err.OperationalError) (2003, "Can't connect to MySQL server on 'localhost' ([Errno 61] Connection refused)")
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:5001 (Press CTRL+C to quit)
```

### 7.3 ìƒˆ í„°ë¯¸ë„ì—ì„œ Health Check í™•ì¸
**ìƒˆ í„°ë¯¸ë„ ì—´ê¸°** â†’ ë‹¤ìŒ ëª…ë ¹ì–´ ì‹¤í–‰:

```bash
curl -s http://localhost:5001/health | python3 -m json.tool
```

**ì˜ˆìƒ ì¶œë ¥:**
```json
{
  "status": "healthy",
  "database": "disconnected ((pymysql.err.OperationalError) (2003, \"Can't connect to MySQL server on 'localhost' ([Errno 61] Connection refused)\"))",
  "timestamp": "2025-12-06T19:33:51.758246"
}
```

### 7.4 Backend API ì„œë²„ ì¤‘ì§€
ì›ë˜ í„°ë¯¸ë„ì—ì„œ `Ctrl+C` ëˆ„ë¥´ê¸°

### 7.5 í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ëŒì•„ê°€ê¸°
```bash
cd ..
```

---

## 8. ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ì‹¤í–‰

### 8.1 ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash

# 1. í™˜ê²½ ì„¤ì •
cd /Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker
source venv/bin/activate
export PYTHONPATH=/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker:/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker/worker-nodes:$PYTHONPATH
export HADOOP_HOME=/Users/juns/code/personal/notion/pknu_workspace/bigdata/hadoop_project/hadoop-3.4.1
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export JAVA_HOME=$(/usr/libexec/java_home)

echo "âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ"

# 2. HDFS ìƒíƒœ í™•ì¸ (í•„ìš”ì‹œ ì‹œì‘)
echo ""
echo "ğŸ“Š HDFS ìƒíƒœ í™•ì¸ ì¤‘..."
if ! nc -z localhost 9000 2>/dev/null; then
    echo "âš ï¸  HDFSê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œì‘í•©ë‹ˆë‹¤..."
    start-dfs.sh
    sleep 5
fi

if nc -z localhost 9000 2>/dev/null; then
    echo "âœ… HDFS ì •ìƒ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 9000)"
else
    echo "âŒ HDFS ì‹œì‘ ì‹¤íŒ¨"
    exit 1
fi

# 3. Kafka ìƒíƒœ í™•ì¸
echo ""
echo "ğŸ“Š Kafka ìƒíƒœ í™•ì¸ ì¤‘..."
if nc -z localhost 9092 2>/dev/null; then
    echo "âœ… Kafka ì •ìƒ ì‹¤í–‰ ì¤‘ (í¬íŠ¸ 9092)"
else
    echo "âŒ Kafkaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Kafkaë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”."
    exit 1
fi

# 4. Scrapy ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
echo ""
echo "ğŸ•·ï¸  Scrapy ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘..."
cd worker-nodes/cointicker
scrapy crawl upbit_trends -s CLOSESPIDER_ITEMCOUNT=10
cd ../..
echo "âœ… Scrapy ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ"

# 5. HDFS ë°ì´í„° í™•ì¸
echo ""
echo "ğŸ“ HDFS ì €ì¥ ë°ì´í„° í™•ì¸..."
TODAY=$(date +%Y%m%d)
hdfs dfs -ls /raw/upbit/${TODAY}/ 2>/dev/null
echo "âœ… HDFS ë°ì´í„° í™•ì¸ ì™„ë£Œ"

# 6. Backend API ì‹œì‘ (ì„ íƒì‚¬í•­)
echo ""
echo "ğŸš€ Backend APIë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:"
echo "   cd backend && python -c \"import uvicorn; uvicorn.run('app:app', host='0.0.0.0', port=5001, log_level='info')\""

echo ""
echo "âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!"
```

### 8.2 ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ë° ì‹¤í–‰

ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ `scripts/run_manual_pipeline.sh`ë¡œ ì €ì¥ í›„:

```bash
chmod +x scripts/run_manual_pipeline.sh
./scripts/run_manual_pipeline.sh
```

---

## 9. ë¬¸ì œ í•´ê²° (Troubleshooting)

### 9.1 HDFS ì—°ê²° ì‹¤íŒ¨ ì‹œ
```bash
# HDFS ë°ëª¬ ì¤‘ì§€
stop-dfs.sh

# ì ì‹œ ëŒ€ê¸°
sleep 3

# HDFS ë°ëª¬ ì¬ì‹œì‘
start-dfs.sh

# ìƒíƒœ í™•ì¸
jps
```

### 9.2 Kafka ì—°ê²° ì‹¤íŒ¨ ì‹œ
```bash
# Kafka ìƒíƒœ í™•ì¸
nc -zv localhost 9092

# Kafka ì¬ì‹œì‘ì´ í•„ìš”í•œ ê²½ìš°
# (Kafka ì„¤ì¹˜ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰)
# bin/kafka-server-stop.sh
# bin/kafka-server-start.sh -daemon config/server.properties
```

### 9.3 Scrapy ì‹¤í–‰ ì˜¤ë¥˜ ì‹œ
```bash
# PYTHONPATH ë‹¤ì‹œ ì„¤ì •
export PYTHONPATH=/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker:/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/cointicker/worker-nodes:$PYTHONPATH

# ëª¨ë“ˆ ì„í¬íŠ¸ í™•ì¸
python -c "from shared.kafka_client import KafkaProducerClient; print('âœ… ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ')"
```

### 9.4 Backend API ì‹¤í–‰ ì˜¤ë¥˜ ì‹œ
```bash
# PYTHONPATH í™•ì¸
echo $PYTHONPATH

# í¬íŠ¸ ì¶©ëŒ í™•ì¸
lsof -i :5001

# í¬íŠ¸ê°€ ì‚¬ìš© ì¤‘ì´ë©´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
# kill -9 <PID>
```

---

## 10. ì°¸ê³  ì‚¬í•­

### 10.1 ì¤‘ìš”í•œ í™˜ê²½ ë³€ìˆ˜
- `PYTHONPATH`: Python ëª¨ë“ˆ ê²½ë¡œ ì„¤ì •
- `HADOOP_HOME`: Hadoop ì„¤ì¹˜ ê²½ë¡œ
- `JAVA_HOME`: Java ì„¤ì¹˜ ê²½ë¡œ

### 10.2 ì£¼ìš” í¬íŠ¸
- **9000**: HDFS NameNode
- **9870**: HDFS Web UI
- **9092**: Kafka Broker
- **5001**: Backend API

### 10.3 ë°ì´í„° ê²½ë¡œ
- **HDFS ì›ë³¸ ë°ì´í„°**: `/raw/upbit/{ë‚ ì§œ}/upbit_{ë‚ ì§œ}_{ì‹œê°„}.json`
- **ë¡œì»¬ ë¡œê·¸**: `logs/scrapy.log`
- **Kafka í† í”½**: `cointicker.raw.upbit_trends`

### 10.4 ìˆ˜ì •ëœ ì´ìŠˆ
1. **kafka-python ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ê·¸**: âœ… ìˆ˜ì • ì™„ë£Œ - `consumer_timeout_ms=2147483647` (ë§¤ìš° í° ì •ìˆ˜ê°’) ì‚¬ìš©
2. **Kafka Consumer ì™€ì¼ë“œì¹´ë“œ**: âœ… ìˆ˜ì • ì™„ë£Œ - `admin_client.list_topics()` ë©”ì„œë“œ ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
3. **MySQL ì—°ê²°**: Backend APIëŠ” MySQL ì—†ì´ë„ ì‹¤í–‰ ê°€ëŠ¥ (ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì„ íƒì‚¬í•­)

---

## 11. ë‹¤ìŒ ë‹¨ê³„

íŒŒì´í”„ë¼ì¸ì´ ì •ìƒ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤ë©´:

1. **GUI ë¬¸ì œ ë””ë²„ê¹…**: `gui/modules/pipeline_orchestrator.py` ê²€í† 
2. **Kafka Consumer ìˆ˜ì •**: `worker-nodes/kafka/kafka_consumer.py`ì˜ íƒ€ì„ì•„ì›ƒ ì„¤ì • ìˆ˜ì •
3. **ìë™í™”**: ìœ„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ systemd ì„œë¹„ìŠ¤ë¡œ ë“±ë¡í•˜ì—¬ ìë™ ì‹œì‘

---

**ì‘ì„±ì¼**: 2025-12-06
**í…ŒìŠ¤íŠ¸ í™˜ê²½**: macOS, Python 3.14, Hadoop 3.4.1, Kafka (latest)
**ìƒíƒœ**: âœ… ê²€ì¦ ì™„ë£Œ
