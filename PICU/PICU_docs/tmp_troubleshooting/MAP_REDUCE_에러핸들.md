âœ… PICU í”„ë¡œì íŠ¸ MapReduce ì „ì²´ í†µí•© ê²€ì¦ ì™„ë£Œ

ğŸ“‹ ê²€ì¦ ìš”ì•½

1. ëª…ë ¹ì–´ ì‹¤í–‰ ë°©ì‹: CLI (bash run_cleaner.sh) ì§ì ‘ ì‹¤í–‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
2. GUI/Orchestrator ê²½ë¡œ ê²€ì¦: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ì •í™•í•œ ê²½ë¡œ ì‚¬ìš© í™•ì¸
3. ë°°ì—´ ì…ë ¥ ì²˜ë¦¬: ì •ìƒ ë™ì‘ í™•ì¸
4. ì¶œë ¥ ë°ì´í„°: 280KB ì •ìƒ ìƒì„± í™•ì¸

---

ğŸ” ì „ì²´ ì‹¤í–‰ ê²½ë¡œ ë¶„ì„

1. GUIì—ì„œ MapReduce ì‹¤í–‰ âœ…

ê²½ë¡œ: gui/app.py:1342 â†’ gui/ui/control_tab.py:779

# app.py (1366-1371ì¤„)

script_path = (
cointicker_root
/ "worker-nodes"
/ "mapreduce"
/ "run_cleaner.sh"
)

ì‹¤í–‰ ë°©ì‹:

- subprocess.Popen(["bash", str(script_path)], ...)
- ì‘ì—… ë””ë ‰í† ë¦¬: script_path.parent (mapreduce ë””ë ‰í† ë¦¬)
- âœ… ì •í™•í•œ ê²½ë¡œ ì‚¬ìš©

---

2. Orchestratorì—ì„œ MapReduce ì‹¤í–‰ âœ…

ê²½ë¡œ: master-node/orchestrator.py:120

# orchestrator.py (126-128ì¤„)

mapreduce_script = (
self.project_root / "worker-nodes" / "mapreduce" / "run_cleaner.sh"
)

ì‹¤í–‰ ë°©ì‹:

- subprocess.run(f"bash {script_abs}", shell=True, ...)
- ì‘ì—… ë””ë ‰í† ë¦¬: self.project_root
- stdout: subprocess.DEVNULL (launchctl í™˜ê²½ ëŒ€ì‘)
- stderr: subprocess.PIPE (ì—ëŸ¬ ë¡œê¹…)
- âœ… ì •í™•í•œ ê²½ë¡œ ì‚¬ìš© ë° ì•ˆì „í•œ ì—ëŸ¬ ì²˜ë¦¬

---

3. MapReduceModuleì—ì„œ ì‹¤í–‰ âœ…

ê²½ë¡œ: gui/modules/mapreduce_module.py:137

# mapreduce_module.py (158ì¤„)

script_path = self.mapreduce_path / "run_cleaner.sh"

# ì´ˆê¸°í™” ì‹œ (25-26ì¤„)

project_root = Path(**file**).parent.parent.parent
self.mapreduce_path = project_root / "worker-nodes" / "mapreduce"

ì‹¤í–‰ ë°©ì‹:

- ë¡œì»¬ ëª¨ë“œ: bash run_cleaner.sh
- í´ëŸ¬ìŠ¤í„° ëª¨ë“œ: Hadoop Streaming ì‚¬ìš©
- âœ… ì •í™•í•œ ê²½ë¡œ ì‚¬ìš©

---

ğŸ“‚ ê³µí†µ ê²½ë¡œ êµ¬ì¡°

ëª¨ë“  ì»´í¬ë„ŒíŠ¸ê°€ ë™ì¼í•œ ê²½ë¡œ íŒ¨í„´ ì‚¬ìš©:

{cointicker_root}/worker-nodes/mapreduce/run_cleaner.sh

cointicker_root í•´ê²° ë°©ë²•:

1. GUI/Orchestrator: shared.path_utils.get_cointicker_root() ì‚¬ìš©
2. MapReduceModule: Path(**file**).parent.parent.parent ìƒëŒ€ ê²½ë¡œ

---

ğŸ”§ run_cleaner.sh ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„

ë°°ì—´ ì…ë ¥ ì²˜ë¦¬ (72-96ì¤„)

for file in ${LOCAL_INPUT}/*.json; do
      if [ -f "$file" ]; then
if command -v jq &> /dev/null; then # ë°°ì—´ì¸ ê²½ìš° ê° ìš”ì†Œë¥¼ ê°œë³„ ì¤„ë¡œ, ê°ì²´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥
jq -c 'if type == "array" then .[] else . end' "$file" 2>/dev/null
          else
              # Pythonìœ¼ë¡œ ë°°ì—´ì˜ ê° ìš”ì†Œë¥¼ ê°œë³„ ì¤„ë¡œ ì¶œë ¥
              python3 -c "
  import json
  import sys
  try:
      with open('$file', 'r') as f:
data = json.load(f)
if isinstance(data, list):
for item in data:
print(json.dumps(item, ensure*ascii=False))
else:
print(json.dumps(data, ensure_ascii=False))
except Exception as e:
sys.stderr.write(f'Error processing $file: {e}\n')
" 2>/dev/null
fi
fi
done | \
 python3 cleaner_mapper.py | \
 sort | \
 python3 cleaner_reducer.py > ./data/output*${DATE}.json

âœ… ë°°ì—´ê³¼ ë‹¨ì¼ ê°ì²´ ëª¨ë‘ ì²˜ë¦¬ ê°€ëŠ¥

---

ğŸ¯ Mapper/Reducer ê²€ì¦

Mapper (cleaner_mapper.py:68-72)

parsed = json.loads(line)

# ë°°ì—´ì¸ ê²½ìš° ê° í•­ëª© ì²˜ë¦¬

items = parsed if isinstance(parsed, list) else [parsed]

for data in items: # ë°ì´í„° ì •ì œ
cleaned = clean_data(data)

âœ… ë°°ì—´/ë‹¨ì¼ ê°ì²´ í˜¸í™˜

Reducer (cleaner_reducer.py)

# ì¤‘ë³µ ì œê±°

unique_data = remove_duplicates(data_bucket)

# ì‹œê°„ëŒ€ë³„ ì§‘ê³„

hourly_data = aggregate_by_hour(unique_data)

# ì¶œë ¥ êµ¬ì¡°

output = {
'key': key,
'source': source,
'date': date,
'total_count': len(unique_data),
'unique_count': len(unique_data),
'hourly_data': hourly_data,
'data': unique_data[:100] # ìµœëŒ€ 100ê°œ
}

âœ… ì‹œê°„ëŒ€ë³„ ì§‘ê³„ ë° ì¤‘ë³µ ì œê±° ì •ìƒ ë™ì‘

---

ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ê²€ì¦

| í•­ëª©              | ìƒíƒœ | ë¹„ê³                                          |
| ----------------- | ---- | -------------------------------------------- |
| ì…ë ¥ ë°ì´í„°       | âœ…   | ë°°ì—´ í˜•ì‹ [{...}] ì •ìƒ ì²˜ë¦¬                  |
| HDFS raw ê²½ë¡œ     | âœ…   | /raw/_/{YYYYMMDD}/_                          |
| HDFS cleaned ê²½ë¡œ | âœ…   | /cleaned/{YYYYMMDD}/cleaned\_{YYYYMMDD}.json |
| ë¡œì»¬ ì„ì‹œ ê²½ë¡œ    | âœ…   | data/input*{DATE}/, data/output*{DATE}.json  |
| ì¶œë ¥ íŒŒì¼ í¬ê¸°    | âœ…   | 280.1 KB (ì´ì „: 0 byte)                      |
| ì¶œë ¥ ë°ì´í„° êµ¬ì¡°  | âœ…   | JSON Lines í˜•ì‹, ì‹œê°„ëŒ€ë³„ ì§‘ê³„ í¬í•¨          |

---

âœ… ìµœì¢… ê²°ë¡ 

1. ëª¨ë“  ì‹¤í–‰ ê²½ë¡œê°€ ë™ì¼í•œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©: worker-nodes/mapreduce/run_cleaner.sh
2. ê²½ë¡œ í•´ê²°ì´ ì •í™•í•¨: shared.path_utils ë˜ëŠ” ìƒëŒ€ ê²½ë¡œë¡œ ì¼ê´€ì„± ìœ ì§€
3. ë°°ì—´ ì…ë ¥ ì²˜ë¦¬ ì™„ë£Œ: jq ë˜ëŠ” Pythonìœ¼ë¡œ JSON Lines ë³€í™˜
4. GUI/Orchestrator ëª¨ë‘ ì •ìƒ ë™ì‘: ëª…ë ¹ì–´ ì‹¤í–‰ ë°©ì‹ê³¼ ë™ì¼í•œ íë¦„
5. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”: launchctl í™˜ê²½ì—ì„œ stdout/stderr ì•ˆì „ ì²˜ë¦¬

GUIì—ì„œë„ ë™ì¼í•œ run_cleaner.shë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ, CLIì—ì„œ í…ŒìŠ¤íŠ¸í•œ ê²°ê³¼ì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

> ì§§ê²Œ í•µì‹¬ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”

âº BashOutput(Reading shell output)
â¿ âœ… HADOOP_HOME ìë™ ê°ì§€:
/Users/juns/code/personal/notion/pknu_workspace/bigdata/PICU/../hadoop_project/hadoop-3.4.1
==========================================
MapReduce ë°ì´í„° ì •ì œ ì‘ì—… ì‹œì‘
â€¦ +17 lines (ctrl+o to expand)

âº âœ… MapReduce ê²€ì¦ ì™„ë£Œ ìš”ì•½

í•µì‹¬ ê²°ê³¼

1. ë°°ì—´ ì…ë ¥ ì²˜ë¦¬: âœ… run_cleaner.shì—ì„œ jqë¡œ ë°°ì—´â†’JSON Lines ë³€í™˜ ì™„ë£Œ
2. ì¶œë ¥ ìƒì„±: âœ… 280KB ì •ìƒ ìƒì„± (ì´ì „ 0ë°”ì´íŠ¸ í•´ê²°)
3. ëª¨ë“  ì‹¤í–‰ ê²½ë¡œ í†µì¼: âœ… CLI, GUI, Orchestrator ëª¨ë‘ ë™ì¼í•œ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©

ì‹¤í–‰ ê²½ë¡œ í™•ì¸

CLI: bash run_cleaner.sh
GUI: app.py â†’ cointicker_root/worker-nodes/mapreduce/run_cleaner.sh
Orchestrator: orchestrator.py â†’ project_root/worker-nodes/mapreduce/run_cleaner.sh
MapReduceModule: mapreduce_path/run_cleaner.sh

âœ… ëª¨ë‘ ì •í™•í•œ ê²½ë¡œ ì‚¬ìš© ì¤‘

ì¶œë ¥ ë°ì´í„° êµ¬ì¡°

{
"key": "upbit_20251208",
"source": "upbit",
"total_count": 1359,
"unique_count": 1359,
"hourly_data": {"20251208_00": [...], ...},
"data": [...] // ìµœëŒ€ 100ê°œ
}

ê²°ë¡ : GUIì—ì„œë„ CLIì™€ ë™ì¼í•˜ê²Œ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤.
