# 파이프라인 아키텍처 설계

상태: ✅ 완료 (2025-12-08 업데이트)

# PICU 2-Tier 데이터 파이프라인 아키텍처

## 1. 시스템 아키텍처 개요

### 1.1 2-Tier 아키텍처

PICU 프로젝트는 **2-Tier 아키텍처**를 채택하여 라즈베리파이의 리소스 제약을 극복하면서도 확장 가능한 시스템을 구현합니다.

```
┌─────────────────────────────────────────────────────────────┐
│  Tier 1: 라즈베리파이 클러스터 (4대)                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Master Node (라즈베리파이 1번)                              │
│  ├─ Hadoop NameNode (HDFS 메타데이터 관리)                  │
│  ├─ YARN ResourceManager (작업 스케줄링)                    │
│  ├─ Orchestrator (파이프라인 오케스트레이션)                 │
│  └─ Scrapyd Scheduler (크롤링 작업 스케줄링)                 │
│                                                             │
│  Worker Nodes (라즈베리파이 2,3,4번)                        │
│  ├─ Worker 1: Upbit + Perplexity 크롤링                    │
│  ├─ Worker 2: Coinness + CNN 크롤링                         │
│  ├─ Worker 3: SaveTicker 크롤링                            │
│  └─ Hadoop DataNode (HDFS 블록 저장)                        │
│                                                             │
│  데이터 흐름 (Tier 1 내부):                                  │
│  Scrapy Spider → HDFSPipeline → HDFS (/raw/)               │
│  ↓                                                          │
│  MapReduce 정제 → HDFS (/cleaned/)                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
                    │
                    │ [Tier 1 → Tier 2 전송]
                    │ SSH 또는 HDFS 클라이언트
                    │ (hdfs dfs -get 또는 HDFSClient.get())
                    ↓
┌─────────────────────────────────────────────────────────────┐
│  Tier 2: 외부 서버 (일반 PC/Mac)                             │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  DataLoader (backend/services/data_loader.py)               │
│  ├─ HDFS에서 /cleaned/ 데이터 다운로드                      │
│  ├─ JSON 파싱                                               │
│  └─ PostgreSQL 적재                                         │
│                                                             │
│  저장 담당:                                                  │
│  ├─ raw_news 테이블                                         │
│  ├─ market_trends 테이블                                    │
│  ├─ fear_greed_index 테이블                                 │
│  ├─ sentiment_analysis 테이블                               │
│  ├─ technical_indicators 테이블                             │
│  └─ crypto_insights 테이블                                  │
│                                                             │
│  FastAPI Backend                                            │
│  React Frontend                                             │
│  GUI 통합 관리 시스템 (PyQt5/tkinter)                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 핵심 컴포넌트

#### Tier 1 컴포넌트

1. **Orchestrator** (`master-node/orchestrator.py`)
   - 전체 파이프라인 오케스트레이션
   - 크롤링 작업 스케줄링 (2분마다)
   - 전체 파이프라인 실행 (5분마다)
   - 공포·탐욕 지수 수집 (매일 자정)

2. **Scrapyd Scheduler** (`master-node/scheduler.py`)
   - Scrapyd 서버 관리 및 자동 시작
   - Spider 스케줄링 (spider_config.yaml 기반)
   - 프로젝트 자동 배포

3. **Scrapy Spiders** (`worker-nodes/cointicker/spiders/`)
   - upbit_trends: 업비트 시장 트렌드
   - saveticker: 세이브티커 뉴스
   - coinness: 코인니스 뉴스
   - perplexity: Perplexity Finance 뉴스
   - cnn_fear_greed: CNN 공포·탐욕 지수

4. **HDFSPipeline** (`worker-nodes/cointicker/pipelines.py`)
   - 크롤링 데이터를 HDFS에 저장
   - 경로: `/raw/{source}/{date}/*.json`

5. **MapReduce** (`worker-nodes/mapreduce/`)
   - 데이터 정제 및 중복 제거
   - 시간대별 집계
   - 경로: `/cleaned/{date}/aggregated_*.json`

#### Tier 2 컴포넌트

1. **Tier 2 Scheduler** (`scripts/run_pipeline_scheduler.py`)
   - HDFS → PostgreSQL 적재 스케줄링 (30분마다)
   - systemd 서비스로 실행 가능

2. **DataLoader** (`backend/services/data_loader.py`)
   - HDFS에서 정제된 데이터 다운로드
   - JSON 파싱 및 타입별 분류
   - PostgreSQL 적재 (중복 체크)

3. **FastAPI Backend** (`backend/app.py`)
   - RESTful API 제공
   - 대시보드 데이터 조회
   - 인사이트 생성

4. **React Frontend** (`frontend/`)
   - 실시간 대시보드
   - 데이터 시각화

5. **GUI 통합 관리 시스템** (`gui/`)
   - 모든 모듈 통합 관리
   - 클러스터 실시간 모니터링
   - 파이프라인 제어
   - 설정 중앙 관리

## 2. 데이터 흐름

### 2.1 전체 파이프라인 흐름

```
[Tier 1: 라즈베리파이 클러스터]
┌─────────────────────────────────────────────┐
│ 1. Scrapy Spider (Worker 1,2,3)            │
│    ↓ 크롤링                                  │
│ 2. ValidationPipeline                       │
│    ↓ 검증                                    │
│ 3. DuplicatesPipeline                       │
│    ↓ 중복 제거                               │
│ 4. HDFSPipeline                             │
│    ↓ HDFS 저장                               │
│ 5. HDFS /raw/upbit/20251208/*.json         │
│    ↓ MapReduce 정제                         │
│ 6. HDFS /cleaned/20251208/aggregated_*.json │
└─────────────────────────────────────────────┘
                    │
                    │ [Tier 1 → Tier 2 전송]
                    │ SSH 또는 HDFS 클라이언트
                    │ (hdfs dfs -get 또는 HDFSClient.get())
                    ↓
[Tier 2: 외부 서버]
┌─────────────────────────────────────────────┐
│ 7. DataLoader.load_from_hdfs()            │
│    ↓ HDFS에서 /cleaned/ 데이터 다운로드     │
│ 8. JSON 파싱                               │
│    ↓ 타입별 분류 (뉴스/트렌드/지수)        │
│ 9. PostgreSQL 적재                         │
│    ↓ 중복 체크 후 저장                      │
│ 10. raw_news, market_trends,              │
│     fear_greed_index, sentiment_analysis, │
│     technical_indicators, crypto_insights  │
└─────────────────────────────────────────────┘
```

### 2.2 스케줄링 흐름

```
┌─────────────────────────────────────────────┐
│ Orchestrator (Tier 1)                       │
│ - 2분마다: 크롤링 작업 실행                  │
│ - 5분마다: 전체 파이프라인 실행              │
│ - 매일 자정: 공포·탐욕 지수 수집             │
└─────────────────────────────────────────────┘
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ Scrapyd Scheduler (Tier 1)                  │
│ - Spider 스케줄링 (spider_config.yaml 기반) │
│ - Scrapyd 서버 관리                          │
└─────────────────────────────────────────────┘
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ Scrapy Spiders                               │
│ - 크롤링 실행                                 │
│ - HDFS 저장                                  │
└─────────────────────────────────────────────┘
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ MapReduce                                    │
│ - 데이터 정제                                │
│ - HDFS 저장                                  │
└─────────────────────────────────────────────┘
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ Tier 2 Scheduler (Tier 2)                   │
│ - 30분마다: HDFS → PostgreSQL 적재          │
└─────────────────────────────────────────────┘
                    │
                    ↓
┌─────────────────────────────────────────────┐
│ DataLoader                                   │
│ - HDFS 데이터 다운로드                       │
│ - PostgreSQL 적재                           │
└─────────────────────────────────────────────┘
```

## 3. 핵심 컴포넌트 상세

### 3.1 Orchestrator

**위치**: `cointicker/master-node/orchestrator.py`

**주요 기능**:
- 크롤링 작업 스케줄링 (2분마다)
- 전체 파이프라인 실행 (5분마다)
- 공포·탐욕 지수 수집 (매일 자정)
- Spider 실행 및 모니터링
- MapReduce 작업 실행
- DB 적재 작업 실행

**스케줄 설정**:
```python
# 2분마다: 실시간 데이터 크롤링
schedule.every(2).minutes.do(lambda: orchestrator.run_crawlers())

# 5분마다: 전체 파이프라인 (크롤링 → MapReduce → DB 적재)
schedule.every(5).minutes.do(lambda: orchestrator.run_full_pipeline())

# 매일 자정: 공포·탐욕 지수
schedule.every().day.at("00:00").do(run_fear_greed)
```

### 3.2 Scrapyd Scheduler

**위치**: `cointicker/master-node/scheduler.py`

**주요 기능**:
- Scrapyd 서버 자동 시작 및 관리
- 프로젝트 자동 배포
- Spider 스케줄링 (spider_config.yaml 기반)
- Scrapyd 연결 상태 모니터링

**스케줄 설정**:
- `spider_config.yaml`에서 Spider별 스케줄 로드
- 기본값: 5분마다 (upbit_trends, saveticker), 10분마다 (coinness), 1시간마다 (perplexity), 매일 (cnn_fear_greed)

### 3.3 Tier 2 Scheduler

**위치**: `cointicker/scripts/run_pipeline_scheduler.py`

**주요 기능**:
- HDFS → PostgreSQL 적재 스케줄링 (30분마다)
- systemd 서비스로 실행 가능
- 파이프라인 작업 실행 및 모니터링

**스케줄 설정**:
```python
# 30분마다 실행
schedule.every(30).minutes.do(run_pipeline_job)
```

### 3.4 DataLoader

**위치**: `cointicker/backend/services/data_loader.py`

**주요 기능**:
- HDFS에서 정제된 데이터 다운로드
- JSON 파싱 및 타입별 분류
- PostgreSQL 적재 (중복 체크)
- 데이터 검증

**데이터 타입별 처리**:
- 뉴스 데이터 → `raw_news` 테이블
- 시장 트렌드 → `market_trends` 테이블
- 공포·탐욕 지수 → `fear_greed_index` 테이블
- 감성 분석 → `sentiment_analysis` 테이블
- 기술적 지표 → `technical_indicators` 테이블
- 인사이트 → `crypto_insights` 테이블

## 4. GUI 통합 관리 시스템

### 4.1 주요 기능

**위치**: `cointicker/gui/`

**핵심 기능**:
1. **모듈 통합 관리**
   - Spider, MapReduce, HDFS, Backend, Frontend, Orchestrator, Scheduler 통합 관리
   - 모듈별 시작/중지/재시작
   - 의존성 자동 관리

2. **클러스터 모니터링**
   - 라즈베리파이 노드 상태 실시간 모니터링
   - CPU, 메모리, 디스크 사용률 추적
   - Hadoop/HDFS 상태 확인
   - Scrapy 프로세스 모니터링

3. **Tier2 서버 관리**
   - FastAPI 백엔드 서버 헬스 체크
   - 대시보드 데이터 조회
   - 인사이트 생성 및 관리

4. **파이프라인 제어**
   - Spider 시작/중지
   - MapReduce 작업 실행
   - 파이프라인 오케스트레이터 제어
   - 스케줄러 관리

5. **설정 관리**
   - 중앙 집중식 설정 관리
   - YAML/JSON 설정 파일 지원
   - 설정 유효성 검사

6. **설치 마법사**
   - 의존성 자동 설치
   - 가상환경 자동 생성
   - 시스템 의존성 확인 및 설치

### 4.2 아키텍처

```
gui/
├── core/                    # 핵심 모듈
│   ├── module_manager.py    # 모듈 매니저
│   ├── config_manager.py    # 설정 관리자
│   ├── cache_manager.py     # 캐시 관리자
│   ├── retry_utils.py       # 재시도 유틸리티
│   └── timing_config.py     # 타이밍 설정
├── modules/                 # 기능 모듈
│   ├── spider_module.py     # Spider 관리
│   ├── mapreduce_module.py # MapReduce 관리
│   ├── hdfs_module.py       # HDFS 관리
│   ├── backend_module.py   # Backend 관리
│   ├── pipeline_module.py  # 파이프라인 관리
│   ├── pipeline_orchestrator.py # 파이프라인 오케스트레이터
│   └── process_monitor.py   # 프로세스 모니터
├── ui/                      # UI 탭 컴포넌트
│   ├── dashboard_tab.py     # 대시보드 탭
│   ├── cluster_tab.py       # 클러스터 탭
│   ├── tier2_tab.py         # Tier2 서버 탭
│   ├── modules_tab.py      # 모듈 관리 탭
│   ├── control_tab.py      # 제어 탭
│   └── config_tab.py        # 설정 탭
├── monitors/                # 모니터링 모듈
│   ├── cluster_monitor.py   # 클러스터 모니터링
│   └── tier2_monitor.py     # Tier2 서버 모니터링
└── installer/               # 설치 마법사
    ├── installer.py         # 설치 로직
    ├── installer_cli.py     # CLI 설치
    └── installer_gui.py    # GUI 설치
```

## 5. 데이터베이스 스키마

### 5.1 PostgreSQL 테이블 구조

**기본 데이터베이스**: PostgreSQL (MariaDB도 지원하지만 기본은 PostgreSQL)

**주요 테이블**:

1. **raw_news**
   - 뉴스 원본 데이터
   - 컬럼: id, source, title, url, content, published_at, keywords, collected_at

2. **market_trends**
   - 시장 트렌드 데이터
   - 컬럼: id, source, symbol, volume_24h, price, change_24h, timestamp

3. **fear_greed_index**
   - 공포·탐욕 지수
   - 컬럼: id, value, classification, timestamp

4. **sentiment_analysis**
   - 감성 분석 결과
   - 컬럼: id, news_id, sentiment_score, sentiment_label, confidence, analyzed_at

5. **technical_indicators**
   - 기술적 지표
   - 컬럼: id, symbol, timestamp, rsi, macd, macd_signal, bb_upper, bb_middle, bb_lower, volume

6. **crypto_insights**
   - 암호화폐 인사이트
   - 컬럼: id, insight_type, symbol, description, severity, related_news, created_at

## 6. 배포 및 운영

### 6.1 서비스 관리

**Tier 1 서비스**:
- Orchestrator 서비스: `systemctl start/stop orchestrator`
- Scrapyd 서비스: `systemctl start/stop scrapyd`

**Tier 2 서비스**:
- Tier 2 Scheduler 서비스: `systemctl start/stop tier2-scheduler`
- FastAPI Backend: `uvicorn backend.app:app --host 0.0.0.0 --port 5000`
- React Frontend: `npm run dev`

### 6.2 모니터링

**로그 위치**:
- Orchestrator: `cointicker/logs/orchestrator.log`
- Scheduler: `cointicker/logs/scheduler.log`
- Scrapyd: `cointicker/logs/scrapyd.log`

**모니터링 방법**:
1. GUI 애플리케이션에서 실시간 모니터링
2. 터미널에서 로그 모니터링: `bash scripts/monitor_logs.sh`
3. DB 상태 확인: `python scripts/check_db_status.py`

## 7. 확장성 고려사항

### 7.1 수평 확장

- **워커 노드 추가**: 새로운 라즈베리파이를 워커 노드로 추가
- **Spider 분산**: Spider를 여러 워커 노드에 분산 배치
- **HDFS 확장**: DataNode 추가로 저장 용량 확장

### 7.2 수직 확장

- **하드웨어 업그레이드**: 라즈베리파이 4/5로 교체
- **저장소 확장**: HDFS 용량 증설
- **네트워크 최적화**: 유선 연결, 전용 VLAN

## 8. 장애 대응

### 8.1 자동 복구 메커니즘

- **프로세스 모니터링**: systemd watchdog 활용
- **네트워크 장애**: 재시도 로직 + 백오프 전략
- **DB 연결 실패**: 로컬 큐잉 + 배치 동기화
- **SD카드 보호**: tmpfs 활용, 로그 순환

### 8.2 모니터링 및 알람

- **GUI 통합 모니터링**: 실시간 상태 확인
- **로그 모니터링**: 자동 로그 분석
- **DB 상태 확인**: 정기적 데이터 품질 체크

---

**최종 업데이트**: 2025-12-08
**버전**: 2.0 (2-Tier 아키텍처 기반)
