# PICU ê¸°ìˆ  ìŠ¤íƒ êµ¬í˜„ ìƒì„¸ ë¬¸ì„œ

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-12-08

PICU í”„ë¡œì íŠ¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ(Selenium, Kafka, Hadoop, Scrapy)ì˜ êµ¬í˜„ ë°©ì‹ì„ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“Š í”„ë¡œì„¸ìŠ¤ë³„ ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½

### ê¸°ìˆ  ìŠ¤íƒ ë§¤íŠ¸ë¦­ìŠ¤

| í”„ë¡œì„¸ìŠ¤        | ì£¼ìš” ê¸°ìˆ          | ë³´ì¡° ê¸°ìˆ             | êµ¬í˜„ ìœ„ì¹˜                         | ì—­í•                           |
| --------------- | ----------------- | -------------------- | --------------------------------- | ----------------------------- |
| **ë°ì´í„° ìˆ˜ì§‘** | Scrapy 2.11+      | Selenium 4.15+       | `worker-nodes/cointicker/`        | ì›¹ í¬ë¡¤ë§ ë° ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ |
| **ë©”ì‹œì§•**      | Kafka 2.x         | kafka-python 2.0+    | `shared/kafka_client.py`          | ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°        |
| **ë¶„ì‚° ì €ì¥**   | Hadoop HDFS 3.4.1 | pyarrow, hdfs CLI    | `shared/hdfs_client.py`           | ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì‚° ì €ì¥       |
| **ë°ì´í„° ì •ì œ** | MapReduce         | Hadoop YARN          | `worker-nodes/mapreduce/`         | ë°ì´í„° ì •ì œ ë° ì§‘ê³„           |
| **ë°ì´í„° ì ì¬** | PostgreSQL        | psycopg2-binary      | `backend/services/data_loader.py` | ì •ì œëœ ë°ì´í„° DB ì ì¬         |
| **API ì„œë²„**    | FastAPI 0.110+    | SQLAlchemy, Pydantic | `backend/app.py`                  | RESTful API ì œê³µ              |
| **í”„ë¡ íŠ¸ì—”ë“œ**  | React 18+         | Vite, TypeScript     | `frontend/`                       | ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ               |
| **í†µí•© ê´€ë¦¬**   | PyQt5/tkinter     | Python 3.8+          | `gui/`                            | ëª¨ë“  ëª¨ë“ˆ í†µí•© ê´€ë¦¬           |

### ê¸°ìˆ  ìŠ¤íƒ ê³„ì¸µ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    í†µí•© ê´€ë¦¬ ê³„ì¸µ (Tier 2)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GUI ì• í”Œë¦¬ì¼€ì´ì…˜ (PyQt5/tkinter)                                â”‚
â”‚  â”œâ”€ ëª¨ë“ˆ í†µí•© ê´€ë¦¬                                                â”‚
â”‚  â”œâ”€ í´ëŸ¬ìŠ¤í„° ëª¨ë‹ˆí„°ë§                                             â”‚
â”‚  â””â”€ íŒŒì´í”„ë¼ì¸ ì œì–´                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    í”„ë¡ íŠ¸ì—”ë“œ ê³„ì¸µ (Tier 2)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  React 18+ (Vite, TypeScript)                                    â”‚
â”‚  â”œâ”€ ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ                                              â”‚
â”‚  â”œâ”€ ë°ì´í„° ì‹œê°í™”                                                 â”‚
â”‚  â””â”€ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API ê³„ì¸µ (Tier 2)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FastAPI 0.110+                                                  â”‚
â”‚  â”œâ”€ RESTful API                                                  â”‚
â”‚  â”œâ”€ SQLAlchemy (ORM)                                             â”‚
â”‚  â””â”€ Pydantic (ë°ì´í„° ê²€ì¦)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° ì ì¬ ê³„ì¸µ (Tier 2)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL (psycopg2-binary)                                   â”‚
â”‚  â”œâ”€ raw_news, market_trends, fear_greed_index                   â”‚
â”‚  â”œâ”€ sentiment_analysis, technical_indicators                    â”‚
â”‚  â””â”€ crypto_insights                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° ì •ì œ ê³„ì¸µ (Tier 1)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MapReduce (Hadoop YARN)                                         â”‚
â”‚  â”œâ”€ ì¤‘ë³µ ì œê±°                                                    â”‚
â”‚  â”œâ”€ ì‹œê°„ëŒ€ë³„ ì§‘ê³„                                                â”‚
â”‚  â””â”€ í˜•ì‹ í†µì¼                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë©”ì‹œì§• ê³„ì¸µ (Tier 1)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kafka 2.x (kafka-python 2.0+)                                  â”‚
â”‚  â”œâ”€ Producer: Scrapy â†’ Kafka                                     â”‚
â”‚  â”œâ”€ Consumer: Kafka â†’ HDFS/Backend                              â”‚
â”‚  â””â”€ Topics: cointicker.raw.*, cointicker.processed.*           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë¶„ì‚° ì €ì¥ ê³„ì¸µ (Tier 1)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Hadoop HDFS 3.4.1                                              â”‚
â”‚  â”œâ”€ Java API (pyarrow.fs) ìš°ì„                                    â”‚
â”‚  â”œâ”€ CLI í´ë°± (hdfs dfs)                                          â”‚
â”‚  â””â”€ ê²½ë¡œ: /raw/{source}/{date}/*.json                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ë°ì´í„° ìˆ˜ì§‘ ê³„ì¸µ (Tier 1)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Scrapy 2.11+                                                    â”‚
â”‚  â”œâ”€ 5ê°œ Spider (upbit_trends, saveticker, coinness, etc.)      â”‚
â”‚  â”œâ”€ Pipeline: Validation â†’ Duplicates â†’ HDFS â†’ Kafka           â”‚
â”‚  â””â”€ Middleware: UserAgent, Selenium                             â”‚
â”‚                                                                  â”‚
â”‚  Selenium 4.15+ (ë™ì  ì½˜í…ì¸  ì²˜ë¦¬)                                â”‚
â”‚  â”œâ”€ Chrome WebDriver                                             â”‚
â”‚  â”œâ”€ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ                                                â”‚
â”‚  â””â”€ ê°ì§€ ìš°íšŒ ê¸°ëŠ¥                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í”„ë¡œì„¸ìŠ¤ë³„ ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸

#### 1. ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapy Spider                           â”‚
â”‚  â”œâ”€ upbit_trends.py                     â”‚
â”‚  â”œâ”€ saveticker.py                       â”‚
â”‚  â”œâ”€ coinness.py                         â”‚
â”‚  â”œâ”€ perplexity.py                       â”‚
â”‚  â””â”€ cnn_fear_greed.py                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Selenium Middleware (ì„ íƒì )            â”‚
â”‚  â”œâ”€ Chrome WebDriver                    â”‚
â”‚  â”œâ”€ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ                        â”‚
â”‚  â””â”€ ë™ì  ì½˜í…ì¸  ë Œë”ë§                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapy Pipeline                        â”‚
â”‚  â”œâ”€ ValidationPipeline (300)            â”‚
â”‚  â”œâ”€ DuplicatesPipeline (400)           â”‚
â”‚  â”œâ”€ HDFSPipeline (500)                  â”‚
â”‚  â””â”€ KafkaPipeline (600)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê¸°ìˆ  ìŠ¤íƒ**:

- **Scrapy 2.11+**: ì›¹ í¬ë¡¤ë§ í”„ë ˆì„ì›Œí¬
- **Selenium 4.15+**: ë™ì  ì½˜í…ì¸  ì²˜ë¦¬
- **webdriver-manager**: ChromeDriver ìë™ ê´€ë¦¬
- **itemloaders**: ë°ì´í„° ì „ì²˜ë¦¬

#### 2. ë©”ì‹œì§• í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Producer                          â”‚
â”‚  â”œâ”€ KafkaProducerClient                 â”‚
â”‚  â”œâ”€ ë°°ì¹˜ ì „ì†¡ (10ê°œ ë‹¨ìœ„)                â”‚
â”‚  â”œâ”€ ì••ì¶• (gzip)                         â”‚
â”‚  â””â”€ ì¬ì‹œë„ (3íšŒ)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topics                           â”‚
â”‚  â”œâ”€ cointicker.raw.upbit_trends         â”‚
â”‚  â”œâ”€ cointicker.raw.saveticker           â”‚
â”‚  â”œâ”€ cointicker.raw.coinness             â”‚
â”‚  â””â”€ cointicker.raw.* (ì™€ì¼ë“œì¹´ë“œ)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Consumer                         â”‚
â”‚  â”œâ”€ KafkaConsumerClient                 â”‚
â”‚  â”œâ”€ ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì§€ì›                 â”‚
â”‚  â”œâ”€ ìë™ ì˜¤í”„ì…‹ ê´€ë¦¬                     â”‚
â”‚  â””â”€ HDFS ì €ì¥ ë˜ëŠ” Backend ì²˜ë¦¬          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê¸°ìˆ  ìŠ¤íƒ**:

- **Kafka 2.x**: ë¶„ì‚° ë©”ì‹œì§• ì‹œìŠ¤í…œ
- **kafka-python 2.0+**: Python Kafka í´ë¼ì´ì–¸íŠ¸
- **KafkaAdminClient**: í† í”½ ê´€ë¦¬

#### 3. ë¶„ì‚° ì €ì¥ í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HDFSClient (í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼)            â”‚
â”‚  â”œâ”€ Java API (pyarrow.fs) ìš°ì„            â”‚
â”‚  â”‚  â””â”€ HadoopFileSystem.from_uri()      â”‚
â”‚  â””â”€ CLI í´ë°± (hdfs dfs)                 â”‚
â”‚     â””â”€ subprocess ì‹¤í–‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HDFSUploadManager                      â”‚
â”‚  â”œâ”€ ìë™ ì¬ì—…ë¡œë“œ                       â”‚
â”‚  â”œâ”€ ì¬ì‹œë„ ë¡œì§ (3íšŒ, ë°±ì˜¤í”„)            â”‚
â”‚  â”œâ”€ í—¬ìŠ¤ ì²´í¬ (300ì´ˆ ê°„ê²©)               â”‚
â”‚  â””â”€ ë¡œì»¬ ì„ì‹œ ì €ì¥                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hadoop HDFS 3.4.1                     â”‚
â”‚  â”œâ”€ NameNode (í¬íŠ¸: 9000, ì›¹ UI: 9870) â”‚
â”‚  â”œâ”€ DataNode (ë³µì œ: 3íšŒ)                â”‚
â”‚  â””â”€ ê²½ë¡œ: /raw/{source}/{date}/*.json  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê¸°ìˆ  ìŠ¤íƒ**:

- **Hadoop HDFS 3.4.1**: ë¶„ì‚° íŒŒì¼ ì‹œìŠ¤í…œ
- **pyarrow**: Java FileSystem API ë˜í¼
- **hdfs CLI**: í´ë°± ëª¨ë“œ

#### 4. ë°ì´í„° ì •ì œ í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MapReduce ì‘ì—…                          â”‚
â”‚  â”œâ”€ Mapper: ë°ì´í„° ì •ì œ                  â”‚
â”‚  â”œâ”€ Reducer: ì§‘ê³„ ë° ì¤‘ë³µ ì œê±°          â”‚
â”‚  â””â”€ ì…ë ¥: /raw/{source}/{date}/*.json   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hadoop YARN                            â”‚
â”‚  â”œâ”€ ResourceManager                    â”‚
â”‚  â”œâ”€ NodeManager                        â”‚
â”‚  â””â”€ ì‘ì—… ìŠ¤ì¼€ì¤„ë§                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HDFS ì •ì œëœ ë°ì´í„°                     â”‚
â”‚  â””â”€ ê²½ë¡œ: /cleaned/{date}/aggregated_*.json â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê¸°ìˆ  ìŠ¤íƒ**:

- **Hadoop MapReduce**: ë¶„ì‚° ë°ì´í„° ì²˜ë¦¬
- **Hadoop YARN**: ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
- **Java/Python**: MapReduce ì‘ì—… êµ¬í˜„

#### 5. ë°ì´í„° ì ì¬ í”„ë¡œì„¸ìŠ¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataLoader                            â”‚
â”‚  â”œâ”€ HDFSì—ì„œ ì •ì œëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ     â”‚
â”‚  â”œâ”€ JSON íŒŒì‹±                          â”‚
â”‚  â””â”€ íƒ€ì…ë³„ ë¶„ë¥˜                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL ì ì¬                        â”‚
â”‚  â”œâ”€ raw_news                           â”‚
â”‚  â”œâ”€ market_trends                      â”‚
â”‚  â”œâ”€ fear_greed_index                   â”‚
â”‚  â”œâ”€ sentiment_analysis                 â”‚
â”‚  â”œâ”€ technical_indicators               â”‚
â”‚  â””â”€ crypto_insights                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ê¸°ìˆ  ìŠ¤íƒ**:

- **PostgreSQL**: ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤
- **psycopg2-binary**: PostgreSQL ë“œë¼ì´ë²„
- **SQLAlchemy**: ORM (ì„ íƒì )

### ì „ì²´ ì•„í‚¤í…ì²˜ ì‹œê°í™”

```mermaid
graph TB
    subgraph "Tier 1: ë¼ì¦ˆë² ë¦¬íŒŒì´ í´ëŸ¬ìŠ¤í„°"
        subgraph "ë°ì´í„° ìˆ˜ì§‘"
            S1[Scrapy Spider<br/>upbit_trends]
            S2[Scrapy Spider<br/>saveticker]
            S3[Scrapy Spider<br/>coinness]
            S4[Scrapy Spider<br/>perplexity]
            S5[Scrapy Spider<br/>cnn_fear_greed]
            SEL[Selenium Middleware<br/>ë™ì  ì½˜í…ì¸ ]
        end

        subgraph "Pipeline"
            VP[ValidationPipeline]
            DP[DuplicatesPipeline]
            HP[HDFSPipeline]
            KP[KafkaPipeline]
        end

        subgraph "ë©”ì‹œì§•"
            KF[Kafka<br/>Producer/Consumer]
            KT[Kafka Topics<br/>cointicker.raw.*]
        end

        subgraph "ë¶„ì‚° ì €ì¥"
            HDFS[Hadoop HDFS 3.4.1<br/>NameNode/DataNode]
            HCL[HDFSClient<br/>Java API/CLI]
        end

        subgraph "ë°ì´í„° ì •ì œ"
            MR[MapReduce<br/>ì •ì œ ë° ì§‘ê³„]
            YARN[Hadoop YARN<br/>ResourceManager]
        end
    end

    subgraph "Tier 2: ì™¸ë¶€ ì„œë²„"
        subgraph "ë°ì´í„° ì ì¬"
            DL[DataLoader<br/>HDFS â†’ PostgreSQL]
            PG[PostgreSQL<br/>6ê°œ í…Œì´ë¸”]
        end

        subgraph "API"
            API[FastAPI<br/>RESTful API]
        end

        subgraph "í”„ë¡ íŠ¸ì—”ë“œ"
            REACT[React 18+<br/>ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ]
        end

        subgraph "í†µí•© ê´€ë¦¬"
            GUI[GUI ì• í”Œë¦¬ì¼€ì´ì…˜<br/>PyQt5/tkinter]
        end
    end

    S1 --> SEL
    S2 --> SEL
    S3 --> SEL
    S4 --> SEL
    S5 --> SEL

    SEL --> VP
    VP --> DP
    DP --> HP
    DP --> KP

    HP --> HCL
    HCL --> HDFS

    KP --> KF
    KF --> KT
    KT --> KF

    HDFS --> MR
    MR --> YARN
    YARN --> HDFS

    HDFS --> DL
    DL --> PG

    PG --> API
    API --> REACT

    GUI --> S1
    GUI --> S2
    GUI --> S3
    GUI --> KF
    GUI --> HDFS
    GUI --> API
```

### ê¸°ìˆ  ìŠ¤íƒ ì˜ì¡´ì„±

```
Python 3.8+
â”‚
â”œâ”€ Scrapy 2.11+
â”‚  â”œâ”€ requests 2.32+
â”‚  â”œâ”€ lxml 6.0+
â”‚  â”œâ”€ itemloaders 1.3+
â”‚  â””â”€ Selenium 4.15+ (ì„ íƒ)
â”‚     â””â”€ webdriver-manager 4.0+
â”‚
â”œâ”€ Kafka
â”‚  â””â”€ kafka-python 2.0+
â”‚
â”œâ”€ Hadoop HDFS
â”‚  â”œâ”€ pyarrow (Java API)
â”‚  â””â”€ hdfs CLI (í´ë°±)
â”‚
â”œâ”€ PostgreSQL
â”‚  â””â”€ psycopg2-binary 2.9+
â”‚
â”œâ”€ FastAPI 0.110+
â”‚  â”œâ”€ SQLAlchemy
â”‚  â””â”€ Pydantic
â”‚
â”œâ”€ React 18+
â”‚  â”œâ”€ Vite
â”‚  â””â”€ TypeScript
â”‚
â””â”€ GUI
   â”œâ”€ PyQt5 (ìš°ì„ )
   â””â”€ tkinter (í´ë°±)
```

## ğŸ“‹ ëª©ì°¨

1. [Scrapy êµ¬í˜„](#1-scrapy-êµ¬í˜„)
2. [Selenium êµ¬í˜„](#2-selenium-êµ¬í˜„)
3. [Kafka êµ¬í˜„](#3-kafka-êµ¬í˜„)
4. [Hadoop HDFS êµ¬í˜„](#4-hadoop-hdfs-êµ¬í˜„)
5. [í†µí•© íŒŒì´í”„ë¼ì¸](#5-í†µí•©-íŒŒì´í”„ë¼ì¸)

---

## 1. Scrapy êµ¬í˜„

### 1.1 ê°œìš”

ScrapyëŠ” PICU í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ë°ì´í„° ìˆ˜ì§‘ ì—”ì§„ìœ¼ë¡œ, 5ê°œì˜ Spiderë¥¼ í†µí•´ ì•”í˜¸í™”í ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

**êµ¬í˜„ ìœ„ì¹˜**: `cointicker/worker-nodes/cointicker/`

### 1.2 Spider ëª©ë¡

#### 1.2.1 Upbit Trends Spider

**íŒŒì¼**: `spiders/upbit_trends.py`

**ê¸°ëŠ¥**:

- Upbit ê±°ë˜ì†Œì˜ ì‹œì¥ íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘
- ì£¼ìš” ì•”í˜¸í™”í(BTC, ETH, XRP, ADA ë“±)ì˜ ì‹¤ì‹œê°„ ê°€ê²© ì •ë³´
- 24ì‹œê°„ ê±°ë˜ëŸ‰ ë° ë³€ë™ë¥  ìˆ˜ì§‘

**êµ¬í˜„ íŠ¹ì§•**:

```python
class UpbitTrendsSpider(scrapy.Spider):
    name = "upbit_trends"
    allowed_domains = ["upbit.com"]

    start_urls = [
        "https://api.upbit.com/v1/market/all",  # ì „ì²´ ë§ˆì¼“ ëª©ë¡
    ]

    def parse(self, response):
        # JSON API ì‘ë‹µ íŒŒì‹±
        data = json.loads(response.text)

        # KRW ë§ˆì¼“ë§Œ í•„í„°ë§
        krw_markets = [m for m in data if m.get("market", "").startswith("KRW-")]

        # ë°°ì¹˜ë¡œ ìš”ì²­ (ìµœëŒ€ 100ê°œì”©)
        for batch in chunks(krw_markets, 100):
            ticker_url = f"https://api.upbit.com/v1/ticker?markets={','.join(batch)}"
            yield scrapy.Request(ticker_url, callback=self.parse_ticker)
```

**ìŠ¤ì¼€ì¤„ë§**: 5ë¶„ë§ˆë‹¤ (spider_config.yaml ì„¤ì •)

#### 1.2.2 SaveTicker Spider

**íŒŒì¼**: `spiders/saveticker.py`

**ê¸°ëŠ¥**:

- ì„¸ì´ë¸Œí‹°ì»¤ ë‰´ìŠ¤ ìˆ˜ì§‘
- ì•”í˜¸í™”í ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§

**ìŠ¤ì¼€ì¤„ë§**: 5ë¶„ë§ˆë‹¤

#### 1.2.3 Coinness Spider

**íŒŒì¼**: `spiders/coinness.py`

**ê¸°ëŠ¥**:

- ì½”ì¸ë‹ˆìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘
- ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ë¥¼ ìœ„í•´ Selenium ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš© ê°€ëŠ¥

**ìŠ¤ì¼€ì¤„ë§**: 10ë¶„ë§ˆë‹¤

#### 1.2.4 Perplexity Spider

**íŒŒì¼**: `spiders/perplexity.py`

**ê¸°ëŠ¥**:

- Perplexity Finance ë‰´ìŠ¤ ìˆ˜ì§‘
- AI ê¸°ë°˜ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

**ìŠ¤ì¼€ì¤„ë§**: 1ì‹œê°„ë§ˆë‹¤

#### 1.2.5 CNN Fear & Greed Index Spider

**íŒŒì¼**: `spiders/cnn_fear_greed.py`

**ê¸°ëŠ¥**:

- CNN ê³µí¬Â·íƒìš• ì§€ìˆ˜ ìˆ˜ì§‘
- ì‹œì¥ ì‹¬ë¦¬ ì§€í‘œ ìˆ˜ì§‘

**ìŠ¤ì¼€ì¤„ë§**: ë§¤ì¼ ìì •

### 1.3 Pipeline êµ¬ì¡°

**íŒŒì¼**: `pipelines/__init__.py`

#### 1.3.1 ValidationPipeline

**ì—­í• **: ë°ì´í„° ìœ íš¨ì„± ê²€ì¦

```python
class ValidationPipeline:
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not adapter.get('source'):
            raise DropItem(f"Missing source in {item}")

        # JSON ìœ íš¨ì„± ê²€ì¦
        if not validate_json(dict(adapter)):
            raise DropItem(f"Invalid JSON in {item}")

        return item
```

#### 1.3.2 DuplicatesPipeline

**ì—­í• **: ì¤‘ë³µ ë°ì´í„° ì œê±°

```python
class DuplicatesPipeline:
    def __init__(self):
        self.seen = set()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)

        # ì½˜í…ì¸  í•´ì‹œ ìƒì„±
        content_hash = self._generate_content_hash(adapter)

        if content_hash in self.seen:
            raise DropItem(f"Duplicate item: {item}")

        self.seen.add(content_hash)
        return item
```

#### 1.3.3 HDFSPipeline

**ì—­í• **: HDFSì— ë°ì´í„° ì €ì¥

**êµ¬í˜„ íŠ¹ì§•**:

- ë°°ì¹˜ ì²˜ë¦¬ (100ê°œ ë‹¨ìœ„)
- ìë™ ì¬ì—…ë¡œë“œ ê¸°ëŠ¥
- ë¡œì»¬ ì„ì‹œ ì €ì¥ í›„ HDFS ì—…ë¡œë“œ

```python
class HDFSPipeline:
    def __init__(self):
        self.hdfs_client = None
        self.upload_manager = None
        self.items = []
        self.batch_size = 100

    def open_spider(self, spider):
        # HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        namenode = spider.settings.get("HDFS_NAMENODE", "hdfs://localhost:9000")
        self.hdfs_client = HDFSClient(namenode=namenode)

        # ìë™ ì—…ë¡œë“œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.upload_manager = HDFSUploadManager(
            hdfs_client=self.hdfs_client,
            temp_dir="data/temp",
            max_retries=3,
        )
        self.upload_manager.start_auto_upload()

    def process_item(self, item, spider):
        self.items.append(dict(ItemAdapter(item)))

        # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì €ì¥
        if len(self.items) >= self.batch_size:
            self._save_batch(spider)
            self.items = []

        return item

    def _save_batch(self, spider):
        # ì—…ë¡œë“œ ë§¤ë‹ˆì €ë¥¼ í†µí•´ ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        success = self.upload_manager.save_to_hdfs(
            items=self.items,
            source=self.items[0].get("source", "unknown"),
            date=datetime.now(),
        )
```

#### 1.3.4 KafkaPipeline

**ì—­í• **: Kafkaë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°

**íŒŒì¼**: `pipelines/kafka_pipeline.py`

```python
class KafkaPipeline:
    def __init__(self):
        self.producer = None
        self.items = []
        self.batch_size = 10
        self.topic_prefix = "cointicker"

    def open_spider(self, spider):
        # Kafka Producer ì´ˆê¸°í™”
        bootstrap_servers = spider.settings.get("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
        self.producer = KafkaProducerClient(
            bootstrap_servers=[s.strip() for s in bootstrap_servers.split(",")],
            timeout=10,
        )
        self.producer.connect()

    def process_item(self, item, spider):
        item_dict = dict(ItemAdapter(item))
        item_dict["_spider"] = spider.name
        item_dict["_collected_at"] = datetime.now().isoformat()

        self.items.append(item_dict)

        # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì „ì†¡
        if len(self.items) >= self.batch_size:
            self._send_batch(spider)
            self.items = []

        return item

    def _send_batch(self, spider):
        topic = f"{self.topic_prefix}.raw.{spider.name}"
        for item in self.items:
            key = f"{item.get('source', 'unknown')}_{item.get('timestamp', '')}"
            self.producer.send(topic, item, key=key)
```

### 1.4 Middleware êµ¬ì¡°

**íŒŒì¼**: `middlewares.py`

#### 1.4.1 RotateUserAgentMiddleware

**ì—­í• **: User-Agent íšŒì „ìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€

```python
class RotateUserAgentMiddleware(UserAgentMiddleware):
    def __init__(self, user_agent_list=None):
        self.user_agent_list = user_agent_list or []

    def process_request(self, request, spider):
        if self.user_agent_list:
            ua = random.choice(self.user_agent_list)
            request.headers["User-Agent"] = ua
        return None
```

#### 1.4.2 SeleniumMiddleware

**ì—­í• **: ë™ì  ì½˜í…ì¸  ì²˜ë¦¬ë¥¼ ìœ„í•œ Selenium í†µí•©

```python
class SeleniumMiddleware:
    def __init__(self, enabled_domains=None, headless=True, scroll=False):
        self.enabled_domains = enabled_domains or []
        self.headless = headless
        self.scroll = scroll
        self.driver = None

    def process_request(self, request, spider):
        # Selenium ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_selenium = request.meta.get("selenium", False)

        # ë„ë©”ì¸ ê¸°ë°˜ í™•ì¸
        if not use_selenium and self.enabled_domains:
            parsed_url = urlparse(request.url)
            use_selenium = parsed_url.netloc in self.enabled_domains

        if not use_selenium:
            return None  # ë‹¤ìŒ ë¯¸ë“¤ì›¨ì–´ë¡œ ì „ë‹¬

        # WebDriver ì´ˆê¸°í™”
        if self.driver is None:
            self.driver = create_chrome_driver(
                headless=self.headless,
                disable_blink=True,
            )

        # í˜ì´ì§€ ë¡œë“œ
        self.driver.get(request.url)
        wait_for_page_load(self.driver, timeout=30)

        # ìŠ¤í¬ë¡¤ (ë™ì  ì½˜í…ì¸  ë¡œë“œìš©)
        if self.scroll:
            scroll_to_bottom(self.driver, pause_time=1.0)

        # HTML ê°€ì ¸ì˜¤ê¸°
        body = self.driver.page_source.encode("utf-8")
        return HtmlResponse(
            url=request.url, body=body, encoding="utf-8", request=request
        )
```

### 1.5 ì„¤ì • íŒŒì¼

**íŒŒì¼**: `settings.py`

**ì£¼ìš” ì„¤ì •**:

```python
# Pipeline ì„¤ì •
ITEM_PIPELINES = {
    "cointicker.pipelines.ValidationPipeline": 300,
    "cointicker.pipelines.DuplicatesPipeline": 400,
    "cointicker.pipelines.HDFSPipeline": 500,
    "cointicker.pipelines.kafka_pipeline.KafkaPipeline": 600,
}

# Selenium ì„¤ì •
SELENIUM_ENABLED_DOMAINS = [
    'coinness.live',
    'perplexity.ai',
]
SELENIUM_HEADLESS = True
SELENIUM_SCROLL = True

# HDFS ì„¤ì •
HDFS_NAMENODE = "hdfs://localhost:9000"
HDFS_MAX_RETRIES = 3

# Kafka ì„¤ì •
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
KAFKA_TOPIC_PREFIX = "cointicker"
```

---

## 2. Selenium êµ¬í˜„

### 2.1 ê°œìš”

Seleniumì€ JavaScriptë¡œ ë Œë”ë§ë˜ëŠ” ë™ì  ì½˜í…ì¸ ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

**êµ¬í˜„ ìœ„ì¹˜**: `cointicker/shared/selenium_utils.py`

### 2.2 í•µì‹¬ ê¸°ëŠ¥

#### 2.2.1 ChromeDriver ê´€ë¦¬

```python
def get_chromedriver_path() -> str:
    """
    ChromeDriver ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    1. í™˜ê²½ë³€ìˆ˜ CHROMEDRIVER_PATH í™•ì¸
    2. ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œëœ ê²½ë¡œ í™•ì¸
    3. ì—†ìœ¼ë©´ webdriver-managerë¡œ ìë™ ë‹¤ìš´ë¡œë“œ
    """
    # 1. í™˜ê²½ë³€ìˆ˜ í™•ì¸
    env_path = os.environ.get("CHROMEDRIVER_PATH")
    if env_path and os.path.exists(env_path):
        return env_path

    # 2. ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œëœ ê²½ë¡œ í™•ì¸
    predefined_paths = [
        "~/.wdm/drivers/chromedriver/mac64/*/chromedriver-mac-arm64/chromedriver",
        "~/.wdm/drivers/chromedriver/linux64/*/chromedriver",
    ]

    for pattern in predefined_paths:
        matches = glob.glob(os.path.expanduser(pattern))
        if matches:
            return matches[0]

    # 3. webdriver-managerë¡œ ìë™ ë‹¤ìš´ë¡œë“œ
    if WEBDRIVER_MANAGER_AVAILABLE:
        return ChromeDriverManager().install()

    raise RuntimeError("ChromeDriverë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

#### 2.2.2 Chrome WebDriver ìƒì„±

```python
def create_chrome_driver(
    headless: bool = True,
    disable_blink: bool = True,
    custom_user_agent: Optional[str] = None,
    window_size: tuple = (1920, 1080),
    implicit_wait: int = 10,
) -> webdriver.Chrome:
    """Chrome WebDriverë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    options = Options()

    # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
    if headless:
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")

    # AutomationControlled ë¹„í™œì„±í™” (ê°ì§€ ìš°íšŒ)
    if disable_blink:
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

    # User-Agent ì„¤ì •
    if custom_user_agent:
        options.add_argument(f"user-agent={custom_user_agent}")
    else:
        default_ua = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        options.add_argument(f"user-agent={default_ua}")

    # ChromeDriver ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
    chromedriver_path = get_chromedriver_path()
    service = Service(chromedriver_path)

    # WebDriver ìƒì„±
    driver = webdriver.Chrome(service=service, options=options)

    # navigator.webdriver ë¹„í™œì„±í™” (ê°ì§€ ìš°íšŒ)
    if disable_blink:
        driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                """
            },
        )

    driver.implicitly_wait(implicit_wait)
    return driver
```

#### 2.2.3 ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

**í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°**:

```python
def wait_for_page_load(driver: webdriver.Chrome, timeout: int = 30) -> bool:
    """í˜ì´ì§€ ë¡œë“œê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤."""
    try:
        WebDriverWait(driver, timeout).until(
            lambda d: d.execute_script("return document.readyState") == "complete"
        )
        return True
    except TimeoutException:
        return False
```

**ìŠ¤í¬ë¡¤ ì²˜ë¦¬**:

```python
def scroll_to_bottom(driver: webdriver.Chrome, pause_time: float = 1.0) -> None:
    """í˜ì´ì§€ë¥¼ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•©ë‹ˆë‹¤ (ë™ì  ì½˜í…ì¸  ë¡œë“œìš©)."""
    last_height = driver.execute_script("return document.body.scrollHeight")

    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause_time)

        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height
```

**iframe ì²˜ë¦¬**:

```python
def switch_to_iframe(
    driver: webdriver.Chrome,
    iframe_selector: str,
    by: By = By.ID,
    timeout: int = 10,
) -> bool:
    """iframeìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤."""
    try:
        iframe = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((by, iframe_selector))
        )
        driver.switch_to.frame(iframe)
        return True
    except TimeoutException:
        return False
```

### 2.3 Scrapy í†µí•©

Seleniumì€ Scrapyì˜ Downloader Middlewareë¡œ í†µí•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

```python
# settings.py
DOWNLOADER_MIDDLEWARES = {
    'cointicker.middlewares.SeleniumMiddleware': 543,
}

# Spiderì—ì„œ ì‚¬ìš©
def parse(self, response):
    # request.metaì— selenium=True ì„¤ì •
    yield scrapy.Request(
        url,
        callback=self.parse_page,
        meta={'selenium': True}  # Selenium ì‚¬ìš©
    )
```

---

## 3. Kafka êµ¬í˜„

### 3.1 ê°œìš”

KafkaëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ ì‚¬ìš©ë˜ë©°, Scrapyì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

**êµ¬í˜„ ìœ„ì¹˜**: `cointicker/shared/kafka_client.py`

### 3.2 Kafka Producer

#### 3.2.1 KafkaProducerClient

```python
class KafkaProducerClient(KafkaClient):
    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
        acks: str = "all",
        retries: int = 3,
        compression_type: str = "gzip",
        linger_ms: int = 100,
    ):
        super().__init__(bootstrap_servers, timeout)

        # ê¸°ë³¸ ì§ë ¬í™” í•¨ìˆ˜
        self.value_serializer = lambda v: json.dumps(v, ensure_ascii=False).encode("utf-8")
        self.key_serializer = lambda k: k.encode("utf-8") if k else None

        self.acks = acks
        self.retries = retries
        self.compression_type = compression_type
        self.linger_ms = linger_ms

    def connect(self) -> bool:
        """Producer ì—°ê²°"""
        producer_config = {
            "bootstrap_servers": self.bootstrap_servers,
            "value_serializer": self.value_serializer,
            "key_serializer": self.key_serializer,
            "acks": self.acks,
            "retries": self.retries,
            "compression_type": self.compression_type,
            "linger_ms": self.linger_ms,
        }

        self.producer = KafkaProducer(**producer_config)
        return True

    def send(
        self,
        topic: str,
        value: Any,
        key: Optional[str] = None,
        partition: Optional[int] = None,
    ) -> bool:
        """ë©”ì‹œì§€ ì „ì†¡"""
        try:
            future = self.producer.send(
                topic,
                value=value,
                key=key,
                partition=partition,
            )
            record_metadata = future.get(timeout=self.timeout)
            return True
        except KafkaError as e:
            logger.error(f"Kafka send error: {e}")
            return False
```

#### 3.2.2 ë°°ì¹˜ ì „ì†¡

```python
def send_batch(self, topic: str, messages: List[Dict[str, Any]]) -> int:
    """ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡"""
    success_count = 0
    for msg in messages:
        key = msg.get("key")
        value = msg.get("value")
        if self.send(topic, value, key):
            success_count += 1
    return success_count
```

### 3.3 Kafka Consumer

#### 3.3.1 KafkaConsumerClient

```python
class KafkaConsumerClient(KafkaClient):
    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        group_id: str = "cointicker-consumer",
        auto_offset_reset: str = "earliest",
        enable_auto_commit: bool = True,
    ):
        super().__init__(bootstrap_servers, timeout)

        self.group_id = group_id
        self.auto_offset_reset = auto_offset_reset
        self.enable_auto_commit = enable_auto_commit

        # ê¸°ë³¸ ì—­ì§ë ¬í™” í•¨ìˆ˜
        self.value_deserializer = lambda v: json.loads(v.decode("utf-8")) if v else None
        self.key_deserializer = lambda k: k.decode("utf-8") if k else None

    def connect(self, topics: List[str], max_retries: int = 3) -> bool:
        """Consumer ì—°ê²° (ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì§€ì›)"""
        # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì²˜ë¦¬
        pattern_topics = [t for t in topics if "*" in t or "?" in t]
        direct_topics = [t for t in topics if t not in pattern_topics]

        if pattern_topics:
            # AdminClientë¡œ íŒ¨í„´ ë§¤ì¹­ í† í”½ ì°¾ê¸°
            admin_client = KafkaAdminClient(
                bootstrap_servers=self.bootstrap_servers,
            )
            all_topics = admin_client.list_topics()

            # íŒ¨í„´ ë§¤ì¹­
            matched_topics = []
            for pattern_str in pattern_topics:
                pattern_regex = pattern_str.replace(".", r"\.").replace("*", ".*").replace("?", ".")
                compiled_pattern = re.compile(f"^{pattern_regex}$")
                for topic in all_topics:
                    if compiled_pattern.match(topic):
                        matched_topics.append(topic)

            # Consumer ìƒì„±
            self.consumer = KafkaConsumer(
                bootstrap_servers=self.bootstrap_servers,
                group_id=self.group_id,
                auto_offset_reset=self.auto_offset_reset,
                enable_auto_commit=self.enable_auto_commit,
                value_deserializer=self.value_deserializer,
                key_deserializer=self.key_deserializer,
                consumer_timeout_ms=2147483647,  # ë¬´í•œ ëŒ€ê¸°
            )

            # ë§¤ì¹­ëœ í† í”½ êµ¬ë…
            if matched_topics:
                self.consumer.subscribe(topics=matched_topics)
            else:
                # íŒ¨í„´ êµ¬ë… (ìƒˆ í† í”½ ìë™ ê°ì§€)
                kafka_pattern = f"^{pattern_str.replace('.', r'\\.').replace('*', '.*')}$"
                self.consumer.subscribe(pattern=kafka_pattern)

        return True

    def consume(self, callback=None, max_messages: Optional[int] = None):
        """ë©”ì‹œì§€ ì†Œë¹„"""
        message_count = 0

        while True:
            try:
                # poll()ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
                message_batch = self.consumer.poll(timeout_ms=1000)

                if not message_batch:
                    continue

                # ë°°ì¹˜ì˜ ê° ë©”ì‹œì§€ ì²˜ë¦¬
                for topic_partition, messages in message_batch.items():
                    for message in messages:
                        if callback:
                            callback(message)
                        else:
                            logger.info(f"Received message: topic={message.topic}")

                        message_count += 1
                        if max_messages and message_count >= max_messages:
                            return

            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"Error consuming messages: {e}")
                continue
```

### 3.4 ì„¤ì • íŒŒì¼

**íŒŒì¼**: `config/kafka_config.yaml`

```yaml
kafka:
  bootstrap_servers:
    - "localhost:9092"

  topics:
    raw_prefix: "cointicker.raw"
    processed_prefix: "cointicker.processed"
    insights_prefix: "cointicker.insights"

  producer:
    acks: "all"
    retries: 3
    batch_size: 10
    timeout: 10

  consumer:
    group_id: "cointicker-consumer"
    auto_offset_reset: "earliest"
    enable_auto_commit: true
    timeout: 10

pipeline:
  kafka_enabled: true
  hdfs_enabled: true
  kafka_first: true
```

### 3.5 Kafka Consumer ì„œë¹„ìŠ¤

**íŒŒì¼**: `worker-nodes/kafka/kafka_consumer.py`

```python
class KafkaConsumerService:
    def __init__(self):
        self.consumer = None
        self.hdfs_client = None

    def start(self):
        """Consumer ì‹œì‘"""
        # Kafka Consumer ì´ˆê¸°í™”
        self.consumer = KafkaConsumerClient(
            bootstrap_servers=["localhost:9092"],
            group_id="cointicker-consumer",
        )

        # í† í”½ êµ¬ë…
        self.consumer.connect(topics=["cointicker.raw.*"])

        # HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.hdfs_client = HDFSClient(namenode="hdfs://localhost:9000")

        # ë©”ì‹œì§€ ì†Œë¹„ ì‹œì‘
        self.consumer.consume(callback=self.process_message)

    def process_message(self, message):
        """ë©”ì‹œì§€ ì²˜ë¦¬"""
        data = message.value

        # HDFSì— ì €ì¥
        hdfs_path = f"/raw/{data.get('source')}/{datetime.now().strftime('%Y%m%d')}/{message.offset}.json"
        self.hdfs_client.put(json.dumps(data), hdfs_path)
```

---

## 4. Hadoop HDFS êµ¬í˜„

### 4.1 ê°œìš”

Hadoop HDFSëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë¶„ì‚° ì €ì¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

**êµ¬í˜„ ìœ„ì¹˜**: `cointicker/shared/hdfs_client.py`

### 4.2 HDFSClient êµ¬í˜„

#### 4.2.1 í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹

HDFSClientëŠ” Java ê¸°ë°˜ APIë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ CLIë¡œ í´ë°±í•©ë‹ˆë‹¤:

```python
class HDFSClient:
    def __init__(
        self,
        namenode: str = "hdfs://localhost:9000",
        use_java: bool = True,
    ):
        self.namenode = namenode
        self.hadoop_home = self._get_hadoop_home()
        self.use_java = use_java and PYARROW_AVAILABLE
        self.fs = None

        # Java ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œë„
        if self.use_java and pafs:
            try:
                self.fs = pafs.HadoopFileSystem.from_uri(namenode)
                logger.info(f"Java-based HDFS client initialized: {namenode}")
            except Exception as e:
                logger.warning(f"Failed to initialize Java-based HDFS client: {e}. Falling back to CLI.")
                self.use_java = False
```

#### 4.2.2 íŒŒì¼ ì—…ë¡œë“œ

```python
def put(self, local_path: str, hdfs_path: str) -> bool:
    """ë¡œì»¬ íŒŒì¼ì„ HDFSì— ì—…ë¡œë“œ"""
    # ë¶€ëª¨ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
    parent_dir = str(Path(hdfs_path).parent)
    if parent_dir and not self.exists(parent_dir):
        self.mkdir(parent_dir)

    # Java ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì‹œë„
    if self.use_java and self.fs:
        try:
            with open(local_path, "rb") as local_file:
                with self.fs.open_output_stream(hdfs_path) as hdfs_file:
                    hdfs_file.write(local_file.read())
            logger.info(f"HDFS ì—…ë¡œë“œ ì„±ê³µ (Java): {local_path} -> {hdfs_path}")
            return True
        except Exception as e:
            logger.warning(f"Java-based upload failed: {e}. Falling back to CLI.")

    # CLI í´ë°±
    command = f"hdfs dfs -put {local_path} {hdfs_path}"
    success, stdout, stderr = self._run_command(command)
    return success
```

#### 4.2.3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ

```python
def get(self, hdfs_path: str, local_path: str) -> bool:
    """HDFS íŒŒì¼ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œ"""
    # Java ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì‹œë„
    if self.use_java and self.fs:
        try:
            with self.fs.open_input_stream(hdfs_path) as hdfs_file:
                with open(local_path, "wb") as local_file:
                    local_file.write(hdfs_file.read())
            logger.info(f"HDFS ë‹¤ìš´ë¡œë“œ ì„±ê³µ (Java): {hdfs_path} -> {local_path}")
            return True
        except Exception as e:
            logger.warning(f"Java-based download failed: {e}. Falling back to CLI.")

    # CLI í´ë°±
    command = f"hdfs dfs -get {hdfs_path} {local_path}"
    success, stdout, stderr = self._run_command(command)
    return success
```

#### 4.2.4 ë””ë ‰í† ë¦¬ ë° íŒŒì¼ ëª©ë¡

```python
def list_files(self, hdfs_path: str) -> List[str]:
    """HDFS ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    # Java ê¸°ë°˜ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì‹œë„
    if self.use_java and self.fs:
        try:
            file_infos = self.fs.get_file_info(hdfs_path)
            if file_infos.is_file:
                return [hdfs_path]
            else:
                files = []
                for file_info in self.fs.get_file_info_selector(hdfs_path):
                    files.append(file_info.path)
                return files
        except Exception as e:
            logger.warning(f"Java-based list failed: {e}. Falling back to CLI.")

    # CLI í´ë°±
    command = f"hdfs dfs -ls {hdfs_path}"
    success, stdout, stderr = self._run_command(command)
    if success:
        # stdout íŒŒì‹±í•˜ì—¬ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ
        files = []
        for line in stdout.strip().split('\n')[1:]:  # ì²« ì¤„ì€ í—¤ë”
            parts = line.split()
            if len(parts) >= 8:
                files.append(parts[-1])
        return files
    return []
```

### 4.3 HDFSUploadManager

**íŒŒì¼**: `shared/hdfs_upload_manager.py`

**ì—­í• **: ìë™ ì¬ì—…ë¡œë“œ ê¸°ëŠ¥ ì œê³µ

```python
class HDFSUploadManager:
    def __init__(
        self,
        hdfs_client: HDFSClient,
        temp_dir: str = "data/temp",
        max_retries: int = 3,
        initial_delay: float = 2.0,
        backoff_factor: float = 2.0,
        health_check_interval: int = 300,
    ):
        self.hdfs_client = hdfs_client
        self.temp_dir = Path(temp_dir)
        self.max_retries = max_retries
        self.initial_delay = initial_delay
        self.backoff_factor = backoff_factor
        self.health_check_interval = health_check_interval

        self.pending_files = []
        self.auto_upload_thread = None
        self.stop_event = threading.Event()

    def save_to_hdfs(
        self,
        items: List[Dict],
        source: str,
        date: datetime,
    ) -> bool:
        """ë°ì´í„°ë¥¼ HDFSì— ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        # ë¡œì»¬ ì„ì‹œ íŒŒì¼ì— ì €ì¥
        date_str = date.strftime("%Y%m%d")
        temp_dir = self.temp_dir / date_str
        temp_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        temp_file = temp_dir / f"{source}_{timestamp}.json"

        with open(temp_file, "w", encoding="utf-8") as f:
            json.dump(items, f, ensure_ascii=False, indent=2)

        # HDFS ê²½ë¡œ ìƒì„±
        hdfs_path = f"/raw/{source}/{date_str}/{temp_file.name}"

        # HDFS ì—…ë¡œë“œ ì‹œë„
        for attempt in range(self.max_retries):
            if self.hdfs_client.put(str(temp_file), hdfs_path):
                # ì„±ê³µ ì‹œ ë¡œì»¬ íŒŒì¼ ì‚­ì œ
                temp_file.unlink()
                return True

            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            delay = self.initial_delay * (self.backoff_factor ** attempt)
            time.sleep(delay)

        # ì‹¤íŒ¨ ì‹œ pending ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        self.pending_files.append({
            "local_path": str(temp_file),
            "hdfs_path": hdfs_path,
            "source": source,
            "date": date,
        })
        return False

    def start_auto_upload(self):
        """ìë™ ì¬ì—…ë¡œë“œ ì‹œì‘"""
        if self.auto_upload_thread and self.auto_upload_thread.is_alive():
            return

        self.stop_event.clear()
        self.auto_upload_thread = threading.Thread(
            target=self._auto_upload_loop,
            daemon=True,
        )
        self.auto_upload_thread.start()

    def _auto_upload_loop(self):
        """ìë™ ì¬ì—…ë¡œë“œ ë£¨í”„"""
        while not self.stop_event.is_set():
            if self.pending_files:
                # HDFS í—¬ìŠ¤ ì²´í¬
                if self._check_hdfs_health():
                    # pending íŒŒì¼ ì¬ì—…ë¡œë“œ ì‹œë„
                    remaining = []
                    for pending in self.pending_files:
                        if self.hdfs_client.put(pending["local_path"], pending["hdfs_path"]):
                            # ì„±ê³µ ì‹œ ë¡œì»¬ íŒŒì¼ ì‚­ì œ
                            Path(pending["local_path"]).unlink()
                        else:
                            remaining.append(pending)
                    self.pending_files = remaining

            # ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬
            self.stop_event.wait(self.health_check_interval)
```

### 4.4 ì„¤ì • íŒŒì¼

**íŒŒì¼**: `config/cluster_config.yaml`

```yaml
hadoop:
  version: "3.4.1"
  home: "/opt/hadoop"
  hdfs:
    namenode: "hdfs://raspberry-master:9000"
    replication: 3
  yarn:
    resourcemanager: "raspberry-master:8032"
```

---

## 5. í†µí•© íŒŒì´í”„ë¼ì¸

### 5.1 ì „ì²´ ë°ì´í„° íë¦„

```
[Scrapy Spider]
    â†“
[ValidationPipeline] â†’ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
    â†“
[DuplicatesPipeline] â†’ ì¤‘ë³µ ì œê±°
    â†“
[HDFSPipeline] â†’ HDFS ì €ì¥ (/raw/{source}/{date}/*.json)
    â†“
[KafkaPipeline] â†’ Kafka ì „ì†¡ (cointicker.raw.{spider_name})
    â†“
[Kafka Consumer] â†’ HDFS ì €ì¥ ë˜ëŠ” Backend ì²˜ë¦¬
    â†“
[MapReduce] â†’ ë°ì´í„° ì •ì œ (/cleaned/{date}/aggregated_*.json)
    â†“
[DataLoader] â†’ PostgreSQL ì ì¬
```

### 5.2 Pipeline ì‹¤í–‰ ìˆœì„œ

**ì„¤ì •**: `settings.py`

```python
ITEM_PIPELINES = {
    "cointicker.pipelines.ValidationPipeline": 300,  # ìš°ì„ ìˆœìœ„ 300
    "cointicker.pipelines.DuplicatesPipeline": 400,  # ìš°ì„ ìˆœìœ„ 400
    "cointicker.pipelines.HDFSPipeline": 500,         # ìš°ì„ ìˆœìœ„ 500
    "cointicker.pipelines.kafka_pipeline.KafkaPipeline": 600,  # ìš°ì„ ìˆœìœ„ 600
}
```

### 5.3 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**íŒŒì¼**: `master-node/orchestrator.py`

```python
class PipelineOrchestrator:
    def run_crawlers(self):
        """í¬ë¡¤ë§ ì‘ì—… ì‹¤í–‰"""
        spiders = ['upbit_trends', 'saveticker', 'coinness', 'perplexity']
        for spider_name in spiders:
            self.schedule_spider(spider_name)

    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        # 1. í¬ë¡¤ë§
        self.run_crawlers()

        # 2. MapReduce ì •ì œ
        self.run_mapreduce()

        # 3. DB ì ì¬
        self.run_data_loader()
```

---

## 6. ìš”ì•½

### 6.1 ê¸°ìˆ  ìŠ¤íƒë³„ ì—­í• 

| ê¸°ìˆ             | ì—­í•                    | êµ¬í˜„ ìœ„ì¹˜                  |
| --------------- | ---------------------- | -------------------------- |
| **Scrapy**      | ì›¹ í¬ë¡¤ë§ í”„ë ˆì„ì›Œí¬   | `worker-nodes/cointicker/` |
| **Selenium**    | ë™ì  ì½˜í…ì¸  ì²˜ë¦¬       | `shared/selenium_utils.py` |
| **Kafka**       | ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° | `shared/kafka_client.py`   |
| **Hadoop HDFS** | ë¶„ì‚° ë°ì´í„° ì €ì¥       | `shared/hdfs_client.py`    |

### 6.2 ì£¼ìš” íŠ¹ì§•

1. **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**: Java API ìš°ì„ , CLI í´ë°±
2. **ìë™ ì¬ì‹œë„**: ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ ë¡œì§
3. **ë°°ì¹˜ ì²˜ë¦¬**: ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
4. **ê°ì§€ ìš°íšŒ**: Selenium ê°ì§€ ìš°íšŒ ê¸°ëŠ¥
5. **ì™€ì¼ë“œì¹´ë“œ ì§€ì›**: Kafka í† í”½ íŒ¨í„´ ë§¤ì¹­

### 6.3 ì„¤ì • íŒŒì¼

- **Scrapy**: `worker-nodes/cointicker/settings.py`
- **Kafka**: `config/kafka_config.yaml`
- **HDFS**: `config/cluster_config.yaml`
- **Spider ìŠ¤ì¼€ì¤„**: `config/spider_config.yaml`

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-12-08
**ë²„ì „**: 1.0
