"""
HDFS ë§¤ë‹ˆì €
HDFS ì„¤ì • ë° ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
"""

import os
import subprocess
import time
import socket
import getpass
from pathlib import Path
from typing import Dict, List, Optional, Callable

from shared.logger import setup_logger
from gui.modules.managers.ssh_manager import SSHManager
from gui.core.timing_config import TimingConfig
from gui.core.retry_utils import execute_with_retry
from gui.core.config_manager import ConfigManager

logger = setup_logger(__name__)


class HDFSManager:
    """HDFS ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(
        self,
        user_confirm_callback: Optional[Callable[[str, str], bool]] = None,
        user_password_callback: Optional[Callable[[str, str], Optional[str]]] = None,
    ):
        """
        ì´ˆê¸°í™”

        Args:
            user_confirm_callback: ì‚¬ìš©ì í™•ì¸ ì½œë°± í•¨ìˆ˜
            user_password_callback: ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì½œë°± í•¨ìˆ˜
                - ë³´ì•ˆ: ë¹„ë°€ë²ˆí˜¸ëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ë˜ê³  ì‚¬ìš© í›„ ì¦‰ì‹œ ì‚­ì œë¨
                - ì„œë²„ë‚˜ íŒŒì¼ì— ì €ì¥ë˜ì§€ ì•ŠìŒ
        """
        self.user_confirm_callback = user_confirm_callback
        self.user_password_callback = user_password_callback
        self.ssh_manager = SSHManager()
        self.config_manager = ConfigManager()
        self._cached_hadoop_home: Optional[str] = None

    def check_running(self, ports: Optional[List[int]] = None) -> bool:
        """
        HDFS ì‹¤í–‰ ì—¬ë¶€ í™•ì¸

        Args:
            ports: í™•ì¸í•  í¬íŠ¸ ëª©ë¡ (ê¸°ë³¸: [9000, 9870])

        Returns:
            HDFS ì‹¤í–‰ ì—¬ë¶€
        """
        if ports is None:
            ports = [9000, 9870]

        for port in ports:
            try:
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.settimeout(1)
                result = sock.connect_ex(("localhost", port))
                sock.close()
                if result == 0:
                    logger.info(f"HDFSê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤ (í¬íŠ¸ {port})")
                    return True
            except:
                pass
        return False

    def stop_all_daemons(self, hadoop_home: str) -> bool:
        """
        ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  HDFS ë°ëª¬ ì¤‘ì§€

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ

        Returns:
            ì¤‘ì§€ ì„±ê³µ ì—¬ë¶€
        """
        try:
            hadoop_path = Path(hadoop_home)

            # í•˜ë‘¡ ê²½ë¡œ ê²€ì¦
            if not hadoop_path.exists():
                logger.warning(f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}")
                return False

            bin_dir = hadoop_path / "bin"
            sbin_dir = hadoop_path / "sbin"

            if not bin_dir.exists():
                logger.warning(f"Hadoop bin ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bin_dir}")
                return False

            # HDFS í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            hdfs_env = {**os.environ, "HADOOP_HOME": str(hadoop_home)}
            current_user = getpass.getuser()
            hdfs_env["HDFS_NAMENODE_USER"] = current_user
            hdfs_env["HDFS_DATANODE_USER"] = current_user
            hdfs_env["HDFS_SECONDARYNAMENODE_USER"] = current_user

            logger.info("ì‹¤í–‰ ì¤‘ì¸ HDFS ë°ëª¬ì„ ì¤‘ì§€í•©ë‹ˆë‹¤...")

            # 1. stop-dfs.sh ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê°€ì¥ ì•ˆì „í•œ ë°©ë²•)
            stop_dfs_script = sbin_dir / "stop-dfs.sh"
            if stop_dfs_script.exists():
                try:
                    script_timeout = TimingConfig.get("hdfs.script_timeout", 30)
                    stop_result = subprocess.run(
                        ["bash", str(stop_dfs_script)],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        timeout=script_timeout,
                        env=hdfs_env,
                        cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
                    )
                    if stop_result.returncode == 0:
                        logger.info("âœ… stop-dfs.shë¡œ HDFS ë°ëª¬ ì¤‘ì§€ ì™„ë£Œ")
                    else:
                        stderr_text = stop_result.stderr.decode(
                            "utf-8", errors="ignore"
                        )
                        logger.debug(f"stop-dfs.sh ì‹¤í–‰ ê²°ê³¼: {stderr_text[:200]}")
                except subprocess.TimeoutExpired:
                    logger.warning("stop-dfs.sh ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
                except Exception as e:
                    logger.warning(f"stop-dfs.sh ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

            # 2. ê°œë³„ ë°ëª¬ ì¤‘ì§€ ì‹œë„ (stop-dfs.shê°€ ì‹¤íŒ¨í•œ ê²½ìš° ëŒ€ë¹„)
            hdfs_cmd = bin_dir / "hdfs"
            if hdfs_cmd.exists():
                daemons = ["namenode", "datanode", "secondarynamenode"]
                for daemon in daemons:
                    try:
                        stop_result = subprocess.run(
                            [
                                str(hdfs_cmd),
                                "--daemon",
                                "stop",
                                daemon,
                            ],  # ê²€ì¦ëœ hdfs ëª…ë ¹ì–´ ì‚¬ìš©
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            timeout=TimingConfig.get("hdfs.daemon_stop_timeout", 10),
                            env=hdfs_env,
                            cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
                        )
                        if stop_result.returncode == 0:
                            logger.debug(f"âœ… {daemon} ì¤‘ì§€ ì™„ë£Œ")
                        else:
                            stderr_text = stop_result.stderr.decode(
                                "utf-8", errors="ignore"
                            )
                            # "no such process" ê°™ì€ ì—ëŸ¬ëŠ” ë¬´ì‹œ (ì´ë¯¸ ì¤‘ì§€ëœ ê²½ìš°)
                            if "no such process" not in stderr_text.lower():
                                logger.debug(f"{daemon} ì¤‘ì§€ ê²°ê³¼: {stderr_text[:100]}")
                    except Exception as e:
                        logger.debug(f"{daemon} ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

            # 3. í”„ë¡œì„¸ìŠ¤ê°€ ì™„ì „íˆ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            time.sleep(2)

            # 4. jpsë¡œ ë‚¨ì•„ìˆëŠ” í”„ë¡œì„¸ìŠ¤ í™•ì¸ ë° ê°•ì œ ì¢…ë£Œ
            try:
                jps_result = subprocess.run(
                    ["jps"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=5,
                )
                if jps_result.returncode == 0:
                    jps_output = jps_result.stdout.decode("utf-8", errors="ignore")
                    hdfs_processes = [
                        "NameNode",
                        "DataNode",
                        "SecondaryNameNode",
                    ]
                    for process_name in hdfs_processes:
                        for line in jps_output.split("\n"):
                            if process_name in line:
                                parts = line.strip().split()
                                if parts:
                                    try:
                                        pid = int(parts[0])
                                        logger.warning(
                                            f"ë‚¨ì•„ìˆëŠ” {process_name} í”„ë¡œì„¸ìŠ¤ ë°œê²¬ (PID: {pid}). ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤."
                                        )
                                        os.kill(pid, 15)  # SIGTERM
                                        time.sleep(1)
                                        # ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì´ë©´ SIGKILL
                                        try:
                                            os.kill(pid, 0)  # í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ í™•ì¸
                                            logger.warning(
                                                f"{process_name} (PID: {pid})ê°€ ì¢…ë£Œë˜ì§€ ì•Šì•„ SIGKILLì„ ë³´ëƒ…ë‹ˆë‹¤."
                                            )
                                            os.kill(pid, 9)  # SIGKILL
                                        except ProcessLookupError:
                                            pass  # ì´ë¯¸ ì¢…ë£Œë¨
                                    except (ValueError, ProcessLookupError, OSError):
                                        pass
            except Exception as e:
                logger.debug(f"jps ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

            logger.info("âœ… HDFS ë°ëª¬ ì •ë¦¬ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"HDFS ë°ëª¬ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def wait_for_ports(
        self,
        ports: Optional[List[int]] = None,
        max_retries: Optional[int] = None,
        retry_interval: Optional[int] = None,
    ) -> Dict:
        """
        HDFS í¬íŠ¸ê°€ ì—´ë¦´ ë•Œê¹Œì§€ ëŒ€ê¸°

        Args:
            ports: í™•ì¸í•  í¬íŠ¸ ëª©ë¡ (ê¸°ë³¸: [9000, 9870])
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (Noneì´ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)
            retry_interval: ì¬ì‹œë„ ê°„ê²© (ì´ˆ, Noneì´ë©´ ì„¤ì •ì—ì„œ ê°€ì ¸ì˜´)

        Returns:
            ì„±ê³µ ì—¬ë¶€ì™€ ë©”ì‹œì§€
        """
        if ports is None:
            ports = [9000, 9870]

        if max_retries is None:
            max_retries = TimingConfig.get("hdfs.port_check_max_retries", 15)
        if retry_interval is None:
            retry_interval = TimingConfig.get("hdfs.port_check_retry_interval", 2)

        for attempt in range(max_retries):
            time.sleep(retry_interval)

            for port in ports:
                try:
                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                    sock.settimeout(1)
                    result = sock.connect_ex(("localhost", port))
                    sock.close()
                    if result == 0:
                        logger.info(
                            f"âœ… HDFS ì‹œì‘ ì„±ê³µ (í¬íŠ¸ {port}, ì‹œë„ {attempt + 1}/{max_retries})"
                        )
                        return {
                            "success": True,
                            "message": f"HDFSê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (í¬íŠ¸ {port})",
                        }
                except Exception as e:
                    logger.debug(f"HDFS í¬íŠ¸ í™•ì¸ ì˜¤ë¥˜ (í¬íŠ¸ {port}): {e}")

            if attempt < max_retries - 1:
                logger.debug(f"HDFS ì‹œì‘ ëŒ€ê¸° ì¤‘... ({attempt + 1}/{max_retries})")

        return {
            "success": False,
            "error": f"HDFS ì‹œì‘ í›„ í¬íŠ¸ í™•ì¸ ì‹¤íŒ¨ (ìµœëŒ€ {max_retries * retry_interval}ì´ˆ ëŒ€ê¸°)",
        }

    def _get_cached_hadoop_home(self) -> Optional[str]:
        """
        ìºì‹œëœ HADOOP_HOME ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°

        Returns:
            HADOOP_HOME ê²½ë¡œ ë˜ëŠ” None
        """
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if self._cached_hadoop_home:
            return self._cached_hadoop_home

        # ì„¤ì • íŒŒì¼ì—ì„œ ìºì‹œëœ ê²½ë¡œ í™•ì¸
        cached_path = self.config_manager.get_config(
            "cluster", "hadoop.cached_home", None
        )
        if cached_path and Path(cached_path).exists():
            self._cached_hadoop_home = cached_path
            logger.debug(f"ìºì‹œëœ HADOOP_HOME ê²½ë¡œ ì‚¬ìš©: {cached_path}")
            return cached_path

        return None

    def _cache_hadoop_home(self, hadoop_home: str) -> None:
        """
        HADOOP_HOME ê²½ë¡œë¥¼ ìºì‹œì— ì €ì¥

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ
        """
        try:
            # ë©”ëª¨ë¦¬ ìºì‹œ
            self._cached_hadoop_home = hadoop_home

            # ì„¤ì • íŒŒì¼ì— ì €ì¥
            self.config_manager.set_config("cluster", "hadoop.cached_home", hadoop_home)
            logger.info(f"âœ… HADOOP_HOME ê²½ë¡œ ìºì‹œ ì €ì¥: {hadoop_home}")
        except Exception as e:
            logger.warning(f"HADOOP_HOME ê²½ë¡œ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _check_hadoop_permissions(self, hadoop_home: str) -> Dict:
        """
        Hadoop ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ

        Returns:
            ê¶Œí•œ í™•ì¸ ê²°ê³¼
        """
        try:
            hadoop_path = Path(hadoop_home)
            current_user = getpass.getuser()

            # í™•ì¸í•  ë””ë ‰í† ë¦¬ ëª©ë¡
            check_dirs = [
                hadoop_path / "logs",  # ë¡œê·¸ ë””ë ‰í† ë¦¬
                hadoop_path / "tmp",  # ì„ì‹œ ë””ë ‰í† ë¦¬
                hadoop_path / "tmp" / "dfs",  # HDFS ë°ì´í„° ë””ë ‰í† ë¦¬
            ]

            permission_issues = []
            for check_dir in check_dirs:
                if check_dir.exists():
                    # ì“°ê¸° ê¶Œí•œ í™•ì¸
                    if not os.access(check_dir, os.W_OK):
                        permission_issues.append(
                            f"ì“°ê¸° ê¶Œí•œ ì—†ìŒ: {check_dir} (ì‚¬ìš©ì: {current_user})"
                        )
                    # ì½ê¸° ê¶Œí•œ í™•ì¸
                    if not os.access(check_dir, os.R_OK):
                        permission_issues.append(
                            f"ì½ê¸° ê¶Œí•œ ì—†ìŒ: {check_dir} (ì‚¬ìš©ì: {current_user})"
                        )
                else:
                    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ë¶€ëª¨ ë””ë ‰í† ë¦¬ì— ì“°ê¸° ê¶Œí•œì´ ìˆëŠ”ì§€ í™•ì¸
                    parent_dir = check_dir.parent
                    if parent_dir.exists() and not os.access(parent_dir, os.W_OK):
                        permission_issues.append(
                            f"ë””ë ‰í† ë¦¬ ìƒì„± ê¶Œí•œ ì—†ìŒ: {check_dir} (ë¶€ëª¨ ë””ë ‰í† ë¦¬: {parent_dir}, ì‚¬ìš©ì: {current_user})"
                        )

            if permission_issues:
                error_msg = "Hadoop ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ:\n" + "\n".join(
                    f"  - {issue}" for issue in permission_issues
                )
                logger.warning(error_msg)
                return {
                    "success": False,
                    "error": error_msg,
                    "permission_issues": permission_issues,
                }

            logger.info(f"âœ… Hadoop ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸ ì™„ë£Œ (ì‚¬ìš©ì: {current_user})")
            return {"success": True, "message": "ê¶Œí•œ í™•ì¸ ì™„ë£Œ"}

        except Exception as e:
            logger.error(f"Hadoop ê¶Œí•œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"Hadoop ê¶Œí•œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            }

    def get_cluster_config(self) -> Optional[Dict]:
        """í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì½ê¸° (cluster_config.yaml ìë™ ì½ê¸°)"""
        try:
            from gui.core.config_manager import ConfigManager

            config_manager = ConfigManager()
            cluster_config = config_manager.load_config("cluster")

            if cluster_config:
                logger.debug(
                    f"í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ë¡œë“œ ì„±ê³µ: {config_manager.config_dir / 'cluster_config.yaml'}"
                )
            else:
                logger.debug("í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

            return cluster_config
        except Exception as e:
            logger.debug(f"í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return None

    def setup_single_node_mode(
        self, hadoop_home: str, cluster_config: Optional[Dict], replication: int
    ) -> bool:
        """
        ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ ì„¤ì • (replication=1)

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ
            cluster_config: í´ëŸ¬ìŠ¤í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì—†ì„ ìˆ˜ ìˆìŒ)
            replication: ë³µì œ ì¸ìˆ˜ (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œì—ì„œëŠ” 1)

        Returns:
            ì„¤ì • ì„±ê³µ ì—¬ë¶€
        """
        try:
            hadoop_path = Path(hadoop_home)

            # í•˜ë‘¡ ê²½ë¡œ ê²€ì¦
            if not hadoop_path.exists():
                logger.error(f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}")
                return False

            etc_hadoop = hadoop_path / "etc" / "hadoop"

            if not etc_hadoop.exists():
                logger.error(f"Hadoop ì„¤ì • ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {etc_hadoop}")
                return False

            # namenode URL ê²°ì • (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œì—ì„œëŠ” í•­ìƒ localhost ì‚¬ìš©)
            namenode_url = "hdfs://localhost:9000"

            # 1. core-site.xml ìƒì„±/ì—…ë°ì´íŠ¸
            core_site = etc_hadoop / "core-site.xml"
            logger.info(f"core-site.xml ì„¤ì • ì¤‘ (ë‹¨ì¼ ë…¸ë“œ): {namenode_url}")
            core_site.write_text(
                f"""<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>{namenode_url}</value>
    </property>
</configuration>
"""
            )

            # 2. hdfs-site.xml ìƒì„±/ì—…ë°ì´íŠ¸ (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œì—ì„œëŠ” í•­ìƒ replication=1)
            hdfs_site = etc_hadoop / "hdfs-site.xml"
            single_node_replication = 1  # ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œì—ì„œëŠ” í•­ìƒ 1

            # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ì˜êµ¬ ì €ì¥ì†Œ)
            namenode_dir = hadoop_path / "data" / "namenode"
            datanode_dir = hadoop_path / "data" / "datanode"
            
            # ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
            namenode_dir.mkdir(parents=True, exist_ok=True)
            datanode_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"NameNode ë°ì´í„° ë””ë ‰í† ë¦¬: {namenode_dir}")
            logger.info(f"DataNode ë°ì´í„° ë””ë ‰í† ë¦¬: {datanode_dir}")
            logger.info(
                f"hdfs-site.xml ì„¤ì • ì¤‘ (ë‹¨ì¼ ë…¸ë“œ): replication={single_node_replication}"
            )
            hdfs_site.write_text(
                f"""<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>{single_node_replication}</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://{namenode_dir}</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://{datanode_dir}</value>
    </property>
</configuration>
"""
            )

            logger.info("âœ… ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def setup_cluster_mode(self, hadoop_home: str, cluster_config: Dict) -> bool:
        """
        í´ëŸ¬ìŠ¤í„° ëª¨ë“œ ì„¤ì • (ìœ ì„  ì—°ê²° ì‹œ ìë™ ì„¤ì •)

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ
            cluster_config: í´ëŸ¬ìŠ¤í„° ì„¤ì • ë”•ì…”ë„ˆë¦¬

        Returns:
            ì„¤ì • ì„±ê³µ ì—¬ë¶€
        """
        try:
            hadoop_path = Path(hadoop_home)

            # í•˜ë‘¡ ê²½ë¡œ ê²€ì¦
            if not hadoop_path.exists():
                logger.error(f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}")
                return False

            etc_hadoop = hadoop_path / "etc" / "hadoop"

            if not etc_hadoop.exists():
                logger.error(f"Hadoop ì„¤ì • ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {etc_hadoop}")
                return False

            cluster_info = cluster_config.get("cluster", {})
            hadoop_info = cluster_config.get("hadoop", {})

            if not cluster_info or not hadoop_info:
                logger.warning("í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ë¶ˆì™„ì „í•©ë‹ˆë‹¤.")
                return False

            master = cluster_info.get("master", {})
            workers = cluster_info.get("workers", [])
            namenode_url = hadoop_info.get("hdfs", {}).get("namenode", "")
            replication = hadoop_info.get("hdfs", {}).get("replication", 3)

            if not master or not workers:
                logger.warning("ë§ˆìŠ¤í„° ë˜ëŠ” ì›Œì»¤ ë…¸ë“œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False

            master_hostname = master.get("hostname", "localhost")

            # 1. ê° ë…¸ë“œì— SSH ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("í´ëŸ¬ìŠ¤í„° ë…¸ë“œ SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            all_nodes_accessible = True

            # localhost SSH í…ŒìŠ¤íŠ¸
            logger.debug("localhost SSH ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            ssh_timeout = TimingConfig.get("ssh.connection_test_timeout", 5)
            if not self.ssh_manager.test_connection("localhost", timeout=ssh_timeout):
                logger.warning("localhost SSH ì—°ê²° ì‹¤íŒ¨")
                all_nodes_accessible = False

            # ë§ˆìŠ¤í„° ë…¸ë“œ í…ŒìŠ¤íŠ¸
            if not self.ssh_manager.test_connection(
                master_hostname, timeout=ssh_timeout
            ):
                logger.warning(f"ë§ˆìŠ¤í„° ë…¸ë“œ({master_hostname}) SSH ì—°ê²° ì‹¤íŒ¨")
                all_nodes_accessible = False

            # ì›Œì»¤ ë…¸ë“œ í…ŒìŠ¤íŠ¸
            for worker in workers:
                worker_hostname = worker.get("hostname", "")
                if worker_hostname:
                    if not self.ssh_manager.test_connection(
                        worker_hostname, timeout=ssh_timeout
                    ):
                        logger.warning(f"ì›Œì»¤ ë…¸ë“œ({worker_hostname}) SSH ì—°ê²° ì‹¤íŒ¨")
                        all_nodes_accessible = False

            if not all_nodes_accessible:
                logger.warning(
                    "ì¼ë¶€ ë…¸ë“œì— SSH ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í´ëŸ¬ìŠ¤í„° ëª¨ë“œ ì„¤ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤."
                )
                return False

            # 2. core-site.xml ìƒì„±/ì—…ë°ì´íŠ¸
            core_site = etc_hadoop / "core-site.xml"
            logger.info(f"core-site.xml ì„¤ì • ì¤‘: {namenode_url}")
            core_site.write_text(
                f"""<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>{namenode_url}</value>
    </property>
</configuration>
"""
            )

            # 3. hdfs-site.xml ìƒì„±/ì—…ë°ì´íŠ¸
            hdfs_site = etc_hadoop / "hdfs-site.xml"
            
            # ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì • (ì˜êµ¬ ì €ì¥ì†Œ)
            namenode_dir = hadoop_path / "data" / "namenode"
            datanode_dir = hadoop_path / "data" / "datanode"

            # ë””ë ‰í† ë¦¬ ìƒì„± (ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°)
            namenode_dir.mkdir(parents=True, exist_ok=True)
            datanode_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"NameNode ë°ì´í„° ë””ë ‰í† ë¦¬: {namenode_dir}")
            logger.info(f"DataNode ë°ì´í„° ë””ë ‰í† ë¦¬: {datanode_dir}")
            logger.info(f"hdfs-site.xml ì„¤ì • ì¤‘: replication={replication}")
            hdfs_site.write_text(
                f"""<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>{replication}</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file://{namenode_dir}</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://{datanode_dir}</value>
    </property>
</configuration>
"""
            )

            # 4. workers íŒŒì¼ ìƒì„±
            workers_file = etc_hadoop / "workers"
            worker_hostnames = [
                w.get("hostname", "") for w in workers if w.get("hostname")
            ]
            if worker_hostnames:
                logger.info(f"workers íŒŒì¼ ìƒì„± ì¤‘: {', '.join(worker_hostnames)}")
                workers_file.write_text("\n".join(worker_hostnames) + "\n")

            # 5. master íŒŒì¼ ìƒì„± (SecondaryNameNodeìš©)
            master_file = etc_hadoop / "master"
            logger.info(f"master íŒŒì¼ ìƒì„± ì¤‘: {master_hostname}")
            master_file.write_text(master_hostname + "\n")

            logger.info("âœ… í´ëŸ¬ìŠ¤í„° ëª¨ë“œ ì„¤ì • ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í´ëŸ¬ìŠ¤í„° ëª¨ë“œ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def start_daemons_direct(
        self, hadoop_home: str, hdfs_env: Dict, namenode_ports: List[int]
    ) -> Dict:
        """
        SSH ì—†ì´ HDFS ë°ëª¬ì„ ì§ì ‘ ì‹œì‘ (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ)

        Args:
            hadoop_home: HADOOP_HOME ê²½ë¡œ
            hdfs_env: HDFS í™˜ê²½ ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
            namenode_ports: NameNode í¬íŠ¸ ëª©ë¡

        Returns:
            ì‹œì‘ ê²°ê³¼
        """
        try:
            hadoop_path = Path(hadoop_home)

            # í•˜ë‘¡ ê²½ë¡œ ê²€ì¦
            if not hadoop_path.exists():
                logger.error(f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}")
                return {
                    "success": False,
                    "error": f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}",
                }

            bin_dir = hadoop_path / "bin"

            if not bin_dir.exists():
                logger.error(f"Hadoop bin ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bin_dir}")
                return {
                    "success": False,
                    "error": f"Hadoop bin ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bin_dir}",
                }

            # hdfs ëª…ë ¹ì–´ íŒŒì¼ ì¡´ì¬ í™•ì¸
            hdfs_cmd = bin_dir / "hdfs"
            if not hdfs_cmd.exists():
                logger.error(f"HDFS ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hdfs_cmd}")
                return {
                    "success": False,
                    "error": f"HDFS ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hdfs_cmd}",
                }

            # ê¶Œí•œ í™•ì¸
            permission_check = self._check_hadoop_permissions(hadoop_home)
            if not permission_check.get("success"):
                logger.warning(
                    f"âš ï¸ Hadoop ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ ê°ì§€: {permission_check.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                )
                logger.warning(
                    "âš ï¸ ë°ëª¬ ì‹œì‘ì„ ì‹œë„í•˜ì§€ë§Œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œí•œ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”."
                )
                # ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (sudoë¡œ í•´ê²° ê°€ëŠ¥í•  ìˆ˜ ìˆìŒ)

            # NameNode í¬ë§· í™•ì¸ (í•„ìš”ì‹œ)
            namenode_dir = hadoop_path / "tmp" / "dfs" / "name"
            if not namenode_dir.exists() or not any(namenode_dir.iterdir()):
                logger.info("NameNode í¬ë§·ì´ í•„ìš”í•©ë‹ˆë‹¤. í¬ë§·ì„ ì‹œë„í•©ë‹ˆë‹¤...")
                format_result = subprocess.run(
                    [
                        str(hdfs_cmd),  # ê²€ì¦ëœ hdfs ëª…ë ¹ì–´ ì‚¬ìš©
                        "namenode",
                        "-format",
                        "-force",
                        "-nonInteractive",
                    ],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=TimingConfig.get("hdfs.format_timeout", 30),
                    env=hdfs_env,
                    cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
                )
                if format_result.returncode == 0:
                    logger.info("âœ… NameNode í¬ë§· ì™„ë£Œ")
                else:
                    stderr_text = format_result.stderr.decode("utf-8", errors="ignore")
                    if "already formatted" not in stderr_text.lower():
                        logger.warning(f"NameNode í¬ë§· ì‹¤íŒ¨: {stderr_text[:200]}")
                    # í¬ë§· ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ì´ë¯¸ í¬ë§·ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)

            # ë°ëª¬ ì§ì ‘ ì‹œì‘
            logger.info("HDFS ë°ëª¬ ì§ì ‘ ì‹œì‘ ì¤‘...")
            daemons = []

            # NameNode ì‹œì‘
            namenode_process = subprocess.Popen(
                [
                    str(hdfs_cmd),
                    "--daemon",
                    "start",
                    "namenode",
                ],  # ê²€ì¦ëœ hdfs ëª…ë ¹ì–´ ì‚¬ìš©
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                start_new_session=True,
                env=hdfs_env,
                cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
            )
            daemons.append(("namenode", namenode_process))
            logger.info("NameNode ë°ëª¬ ì‹œì‘ ì¤‘...")
            daemon_start_delay = TimingConfig.get("hdfs.daemon_start_delay", 2)
            time.sleep(daemon_start_delay)

            # DataNode ì‹œì‘
            datanode_process = subprocess.Popen(
                [
                    str(hdfs_cmd),
                    "--daemon",
                    "start",
                    "datanode",
                ],  # ê²€ì¦ëœ hdfs ëª…ë ¹ì–´ ì‚¬ìš©
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                start_new_session=True,
                env=hdfs_env,
                cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
            )
            daemons.append(("datanode", datanode_process))
            logger.info("DataNode ë°ëª¬ ì‹œì‘ ì¤‘...")
            time.sleep(daemon_start_delay)

            # SecondaryNameNode ì‹œì‘
            secondary_namenode_process = subprocess.Popen(
                [
                    str(hdfs_cmd),
                    "--daemon",
                    "start",
                    "secondarynamenode",
                ],  # ê²€ì¦ëœ hdfs ëª…ë ¹ì–´ ì‚¬ìš©
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                start_new_session=True,
                env=hdfs_env,
                cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
            )
            daemons.append(("secondarynamenode", secondary_namenode_process))
            logger.info("SecondaryNameNode ë°ëª¬ ì‹œì‘ ì¤‘...")
            secondary_namenode_delay = TimingConfig.get(
                "hdfs.secondary_namenode_delay", 3
            )
            time.sleep(secondary_namenode_delay)

            # í¬íŠ¸ í™•ì¸
            port_check_result = self.wait_for_ports(namenode_ports)
            if not port_check_result.get("success"):
                # ì‹¤íŒ¨ ì‹œ ë°ëª¬ ì •ë¦¬
                logger.warning("HDFS ë°ëª¬ ì‹œì‘ ì‹¤íŒ¨. ë°ëª¬ì„ ì •ë¦¬í•©ë‹ˆë‹¤...")
                for daemon_name, proc in daemons:
                    try:
                        proc.terminate()
                        proc.wait(timeout=5)
                    except:
                        pass

                return {
                    "success": False,
                    "error": "HDFS ë°ëª¬ ì‹œì‘ í›„ í¬íŠ¸ í™•ì¸ ì‹¤íŒ¨ (ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°)",
                }

            # Safe Mode í•´ì œ ëŒ€ê¸° (í¬íŠ¸ê°€ ì—´ë¦° í›„ì—ë„ Safe Mode ìƒíƒœì¼ ìˆ˜ ìˆìŒ)
            logger.info("HDFS Safe Mode í•´ì œ ëŒ€ê¸° ì¤‘...")
            safemode_timeout = TimingConfig.get("hdfs.safemode_wait_timeout", 60)
            safemode_check_interval = TimingConfig.get(
                "hdfs.safemode_check_interval", 2
            )

            try:
                # hdfs dfsadmin -safemode wait ëª…ë ¹ì–´ ì‹¤í–‰ (Safe Mode í•´ì œê¹Œì§€ ëŒ€ê¸°)
                safemode_result = subprocess.run(
                    [
                        str(hdfs_cmd),
                        "dfsadmin",
                        "-safemode",
                        "wait",
                    ],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    timeout=safemode_timeout,
                    env=hdfs_env,
                    cwd=str(hadoop_path),
                )

                if safemode_result.returncode == 0:
                    logger.info("âœ… HDFS Safe Mode í•´ì œ ì™„ë£Œ")
                else:
                    stderr_text = safemode_result.stderr.decode(
                        "utf-8", errors="ignore"
                    )
                    # íƒ€ì„ì•„ì›ƒì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ê²½ê³  (íƒ€ì„ì•„ì›ƒì€ ì •ìƒì ì¸ ê²½ìš°ì¼ ìˆ˜ ìˆìŒ)
                    if "timeout" not in stderr_text.lower():
                        logger.warning(
                            f"HDFS Safe Mode í•´ì œ í™•ì¸ ì¤‘ ê²½ê³ : {stderr_text[:200]}"
                        )

                    # Safe Mode ìƒíƒœë¥¼ ì§ì ‘ í™•ì¸
                    safemode_check_result = subprocess.run(
                        [
                            str(hdfs_cmd),
                            "dfsadmin",
                            "-safemode",
                            "get",
                        ],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        timeout=10,
                        env=hdfs_env,
                        cwd=str(hadoop_path),
                    )

                    if safemode_check_result.returncode == 0:
                        safemode_output = safemode_check_result.stdout.decode(
                            "utf-8", errors="ignore"
                        )
                        if "Safe mode is OFF" in safemode_output:
                            logger.info("âœ… HDFS Safe Mode í•´ì œ í™•ì¸ ì™„ë£Œ")
                        else:
                            logger.warning(
                                f"âš ï¸ HDFSê°€ Safe Mode ìƒíƒœì…ë‹ˆë‹¤: {safemode_output[:100]}"
                            )
                            # Safe Mode ìƒíƒœì—¬ë„ ê³„ì† ì§„í–‰ (ì¼ë¶€ ê²½ìš° ì •ìƒ ë™ì‘ ê°€ëŠ¥)
                    else:
                        logger.warning("âš ï¸ HDFS Safe Mode ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

            except subprocess.TimeoutExpired:
                logger.warning(
                    f"âš ï¸ HDFS Safe Mode í•´ì œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ ({safemode_timeout}ì´ˆ)"
                )
                logger.warning("âš ï¸ HDFSê°€ Safe Mode ìƒíƒœì¼ ìˆ˜ ìˆìœ¼ë‚˜ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")
                # íƒ€ì„ì•„ì›ƒì´ì–´ë„ ê³„ì† ì§„í–‰ (Safe ModeëŠ” ìë™ìœ¼ë¡œ í•´ì œë¨)
            except Exception as e:
                logger.warning(f"âš ï¸ HDFS Safe Mode í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e} (ê³„ì† ì§„í–‰)")
                # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (Safe ModeëŠ” ìë™ìœ¼ë¡œ í•´ì œë¨)

            return {
                "success": True,
                "message": "HDFS ë°ëª¬ ì‹œì‘ ì™„ë£Œ (Safe Mode í•´ì œ í™•ì¸)",
            }

        except Exception as e:
            logger.error(f"HDFS ë°ëª¬ ì§ì ‘ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": f"HDFS ë°ëª¬ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}

    def check_and_start(
        self,
        user_confirm_callback: Optional[Callable[[str, str], bool]] = None,
        user_password_callback: Optional[Callable[[str, str], Optional[str]]] = None,
    ) -> Dict:
        """
        HDFS ì²´í¬ ë° ìë™ ì‹œì‘ ì‹œë„

        Args:
            user_confirm_callback: ì‚¬ìš©ì í™•ì¸ ì½œë°± í•¨ìˆ˜ (ê¸°ë³¸: self.user_confirm_callback)
            user_password_callback: ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì½œë°± í•¨ìˆ˜
                - ë³´ì•ˆ: ë¹„ë°€ë²ˆí˜¸ëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ë˜ê³  ì‚¬ìš© í›„ ì¦‰ì‹œ ì‚­ì œë¨
                - ì„œë²„ë‚˜ íŒŒì¼ì— ì €ì¥ë˜ì§€ ì•ŠìŒ

        Returns:
            ì‹œì‘ ê²°ê³¼
        """
        # ì½œë°± í•¨ìˆ˜ ì—…ë°ì´íŠ¸ (í˜¸ì¶œ ì‹œ ì „ë‹¬ëœ ì½œë°± ìš°ì„  ì‚¬ìš©)
        if user_confirm_callback:
            self.user_confirm_callback = user_confirm_callback
        if user_password_callback:
            self.user_password_callback = user_password_callback

        if user_confirm_callback is None:
            user_confirm_callback = self.user_confirm_callback

        try:
            # HDFS NameNode í¬íŠ¸ í™•ì¸ (ê¸°ë³¸: 9000, ì›¹ UI: 9870)
            namenode_ports = [9000, 9870]

            if self.check_running(namenode_ports):
                return {"success": True, "message": "HDFSê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."}

            # HDFS ì‹œì‘ ì‹œë„
            logger.info("HDFSê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤. ìë™ìœ¼ë¡œ ì‹œì‘ì„ ì‹œë„í•©ë‹ˆë‹¤...")

            # í´ëŸ¬ìŠ¤í„° ì„¤ì • í™•ì¸
            cluster_config = self.get_cluster_config()

            # replication ê°’ í™•ì¸
            replication = 3  # ê¸°ë³¸ê°’: ë©€í‹°ë…¸ë“œ ëª¨ë“œ
            if cluster_config:
                replication = (
                    cluster_config.get("hadoop", {})
                    .get("hdfs", {})
                    .get("replication", 3)
                )

            # ê¸°ë³¸ ë™ì‘: cluster ì„¹ì…˜ì´ ìˆìœ¼ë©´ ë©€í‹°ë…¸ë“œ ëª¨ë“œë¡œ ì‹œë„
            has_cluster_config = (
                cluster_config is not None and cluster_config.get("cluster") is not None
            )

            if has_cluster_config and cluster_config:
                master_hostname = (
                    cluster_config.get("cluster", {})
                    .get("master", {})
                    .get("hostname", "unknown")
                )
                worker_count = len(cluster_config.get("cluster", {}).get("workers", []))
                logger.info(
                    "âœ… í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ ê°ì§€ë¨. ë©€í‹°ë…¸ë“œ ëª¨ë“œë¡œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±ì„ ì‹œë„í•©ë‹ˆë‹¤."
                )
                logger.info(
                    f"   ë§ˆìŠ¤í„°: {master_hostname}, ì›Œì»¤ ë…¸ë“œ: {worker_count}ê°œ, Replication: {replication}"
                )
            else:
                logger.debug(
                    "í´ëŸ¬ìŠ¤í„° ì„¤ì • íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                )

            # HADOOP_HOME í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ë° ìë™ ì„¤ì •
            hadoop_home = os.environ.get("HADOOP_HOME")

            # ìºì‹œëœ ê²½ë¡œ ë¨¼ì € í™•ì¸
            if not hadoop_home:
                cached_home = self._get_cached_hadoop_home()
                if cached_home:
                    hadoop_home = cached_home
                    logger.info(f"âœ… ìºì‹œëœ HADOOP_HOME ê²½ë¡œ ì‚¬ìš©: {hadoop_home}")
                    os.environ["HADOOP_HOME"] = hadoop_home

            if not hadoop_home:
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
                current_file = Path(__file__)
                project_root = current_file.parent.parent.parent.parent.parent.parent

                # ê²€ìƒ‰í•  ê²½ë¡œ ëª©ë¡
                search_paths = [
                    project_root / "hadoop_project" / "hadoop-3.4.1",
                    project_root.parent / "hadoop_project" / "hadoop-3.4.1",
                    Path("/opt/hadoop"),
                    Path("/usr/local/hadoop"),
                    Path("/home/bigdata/hadoop-3.4.1"),
                    Path("/usr/lib/hadoop"),
                    Path("/opt/homebrew/opt/hadoop"),
                    Path("/usr/local/opt/hadoop"),
                ]

                for path in search_paths:
                    if path.exists() and (path / "sbin" / "start-dfs.sh").exists():
                        hadoop_home = str(path)
                        logger.info(f"âœ… HADOOP_HOME ìë™ ê°ì§€: {hadoop_home}")
                        os.environ["HADOOP_HOME"] = hadoop_home
                        # ìºì‹œì— ì €ì¥
                        self._cache_hadoop_home(hadoop_home)
                        break

            if hadoop_home:
                # í•˜ë‘¡ ê²½ë¡œ ê²€ì¦
                hadoop_path = Path(hadoop_home)
                if not hadoop_path.exists():
                    logger.error(f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}")
                    return {
                        "success": False,
                        "error": f"HADOOP_HOME ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {hadoop_home}",
                    }

                bin_dir = hadoop_path / "bin"
                sbin_dir = hadoop_path / "sbin"

                if not bin_dir.exists() or not sbin_dir.exists():
                    logger.error(
                        f"Hadoop bin ë˜ëŠ” sbin ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hadoop_home}"
                    )
                    return {
                        "success": False,
                        "error": f"Hadoop bin ë˜ëŠ” sbin ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hadoop_home}",
                    }

                # ê¶Œí•œ í™•ì¸
                permission_check = self._check_hadoop_permissions(hadoop_home)
                if not permission_check.get("success"):
                    logger.warning(
                        f"âš ï¸ Hadoop ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ ê°ì§€: {permission_check.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
                    )
                    logger.warning(
                        "âš ï¸ ë°ëª¬ ì‹œì‘ì„ ì‹œë„í•˜ì§€ë§Œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶Œí•œ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”."
                    )
                    # ê¶Œí•œ ë¬¸ì œê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰ (sudoë¡œ í•´ê²° ê°€ëŠ¥í•  ìˆ˜ ìˆìŒ)

                start_dfs_script = sbin_dir / "start-dfs.sh"
                if start_dfs_script.exists():
                    logger.info(f"HDFS ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ë°œê²¬: {start_dfs_script}")

                    # ê¸°ì¡´ HDFS í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (í¬íŠ¸ëŠ” ë‹«í˜€ìˆì§€ë§Œ í”„ë¡œì„¸ìŠ¤ê°€ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆìŒ)
                    logger.info("ê¸°ì¡´ HDFS í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì¤‘...")
                    self.stop_all_daemons(hadoop_home)

                    # HDFS í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                    hdfs_env = {**os.environ, "HADOOP_HOME": hadoop_home}
                    current_user = getpass.getuser()
                    hdfs_env["HDFS_NAMENODE_USER"] = current_user
                    hdfs_env["HDFS_DATANODE_USER"] = current_user
                    hdfs_env["HDFS_SECONDARYNAMENODE_USER"] = current_user

                    # 1ë‹¨ê³„: ê¸°ë³¸ ë™ì‘ - ë©€í‹°ë…¸ë“œ ëª¨ë“œë¡œ í´ëŸ¬ìŠ¤í„° êµ¬ì„± ì‹œë„
                    if has_cluster_config and cluster_config:
                        logger.info("ë©€í‹°ë…¸ë“œ ëª¨ë“œ: í´ëŸ¬ìŠ¤í„° ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤...")
                        cluster_setup_success = self.setup_cluster_mode(
                            hadoop_home, cluster_config
                        )
                        if cluster_setup_success:
                            logger.info(
                                "âœ… ë©€í‹°ë…¸ë“œ ëª¨ë“œ ì„¤ì • ì™„ë£Œ. localhost SSH ì—°ê²°ì„ í™•ì¸í•©ë‹ˆë‹¤."
                            )
                            ssh_available = self.ssh_manager.test_connection(
                                "localhost", timeout=2
                            )
                            if not ssh_available:
                                logger.warning(
                                    "âš ï¸ ì›ê²© ë…¸ë“œ SSHëŠ” ì„±ê³µí–ˆì§€ë§Œ localhost SSH ì—°ê²° ì‹¤íŒ¨"
                                )
                        else:
                            # ë©€í‹°ë…¸ë“œ ëª¨ë“œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ì í™•ì¸
                            logger.warning(
                                "âš ï¸ ë©€í‹°ë…¸ë“œ ëª¨ë“œ ì„¤ì • ì‹¤íŒ¨. í´ëŸ¬ìŠ¤í„° ë…¸ë“œì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            )

                            use_single_node = False
                            if user_confirm_callback:
                                try:
                                    use_single_node = user_confirm_callback(
                                        "í´ëŸ¬ìŠ¤í„° ì—°ê²° ì‹¤íŒ¨",
                                        "ë©€í‹°ë…¸ë“œ ëª¨ë“œë¡œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n\n"
                                        "ê°œë°œ/í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?\n\n"
                                        "ì˜ˆ: ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì§„í–‰\n"
                                        "ì•„ë‹ˆì˜¤: ë©€í‹°ë…¸ë“œ ëª¨ë“œ ì¬ì‹œë„",
                                    )
                                except Exception as e:
                                    logger.error(f"ì‚¬ìš©ì í™•ì¸ ì½œë°± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                                    use_single_node = True
                            else:
                                logger.info(
                                    "ì‚¬ìš©ì í™•ì¸ ì½œë°±ì´ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤."
                                )
                                use_single_node = True

                            if use_single_node:
                                logger.info(
                                    "âœ… ì‚¬ìš©ì í™•ì¸: ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                                )
                                self.setup_single_node_mode(
                                    hadoop_home, cluster_config, replication
                                )
                                ssh_available = self.ssh_manager.test_connection(
                                    "localhost", timeout=2
                                )
                            else:
                                logger.info(
                                    "ì‚¬ìš©ìê°€ ë©€í‹°ë…¸ë“œ ëª¨ë“œë¥¼ ìœ ì§€í•˜ê¸°ë¡œ ì„ íƒí–ˆìŠµë‹ˆë‹¤."
                                )
                                return {
                                    "success": False,
                                    "error": "ë©€í‹°ë…¸ë“œ ëª¨ë“œ ì„¤ì • ì‹¤íŒ¨. í´ëŸ¬ìŠ¤í„° ë…¸ë“œì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                }
                    else:
                        # í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ì—†ìœ¼ë©´ ì²˜ìŒë¶€í„° ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ
                        logger.info("ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ: HDFS ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
                        self.setup_single_node_mode(
                            hadoop_home, cluster_config, replication
                        )
                        ssh_available = self.ssh_manager.test_connection(
                            "localhost", timeout=2
                        )

                    # 2ë‹¨ê³„: SSH ì‹¤íŒ¨ ì‹œ ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ë¶„ê¸° - ë¡œì»¬ SSH ìë™ ì„¤ì •
                    if not ssh_available:
                        logger.info(
                            "SSH ì—°ê²° ì‹¤íŒ¨. ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ë¡œì»¬ SSH ì„¤ì •ì„ ì‹œë„í•©ë‹ˆë‹¤..."
                        )
                        ssh_setup_success = self.ssh_manager.setup_local_ssh()

                        if ssh_setup_success:
                            ssh_available = self.ssh_manager.test_connection(
                                "localhost", timeout=2
                            )

                            if ssh_available:
                                logger.info("âœ… ë¡œì»¬ SSH ì„¤ì • ì™„ë£Œ. HDFSë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                            else:
                                logger.warning(
                                    "âš ï¸ SSH ì„¤ì • í›„ì—ë„ ì—°ê²° ì‹¤íŒ¨. SSH ì—†ì´ HDFS ë°ëª¬ì„ ì§ì ‘ ì‹œì‘í•©ë‹ˆë‹¤."
                                )
                                ssh_available = False
                        else:
                            logger.warning(
                                "âš ï¸ ë¡œì»¬ SSH ì„¤ì • ì‹¤íŒ¨. SSH ì—†ì´ HDFS ë°ëª¬ì„ ì§ì ‘ ì‹œì‘í•©ë‹ˆë‹¤."
                            )
                            ssh_available = False
                    else:
                        logger.info(
                            "âœ… SSH ì—°ê²° ì„±ê³µ. í´ëŸ¬ìŠ¤í„° ëª¨ë“œë¡œ HDFSë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."
                        )

                    # HDFS ì‹œì‘
                    if not ssh_available:
                        # SSH ì—†ì´ ë°ëª¬ ì§ì ‘ ì‹œì‘ (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œ)
                        logger.info("SSH ì—†ì´ HDFS ë°ëª¬ì„ ì§ì ‘ ì‹œì‘í•©ë‹ˆë‹¤...")
                        return self.start_daemons_direct(
                            hadoop_home, hdfs_env, namenode_ports
                        )
                    else:
                        # SSHë¥¼ í†µí•œ ì¼ë°˜ ì‹œì‘ (í´ëŸ¬ìŠ¤í„° ëª¨ë“œ)
                        logger.info("HDFS ì‹œì‘ ì¤‘...")

                        # sudo ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•œ ê²½ìš° GUIì—ì„œ ì…ë ¥ë°›ê¸° (ê²€ì¦ í¬í•¨)
                        password = None
                        max_password_attempts = 3
                        password_attempt = 0

                        if self.user_password_callback:
                            while password_attempt < max_password_attempts:
                                try:
                                    # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ìš”ì²­
                                    if password_attempt == 0:
                                        message = (
                                            "HDFSë¥¼ ì‹œì‘í•˜ê¸° ìœ„í•´ ì‹œìŠ¤í…œ ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n\n"
                                            "âš ï¸ ë³´ì•ˆ ì•ˆë‚´:\n"
                                            "- ë¹„ë°€ë²ˆí˜¸ëŠ” ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤\n"
                                            "- ì‚¬ìš© í›„ ì¦‰ì‹œ ì‚­ì œë©ë‹ˆë‹¤\n"
                                            "- ì„œë²„ë‚˜ íŒŒì¼ì— ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤\n\n"
                                            "ğŸ’¡ ì…ë ¥í•  ë¹„ë°€ë²ˆí˜¸:\n"
                                            "- macOS/Linux: ì‚¬ìš©ì ê³„ì • ë¹„ë°€ë²ˆí˜¸ (ë¡œê·¸ì¸ ë¹„ë°€ë²ˆí˜¸)\n"
                                            "- sudo ëª…ë ¹ì–´ì— ì‚¬ìš©í•˜ëŠ” ë¹„ë°€ë²ˆí˜¸\n\n"
                                            "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:"
                                        )
                                    else:
                                        message = (
                                            f"âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì‹œë„ {password_attempt}/{max_password_attempts - 1})\n\n"
                                            "ğŸ’¡ í™•ì¸ ì‚¬í•­:\n"
                                            "- macOS ì‚¬ìš©ì ê³„ì • ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”\n"
                                            "- ë¡œê·¸ì¸í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¹„ë°€ë²ˆí˜¸ì…ë‹ˆë‹¤\n\n"
                                            "ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”:"
                                        )

                                    password = self.user_password_callback(
                                        "HDFS ì‹œì‘ - ê´€ë¦¬ì ê¶Œí•œ í•„ìš”",
                                        message,
                                    )

                                    if not password:
                                        logger.warning(
                                            "ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
                                        )
                                        return {
                                            "success": False,
                                            "error": "ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. HDFSë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                        }

                                    # ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ (sudo -Së¡œ í…ŒìŠ¤íŠ¸)
                                    logger.info("ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ ì¤‘...")
                                    test_process = subprocess.Popen(
                                        ["sudo", "-S", "echo", "test"],
                                        stdin=subprocess.PIPE,
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE,
                                        env=hdfs_env,
                                    )

                                    try:
                                        # ë¹„ë°€ë²ˆí˜¸ë¥¼ stdinì— ì“°ê¸°
                                        password_bytes = (password + "\n").encode()
                                        bytes_written = test_process.stdin.write(
                                            password_bytes
                                        )
                                        test_process.stdin.flush()
                                        logger.debug(
                                            f"ë¹„ë°€ë²ˆí˜¸ ì „ë‹¬: {bytes_written} bytes written"
                                        )
                                        # stdin.close()ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - communicate()ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬

                                        # ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
                                        # communicate()ëŠ” stdinì„ ìë™ìœ¼ë¡œ ë‹«ê³  í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                                        stdout, stderr = test_process.communicate(
                                            timeout=5
                                        )

                                        stdout_text = stdout.decode(
                                            "utf-8", errors="ignore"
                                        ).strip()
                                        stderr_text = stderr.decode(
                                            "utf-8", errors="ignore"
                                        ).strip()

                                        if test_process.returncode == 0:
                                            logger.info("âœ… ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ ì„±ê³µ")
                                            break  # ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¦„
                                        else:
                                            # stderrì— ë¹„ë°€ë²ˆí˜¸ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
                                            stderr_lower = stderr_text.lower()
                                            if (
                                                "incorrect password" in stderr_lower
                                                or "sorry" in stderr_lower
                                                or (
                                                    "password" in stderr_lower
                                                    and "try again" in stderr_lower
                                                )
                                                or "no password was provided"
                                                in stderr_lower
                                            ):
                                                logger.warning(
                                                    f"âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì‹œë„ {password_attempt + 1}/{max_password_attempts})"
                                                )
                                                logger.debug(
                                                    f"ì˜¤ë¥˜ ë©”ì‹œì§€: {stderr_text[:200]}"
                                                )
                                                password_attempt += 1
                                                password = None  # ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”
                                                continue  # ì¬ì…ë ¥ ìš”ì²­
                                            else:
                                                # ë‹¤ë¥¸ ì˜¤ë¥˜ (sudo ê¶Œí•œ ì—†ìŒ ë“±)
                                                logger.warning(
                                                    f"ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ (returncode={test_process.returncode}): {stderr_text[:200]}"
                                                )
                                                logger.debug(
                                                    f"stdout: {stdout_text[:100]}"
                                                )
                                                password_attempt += 1
                                                password = None
                                                continue
                                    except subprocess.TimeoutExpired:
                                        test_process.kill()
                                        test_process.wait()
                                        logger.warning("ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ íƒ€ì„ì•„ì›ƒ")
                                        password_attempt += 1
                                        password = None
                                        continue
                                    except Exception as e:
                                        logger.error(f"ë¹„ë°€ë²ˆí˜¸ ê²€ì¦ ì¤‘ ì˜ˆì™¸: {e}")
                                        if test_process.poll() is None:
                                            test_process.kill()
                                            test_process.wait()
                                        password_attempt += 1
                                        password = None
                                        continue
                                    finally:
                                        # ë³´ì•ˆ: í…ŒìŠ¤íŠ¸ìš© ë¹„ë°€ë²ˆí˜¸ ì¦‰ì‹œ ì‚­ì œ
                                        if password:
                                            # ê²€ì¦ ì„±ê³µ ì‹œ passwordëŠ” ìœ ì§€, ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •ë¨
                                            pass

                                except Exception as e:
                                    logger.error(f"ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
                                    password_attempt += 1
                                    password = None
                                    continue

                            # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                            if password_attempt >= max_password_attempts:
                                logger.error(
                                    f"ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì‹¤íŒ¨: ìµœëŒ€ ì‹œë„ íšŸìˆ˜({max_password_attempts}) ì´ˆê³¼"
                                )
                                return {
                                    "success": False,
                                    "error": f"ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ì‹¤íŒ¨: ìµœëŒ€ ì‹œë„ íšŸìˆ˜({max_password_attempts}) ì´ˆê³¼",
                                }

                            if not password:
                                return {
                                    "success": False,
                                    "error": "ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                                }

                        # ë¹„ë°€ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ stdinì„ í†µí•´ sudo -Së¡œ ì „ë‹¬ (ë³´ì•ˆ: shell ëª…ë ¹ì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
                        if password:
                            # sudo -SëŠ” stdinì—ì„œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì½ìŒ
                            # Phase 1 ì´ì „ê³¼ ë™ì¼í•˜ê²Œ bash start-dfs.sh ì‹¤í–‰ (ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ì—ì„œ sudo ì²˜ë¦¬)
                            process = subprocess.Popen(
                                ["bash", str(start_dfs_script)],
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                start_new_session=True,
                                env=hdfs_env,
                                cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
                            )
                            # ë¹„ë°€ë²ˆí˜¸ë¥¼ stdinìœ¼ë¡œ ì „ë‹¬ (ë³´ì•ˆ: ì¦‰ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ)
                            # start-dfs.shê°€ sudoë¥¼ ìš”ì²­í•˜ë©´ stdinì—ì„œ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì½ìŒ
                            try:
                                if process.stdin:
                                    password_bytes = (password + "\n").encode()
                                    process.stdin.write(password_bytes)
                                    process.stdin.flush()
                                    # stdin.close()ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ - í”„ë¡œì„¸ìŠ¤ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬
                                    logger.debug("ë¹„ë°€ë²ˆí˜¸ë¥¼ stdinìœ¼ë¡œ ì „ë‹¬ ì™„ë£Œ")
                            except (ValueError, OSError) as e:
                                # stdinì´ ì´ë¯¸ ë‹«í˜”ê±°ë‚˜ ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°
                                logger.warning(
                                    f"stdin ì „ë‹¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}"
                                )
                            finally:
                                # ë³´ì•ˆ: ë¹„ë°€ë²ˆí˜¸ ì¦‰ì‹œ ì‚­ì œ
                                temp_password = password
                                password = None
                                del temp_password
                                # stdinì€ í”„ë¡œì„¸ìŠ¤ê°€ ìë™ìœ¼ë¡œ ë‹«ìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œ ë‹«ì§€ ì•ŠìŒ
                        else:
                            # ë¹„ë°€ë²ˆí˜¸ ì—†ì´ ì‹¤í–‰ (sudo ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²½ìš°)
                            process = subprocess.Popen(
                                ["bash", str(start_dfs_script)],
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                start_new_session=True,
                                env=hdfs_env,
                                cwd=str(hadoop_path),  # í•˜ë‘¡ ê²½ë¡œì—ì„œ ì‹¤í–‰
                            )

                        # HDFS ì‹œì‘ ëŒ€ê¸° ë° í¬íŠ¸ í™•ì¸
                        max_retries = 15
                        retry_interval = 2

                        for attempt in range(max_retries):
                            time.sleep(retry_interval)

                            # í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                            if process.poll() is not None:
                                try:
                                    stdout, stderr = process.communicate(timeout=2)
                                    if stderr:
                                        stderr_text = stderr.decode(
                                            "utf-8", errors="ignore"
                                        )
                                        # ì˜ˆìƒëœ ê²½ê³  ë©”ì‹œì§€ í•„í„°ë§
                                        expected_warnings = [
                                            "ssh:",
                                            "connection refused",
                                            "nativecodeloader",
                                            "unable to load native-hadoop library",
                                            "using builtin-java classes",
                                        ]
                                        is_expected_warning = any(
                                            warning in stderr_text.lower()
                                            for warning in expected_warnings
                                        )

                                        if is_expected_warning:
                                            # ì˜ˆìƒëœ ê²½ê³ ëŠ” DEBUG ë ˆë²¨ë¡œë§Œ ë¡œê¹… (ë‹¨ì¼ ë…¸ë“œ ëª¨ë“œì—ì„œëŠ” ì •ìƒ)
                                            logger.debug(
                                                f"HDFS ì‹œì‘ ê²½ê³  (ì •ìƒ, ë¬´ì‹œ ê°€ëŠ¥): {stderr_text[:300]}"
                                            )
                                        else:
                                            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ë§Œ WARNINGìœ¼ë¡œ ë¡œê¹…
                                            logger.warning(
                                                f"HDFS ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜ (ì¢…ë£Œ ì½”ë“œ: {process.returncode}): {stderr_text[:500]}"
                                            )
                                            # HDFS ì‹œì‘ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ ë°˜í™˜
                                            return {
                                                "success": False,
                                                "error": f"HDFS ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ì¢…ë£Œ ì½”ë“œ: {process.returncode}): {stderr_text[:300]}",
                                            }
                                    elif stdout:
                                        stdout_text = stdout.decode(
                                            "utf-8", errors="ignore"
                                        )
                                        logger.debug(
                                            f"HDFS ì‹œì‘ stdout: {stdout_text[:300]}"
                                        )
                                except subprocess.TimeoutExpired:
                                    logger.warning("HDFS í”„ë¡œì„¸ìŠ¤ í†µì‹  íƒ€ì„ì•„ì›ƒ")
                                except Exception as e:
                                    logger.error(f"HDFS í”„ë¡œì„¸ìŠ¤ í†µì‹  ì˜¤ë¥˜: {e}")

                            # í¬íŠ¸ í™•ì¸
                            for port in namenode_ports:
                                try:
                                    sock = socket.socket(
                                        socket.AF_INET, socket.SOCK_STREAM
                                    )
                                    sock.settimeout(1)
                                    result = sock.connect_ex(("localhost", port))
                                    sock.close()
                                    if result == 0:
                                        logger.info(
                                            f"âœ… HDFS ì‹œì‘ ì„±ê³µ (í¬íŠ¸ {port}, ì‹œë„ {attempt + 1}/{max_retries})"
                                        )
                                        return {
                                            "success": True,
                                            "message": f"HDFSê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤ (í¬íŠ¸ {port})",
                                        }
                                except Exception as e:
                                    logger.debug(
                                        f"HDFS í¬íŠ¸ í™•ì¸ ì˜¤ë¥˜ (í¬íŠ¸ {port}): {e}"
                                    )

                        return {
                            "success": False,
                            "error": f"HDFS ì‹œì‘ í›„ í¬íŠ¸ í™•ì¸ ì‹¤íŒ¨ (ìµœëŒ€ {max_retries * retry_interval}ì´ˆ ëŒ€ê¸°)",
                        }
                else:
                    return {
                        "success": False,
                        "error": f"HDFS ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {start_dfs_script}",
                    }
            else:
                logger.warning(
                    "HADOOP_HOME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê³  ì¼ë°˜ ê²½ë¡œì—ì„œë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                    "HDFSëŠ” ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤."
                )
                return {
                    "success": False,
                    "error": "HADOOP_HOME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê³  ì¼ë°˜ ê²½ë¡œì—ì„œë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. HDFSëŠ” ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.",
                }
        except Exception as e:
            logger.error(f"HDFS ì²´í¬ ë° ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"success": False, "error": f"HDFS ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
