"""
Scrapyd ìŠ¤ì¼€ì¤„ëŸ¬
í¬ë¡¤ë§ ì‘ì—…ì„ ìŠ¤ì¼€ì¤„ë§
"""

import schedule
import time
import logging
import requests
import yaml
import os
import subprocess
import shutil
import configparser
from datetime import datetime
from pathlib import Path
from typing import Optional

from shared.logger import setup_logger
from shared.path_utils import get_cointicker_root

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
cointicker_root = get_cointicker_root()
log_file = str(cointicker_root / "logs" / "scheduler.log")
logger = setup_logger(__name__, log_file=log_file)


class ScrapydScheduler:
    """Scrapyd ìŠ¤ì¼€ì¤„ëŸ¬"""

    def __init__(self, scrapyd_url: Optional[str] = None):
        """
        ì´ˆê¸°í™”

        Args:
            scrapyd_url: Scrapyd ì„œë²„ URL (Noneì´ë©´ ì„¤ì • íŒŒì¼ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
        """
        # ì„¤ì • íŒŒì¼ì—ì„œ scrapyd_url ë¡œë“œ
        if scrapyd_url is None:
            scrapyd_url = self._load_scrapyd_url()

        # scrapyd_urlì´ Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if scrapyd_url is None:
            scrapyd_url = "http://localhost:6800"

        self.scrapyd_url = scrapyd_url
        self.project = "cointicker"
        self.spiders = self._load_spider_config()
        self.scrapyd_process = None

        # í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • (ë°°í¬ìš©)
        cointicker_root = get_cointicker_root()
        self.project_path = cointicker_root / "worker-nodes" / "cointicker"

    def _load_scrapyd_url(self):
        """ì„¤ì • íŒŒì¼ì—ì„œ Scrapyd URL ë¡œë“œ"""
        try:
            from shared.path_utils import get_cointicker_root

            config_file = get_cointicker_root() / "config" / "spider_config.yaml"
            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    # spider_config.yamlì— scrapyd_urlì´ ìˆìœ¼ë©´ ì‚¬ìš©
                    if config and "scrapyd" in config:
                        return config["scrapyd"].get("url", "http://localhost:6800")
        except Exception as e:
            logger.debug(f"Failed to load scrapyd_url from config: {e}")

        # í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’
        return os.getenv("SCRAPYD_URL", "http://localhost:6800")

    def _load_spider_config(self):
        """spider_config.yamlì—ì„œ Spider ìŠ¤ì¼€ì¤„ ì •ë³´ ë¡œë“œ"""
        try:
            from shared.path_utils import get_cointicker_root

            config_file = get_cointicker_root() / "config" / "spider_config.yaml"
            if config_file.exists():
                with open(config_file, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    if config and "spiders" in config:
                        spiders = {}
                        for name, spider_config in config["spiders"].items():
                            if spider_config.get("enabled", True):
                                spiders[name] = {
                                    "schedule": spider_config.get(
                                        "schedule", "*/5 * * * *"
                                    ),
                                }
                        enabled_names = list(spiders.keys())
                        logger.info(
                            f"âœ… {len(spiders)}ê°œ Spider ë¡œë“œ ì™„ë£Œ: {enabled_names}"
                        )
                        return spiders
        except Exception as e:
            logger.warning(f"Failed to load spider_config.yaml: {e}")

        # ê¸°ë³¸ê°’ (ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
        default_spiders = {
            "upbit_trends": {"schedule": "*/5 * * * *"},
            "saveticker": {"schedule": "*/5 * * * *"},
            "coinness": {"schedule": "*/10 * * * *"},
            "perplexity": {"schedule": "0 * * * *"},
            "cnn_fear_greed": {"schedule": "0 0 * * *"},
        }
        logger.warning(
            f"âš ï¸ spider_config.yaml ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ê°’ ì‚¬ìš©: {list(default_spiders.keys())}"
        )
        return default_spiders

    def _install_scrapyd(self):
        """Scrapyd ìë™ ì„¤ì¹˜"""
        try:
            import sys

            python_cmd = sys.executable

            logger.info("Scrapyd íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ ì‹œë„ ì¤‘...")
            result = subprocess.run(
                [python_cmd, "-m", "pip", "install", "scrapyd>=1.3.0"],
                capture_output=True,
                text=True,
                timeout=120,
            )

            if result.returncode == 0:
                logger.info("âœ… Scrapyd ì„¤ì¹˜ ì™„ë£Œ")
                return True
            else:
                logger.error(f"Scrapyd ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")
                return False
        except subprocess.TimeoutExpired:
            logger.error("Scrapyd ì„¤ì¹˜ íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
            return False
        except Exception as e:
            logger.error(f"Scrapyd ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _install_scrapyd_client(self):
        """scrapyd-client ìë™ ì„¤ì¹˜ (scrapyd-deploy ëª…ë ¹ì–´ í¬í•¨)"""
        try:
            import sys

            python_cmd = sys.executable

            logger.info("scrapyd-client íŒ¨í‚¤ì§€ ìë™ ì„¤ì¹˜ ì‹œë„ ì¤‘...")
            result = subprocess.run(
                [python_cmd, "-m", "pip", "install", "scrapyd-client>=1.2.0"],
                capture_output=True,
                text=True,
                timeout=120,
            )

            if result.returncode == 0:
                logger.info("âœ… scrapyd-client ì„¤ì¹˜ ì™„ë£Œ")
                return True
            else:
                logger.error(f"scrapyd-client ì„¤ì¹˜ ì‹¤íŒ¨: {result.stderr}")
                return False
        except subprocess.TimeoutExpired:
            logger.error("scrapyd-client ì„¤ì¹˜ íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
            return False
        except Exception as e:
            logger.error(f"scrapyd-client ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _start_scrapyd_server(self):
        """Scrapyd ì„œë²„ ìë™ ì‹œì‘"""
        try:
            # scrapyd ëª…ë ¹ì–´ í™•ì¸ (ì—¬ëŸ¬ venv ê²½ë¡œ í™•ì¸)
            scrapyd_cmd = shutil.which("scrapyd")
            if not scrapyd_cmd:
                # venvì˜ scrapyd í™•ì¸ (ìš°ì„ ìˆœìœ„: PICU/venv > cointicker/venv > bigdata/venv)
                cointicker_root = get_cointicker_root()
                project_root = cointicker_root.parent  # PICU/
                bigdata_root = project_root.parent  # bigdata/

                venv_paths = [
                    project_root / "venv" / "bin" / "scrapyd",  # PICU/venv
                    cointicker_root / "venv" / "bin" / "scrapyd",  # cointicker/venv
                    bigdata_root / "venv" / "bin" / "scrapyd",  # bigdata/venv
                ]

                for venv_scrapyd in venv_paths:
                    if venv_scrapyd.exists():
                        scrapyd_cmd = str(venv_scrapyd)
                        logger.info(f"Scrapyd ëª…ë ¹ì–´ ë°œê²¬: {scrapyd_cmd}")
                        break

                if not scrapyd_cmd:
                    # scrapydê°€ ì—†ìœ¼ë©´ ìë™ ì„¤ì¹˜ ì‹œë„
                    logger.warning(
                        "scrapyd ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë™ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤..."
                    )
                    if self._install_scrapyd():
                        # ì„¤ì¹˜ í›„ ë‹¤ì‹œ í™•ì¸
                        scrapyd_cmd = shutil.which("scrapyd")
                        if not scrapyd_cmd:
                            # venv ê²½ë¡œ ë‹¤ì‹œ í™•ì¸
                            for venv_scrapyd in venv_paths:
                                if venv_scrapyd.exists():
                                    scrapyd_cmd = str(venv_scrapyd)
                                    break

                    if not scrapyd_cmd:
                        logger.error(
                            "scrapyd ì„¤ì¹˜ í›„ì—ë„ ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                            "ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install scrapyd"
                        )
                        return False

            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
            if self._check_scrapyd_connection():
                logger.info("Scrapyd ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
                return True

            # Scrapyd ì„œë²„ ì‹œì‘
            logger.info(f"Scrapyd ì„œë²„ ì‹œì‘ ì¤‘: {scrapyd_cmd}")
            self.scrapyd_process = subprocess.Popen(
                [scrapyd_cmd],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                start_new_session=True,  # ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œì—ë„ ê³„ì† ì‹¤í–‰
            )

            # ì‹œì‘ í™•ì¸ (ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°)
            for _ in range(5):
                time.sleep(1)
                if self._check_scrapyd_connection():
                    logger.info("âœ… Scrapyd ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                    return True

            logger.warning("Scrapyd ì„œë²„ ì‹œì‘ í™•ì¸ ì‹¤íŒ¨ (íƒ€ì„ì•„ì›ƒ)")
            return False

        except Exception as e:
            logger.error(f"Scrapyd ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False

    def _check_scrapyd_connection(self):
        """Scrapyd ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            url = f"{self.scrapyd_url}/listprojects.json"
            response = requests.get(url, timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.debug(f"Scrapyd connection check failed: {e}")
            return False

    def _deploy_project(self):
        """Scrapy í”„ë¡œì íŠ¸ë¥¼ Scrapydì— ë°°í¬"""
        try:
            # í”„ë¡œì íŠ¸ê°€ ì´ë¯¸ ë°°í¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            url = f"{self.scrapyd_url}/listprojects.json"
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                projects = response.json().get("projects", [])
                if self.project in projects:
                    logger.info(f"í”„ë¡œì íŠ¸ '{self.project}'ê°€ ì´ë¯¸ ë°°í¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    return True

            # scrapyd-deploy ëª…ë ¹ì–´ ì°¾ê¸°
            deploy_cmd = shutil.which("scrapyd-deploy")
            if not deploy_cmd:
                # venvì˜ scrapyd-deploy í™•ì¸
                cointicker_root = get_cointicker_root()
                project_root = cointicker_root.parent  # PICU/
                bigdata_root = project_root.parent  # bigdata/

                venv_paths = [
                    project_root / "venv" / "bin" / "scrapyd-deploy",  # PICU/venv
                    cointicker_root
                    / "venv"
                    / "bin"
                    / "scrapyd-deploy",  # cointicker/venv
                    bigdata_root / "venv" / "bin" / "scrapyd-deploy",  # bigdata/venv
                ]

                for venv_deploy in venv_paths:
                    if venv_deploy.exists():
                        deploy_cmd = str(venv_deploy)
                        logger.info(f"scrapyd-deploy ëª…ë ¹ì–´ ë°œê²¬: {deploy_cmd}")
                        break

                if not deploy_cmd:
                    # scrapyd-deployê°€ ì—†ìœ¼ë©´ ìë™ ì„¤ì¹˜ ì‹œë„
                    logger.warning(
                        "scrapyd-deploy ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìë™ ì„¤ì¹˜ë¥¼ ì‹œë„í•©ë‹ˆë‹¤..."
                    )
                    if self._install_scrapyd_client():
                        # ì„¤ì¹˜ í›„ ë‹¤ì‹œ í™•ì¸
                        deploy_cmd = shutil.which("scrapyd-deploy")
                        if not deploy_cmd:
                            # venv ê²½ë¡œ ë‹¤ì‹œ í™•ì¸
                            for venv_deploy in venv_paths:
                                if venv_deploy.exists():
                                    deploy_cmd = str(venv_deploy)
                                    break

                    if not deploy_cmd:
                        logger.error(
                            "scrapyd-deploy ì„¤ì¹˜ í›„ì—ë„ ëª…ë ¹ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                            "ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install scrapyd-client"
                        )
                        return False

            # scrapy.cfg íŒŒì¼ í™•ì¸
            scrapy_cfg = self.project_path / "scrapy.cfg"
            if not scrapy_cfg.exists():
                logger.error(f"scrapy.cfg íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scrapy_cfg}")
                return False

            # scrapy.cfgì— deploy URL ì„¤ì • (ì—†ìœ¼ë©´ ì¶”ê°€)
            import configparser

            config = configparser.ConfigParser()
            config.read(scrapy_cfg)

            if "deploy" not in config:
                config.add_section("deploy")

            # deploy URL ì„¤ì •
            config.set("deploy", "url", self.scrapyd_url)
            config.set("deploy", "project", self.project)

            with open(scrapy_cfg, "w") as f:
                config.write(f)

            logger.info(f"í”„ë¡œì íŠ¸ ë°°í¬ ì¤‘: {self.project_path} -> {self.scrapyd_url}")

            # scrapyd-deploy ì‹¤í–‰
            result = subprocess.run(
                [deploy_cmd],
                cwd=str(self.project_path),
                capture_output=True,
                text=True,
                timeout=60,
            )

            if result.returncode == 0:
                logger.info(f"âœ… í”„ë¡œì íŠ¸ '{self.project}' ë°°í¬ ì™„ë£Œ")
                return True
            else:
                logger.error(f"í”„ë¡œì íŠ¸ ë°°í¬ ì‹¤íŒ¨: {result.stderr}")
                return False

        except Exception as e:
            logger.error(f"í”„ë¡œì íŠ¸ ë°°í¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def schedule_spider(self, spider_name: str):
        """
        Spider ìŠ¤ì¼€ì¤„ë§

        Args:
            spider_name: Spider ì´ë¦„
        """
        # Scrapyd ì„œë²„ ì—°ê²° í™•ì¸
        if not self._check_scrapyd_connection():
            logger.error(
                f"Scrapyd ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.scrapyd_url}\n"
                f"Scrapyd ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: scrapyd ë˜ëŠ” scrapyd &"
            )
            return False

        try:
            url = f"{self.scrapyd_url}/schedule.json"
            data = {"project": self.project, "spider": spider_name}

            response = requests.post(url, data=data, timeout=10)
            if response.status_code == 200:
                result = response.json()
                if result.get("status") == "ok":
                    job_id = result.get("jobid", "unknown")
                    logger.info(f"Scheduled spider: {spider_name} (jobid: {job_id})")
                    return True
                else:
                    logger.error(
                        f"Failed to schedule {spider_name}: {result.get('message', 'unknown error')}"
                    )
                    return False
            else:
                logger.error(
                    f"Failed to schedule {spider_name}: HTTP {response.status_code} - {response.text}"
                )
                return False

        except requests.exceptions.ConnectionError as e:
            logger.error(
                f"Scrapyd ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {self.scrapyd_url}\n"
                f"ì˜¤ë¥˜: {e}\n"
                f"Scrapyd ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”: scrapyd ë˜ëŠ” scrapyd &"
            )
            return False
        except Exception as e:
            logger.error(f"Error scheduling {spider_name}: {e}")
            return False

    def start(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        # Scrapyd ì„œë²„ ì—°ê²° í™•ì¸ ë° ìë™ ì‹œì‘
        if not self._check_scrapyd_connection():
            logger.warning(
                f"âš ï¸ Scrapyd ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.scrapyd_url}\n"
                f"Scrapyd ì„œë²„ë¥¼ ìë™ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤..."
            )
            if not self._start_scrapyd_server():
                logger.error(
                    f"Scrapyd ì„œë²„ ìë™ ì‹œì‘ ì‹¤íŒ¨. ìˆ˜ë™ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”:\n"
                    f"  scrapyd\n"
                    f"ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰:\n"
                    f"  scrapyd &\n"
                    f"ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ê³„ì† ì‹¤í–‰ë˜ì§€ë§Œ Scrapydê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ì‘ì—… í• ë‹¹ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤."
                )
                return
        else:
            logger.info(f"âœ… Scrapyd ì„œë²„ ì—°ê²° í™•ì¸: {self.scrapyd_url}")

        # í”„ë¡œì íŠ¸ ë°°í¬ í™•ì¸ ë° ìë™ ë°°í¬
        logger.info("í”„ë¡œì íŠ¸ ë°°í¬ ìƒíƒœ í™•ì¸ ì¤‘...")
        if not self._deploy_project():
            logger.warning(
                "í”„ë¡œì íŠ¸ ë°°í¬ ì‹¤íŒ¨. ìŠ¤ì¼€ì¤„ë§ì€ ê³„ì† ì‹œë„í•˜ì§€ë§Œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                "ìˆ˜ë™ìœ¼ë¡œ ë°°í¬í•˜ì„¸ìš”: cd worker-nodes/cointicker && scrapyd-deploy"
            )

        # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œí•œ Spider ìŠ¤ì¼€ì¤„ ë“±ë¡
        for spider_name, spider_info in self.spiders.items():
            schedule_str = spider_info.get("schedule", "*/5 * * * *")
            # cron í˜•ì‹ íŒŒì‹± (ê°„ë‹¨í•œ í˜•ì‹ë§Œ ì§€ì›: "*/5 * * * *" -> 5ë¶„ë§ˆë‹¤)
            if schedule_str.startswith("*/"):
                minutes = int(schedule_str.split()[0].replace("*/", ""))
                schedule.every(minutes).minutes.do(
                    lambda name=spider_name: self.schedule_spider(name)
                )
            elif schedule_str.startswith("0 * * * *"):
                schedule.every(1).hours.do(
                    lambda name=spider_name: self.schedule_spider(name)
                )
            elif schedule_str.startswith("0 0 * * *"):
                schedule.every().day.at("00:00").do(
                    lambda name=spider_name: self.schedule_spider(name)
                )
            else:
                # ê¸°ë³¸ê°’: 5ë¶„ë§ˆë‹¤
                schedule.every(5).minutes.do(
                    lambda name=spider_name: self.schedule_spider(name)
                )

        enabled_spider_names = list(self.spiders.keys())
        logger.info("=" * 60)
        logger.info(f"âœ… Scrapyd ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")
        logger.info(f"ğŸ“‹ ìŠ¤ì¼€ì¤„ë§ ëŒ€ìƒ Spider ({len(self.spiders)}ê°œ):")
        for spider_name, spider_info in self.spiders.items():
            schedule_str = spider_info.get("schedule", "*/5 * * * *")
            logger.info(f"  - {spider_name}: {schedule_str}")
        logger.info("=" * 60)
        logger.info(
            f"ğŸ’¡ ì°¸ê³ : spider_config.yamlì—ì„œ enabled: falseë¡œ ì„¤ì •í•˜ë©´ ìŠ¤ì¼€ì¤„ë§ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤."
        )

        while True:
            schedule.run_pending()
            time.sleep(60)


if __name__ == "__main__":
    scheduler = ScrapydScheduler()
    scheduler.start()
