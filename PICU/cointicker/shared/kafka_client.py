"""
Kafka í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹°
Producerì™€ Consumerë¥¼ ìœ„í•œ ê³µí†µ í´ë¼ì´ì–¸íŠ¸
"""

import json
import logging
import re
from typing import Optional, List, Dict, Any
from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient
from kafka.errors import KafkaError
from loguru import logger


class KafkaClient:
    """Kafka í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
    ):
        """
        Kafka í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.bootstrap_servers = bootstrap_servers or ["localhost:9092"]
        self.timeout = timeout
        self.logger = logger

    def _get_servers_str(self) -> str:
        """ë¸Œë¡œì»¤ ì„œë²„ ì£¼ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return ",".join(self.bootstrap_servers)


class KafkaProducerClient(KafkaClient):
    """Kafka Producer í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
        value_serializer=None,
        key_serializer=None,
        acks: str = "all",
        retries: int = 3,
        compression_type: str = "gzip",
        linger_ms: int = 100,
    ):
        """
        Kafka Producer ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            value_serializer: ê°’ ì§ë ¬í™” í•¨ìˆ˜
            key_serializer: í‚¤ ì§ë ¬í™” í•¨ìˆ˜
            acks: ACK ì„¤ì • ("all", "1", "0")
            retries: ì¬ì‹œë„ íšŸìˆ˜
            compression_type: ì••ì¶• íƒ€ì… ("gzip", "snappy", "lz4", "zstd", None)
            linger_ms: ë°°ì¹˜ ì „ì†¡ ì „ ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        super().__init__(bootstrap_servers, timeout)

        # ê¸°ë³¸ ì§ë ¬í™” í•¨ìˆ˜
        if value_serializer is None:
            value_serializer = lambda v: json.dumps(v, ensure_ascii=False).encode(
                "utf-8"
            )
        if key_serializer is None:
            key_serializer = lambda k: k.encode("utf-8") if k else None

        self.producer = None
        self.value_serializer = value_serializer
        self.key_serializer = key_serializer
        self.acks = acks
        self.retries = retries
        self.compression_type = compression_type
        self.linger_ms = linger_ms

    def connect(self) -> bool:
        """Producer ì—°ê²°"""
        try:
            producer_config = {
                "bootstrap_servers": self.bootstrap_servers,
                "value_serializer": self.value_serializer,
                "key_serializer": self.key_serializer,
                "acks": self.acks,
                "retries": self.retries,
                "request_timeout_ms": self.timeout * 1000,
            }

            # ê³ ê¸‰ ì„¤ì • ì¶”ê°€ (kafka_projectì˜ producer.properties ì°¸ê³ )
            if self.compression_type:
                producer_config["compression_type"] = self.compression_type
            if self.linger_ms:
                producer_config["linger_ms"] = self.linger_ms

            self.producer = KafkaProducer(**producer_config)
            self.logger.info(
                f"Kafka Producer connected to {self._get_servers_str()} "
                f"(compression={self.compression_type}, linger_ms={self.linger_ms})"
            )
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect Kafka Producer: {e}")
            return False

    def send(
        self,
        topic: str,
        value: Any,
        key: Optional[str] = None,
        partition: Optional[int] = None,
    ) -> bool:
        """
        ë©”ì‹œì§€ ì „ì†¡

        Args:
            topic: í† í”½ ì´ë¦„
            value: ë©”ì‹œì§€ ê°’
            key: ë©”ì‹œì§€ í‚¤ (ì„ íƒ)
            partition: íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì„ íƒ)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.producer:
            if not self.connect():
                return False

        try:
            future = self.producer.send(
                topic,
                value=value,
                key=key,
                partition=partition,
            )
            record_metadata = future.get(timeout=self.timeout)
            self.logger.debug(
                f"Message sent to topic={topic}, "
                f"partition={record_metadata.partition}, "
                f"offset={record_metadata.offset}"
            )
            return True
        except KafkaError as e:
            self.logger.error(f"Kafka send error: {e}")
            return False
        except Exception as e:
            self.logger.error(f"Unexpected error sending message: {e}")
            return False

    def send_batch(self, topic: str, messages: List[Dict[str, Any]]) -> int:
        """
        ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡

        Args:
            topic: í† í”½ ì´ë¦„
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ê° ë©”ì‹œì§€ëŠ” {"key": ..., "value": ...} í˜•ì‹)

        Returns:
            ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ëœ ë©”ì‹œì§€ ìˆ˜
        """
        if not self.producer:
            if not self.connect():
                return 0

        success_count = 0
        for msg in messages:
            key = msg.get("key")
            value = msg.get("value")
            if self.send(topic, value, key):
                success_count += 1

        return success_count

    def send_with_callback(
        self,
        topic: str,
        value: Any,
        key: Optional[str] = None,
        partition: Optional[int] = None,
        callback=None,
    ):
        """
        Callbackì„ ì‚¬ìš©í•œ ë¹„ë™ê¸° ë©”ì‹œì§€ ì „ì†¡ (kafka_projectì˜ CallbackProducer ì°¸ê³ )

        Args:
            topic: í† í”½ ì´ë¦„
            value: ë©”ì‹œì§€ ê°’
            key: ë©”ì‹œì§€ í‚¤ (ì„ íƒ)
            partition: íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì„ íƒ)
            callback: ì½œë°± í•¨ìˆ˜ (metadata, exception) -> None

        Returns:
            Future ê°ì²´ (ì„ íƒì )
        """
        if not self.producer:
            if not self.connect():
                return None

        try:
            future = self.producer.send(
                topic,
                value=value,
                key=key,
                partition=partition,
            )

            # Callbackì´ ì œê³µë˜ë©´ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (kafka-pythonì˜ FutureëŠ” add_callback/add_errback ì‚¬ìš©)
            if callback:

                def on_success(record_metadata):
                    """ì„±ê³µ ì‹œ ì½œë°±"""
                    try:
                        callback(record_metadata, None)
                    except Exception as e:
                        self.logger.error(f"Error in callback: {e}")

                def on_error(exception):
                    """ì‹¤íŒ¨ ì‹œ ì½œë°±"""
                    try:
                        callback(None, exception)
                    except Exception as e:
                        self.logger.error(f"Error in error callback: {e}")

                future.add_callback(on_success)
                future.add_errback(on_error)
                return None  # Callbackì´ ìˆìœ¼ë©´ Futureë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            else:
                return future  # Callbackì´ ì—†ìœ¼ë©´ Future ë°˜í™˜
        except Exception as e:
            self.logger.error(f"Error sending message with callback: {e}")
            if callback:
                callback(None, e)
            return None

    def flush(self):
        """Producer ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if self.producer:
            self.producer.flush()

    def close(self):
        """Producer ì¢…ë£Œ"""
        if self.producer:
            self.producer.close()
            self.logger.info("Kafka Producer closed")


class KafkaConsumerClient(KafkaClient):
    """Kafka Consumer í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
        group_id: str = "cointicker-consumer",
        auto_offset_reset: str = "earliest",
        enable_auto_commit: bool = True,
        value_deserializer=None,
        key_deserializer=None,
    ):
        """
        Kafka Consumer ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            group_id: Consumer Group ID
            auto_offset_reset: ì˜¤í”„ì…‹ ë¦¬ì…‹ ë°©ì‹ ("earliest", "latest")
            enable_auto_commit: ìë™ ì»¤ë°‹ ì—¬ë¶€
            value_deserializer: ê°’ ì—­ì§ë ¬í™” í•¨ìˆ˜
            key_deserializer: í‚¤ ì—­ì§ë ¬í™” í•¨ìˆ˜
        """
        super().__init__(bootstrap_servers, timeout)

        # ê¸°ë³¸ ì—­ì§ë ¬í™” í•¨ìˆ˜
        if value_deserializer is None:
            value_deserializer = lambda v: json.loads(v.decode("utf-8")) if v else None
        if key_deserializer is None:
            key_deserializer = lambda k: k.decode("utf-8") if k else None

        self.consumer = None
        self.group_id = group_id
        self.auto_offset_reset = auto_offset_reset
        self.enable_auto_commit = enable_auto_commit
        self.value_deserializer = value_deserializer
        self.key_deserializer = key_deserializer

    def connect(
        self, topics: List[str], max_retries: int = 3, retry_delay: float = 2.0
    ) -> bool:
        """
        Consumer ì—°ê²° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

        Args:
            topics: êµ¬ë…í•  í† í”½ ë¦¬ìŠ¤íŠ¸ (ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì§€ì›, ì˜ˆ: "cointicker.raw.*")
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 3)
            retry_delay: ì¬ì‹œë„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 2.0)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        from gui.core.retry_utils import execute_with_retry

        def _connect_attempt():
            return self._connect_internal(topics)

        try:
            return execute_with_retry(
                _connect_attempt,
                max_retries=max_retries,
                delay=retry_delay,
                backoff_factor=2.0,
                exceptions=(Exception,),
                on_retry=lambda attempt, e: self.logger.warning(
                    f"Kafka Consumer ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_retries}): {e}. ì¬ì‹œë„ ì¤‘..."
                ),
            )
        except Exception as e:
            self.logger.error(f"Kafka Consumer ì—°ê²° ìµœì¢… ì‹¤íŒ¨: {e}")
            return False

    def _connect_internal(self, topics: List[str]) -> bool:
        """
        Consumer ì—°ê²° ë‚´ë¶€ êµ¬í˜„ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)

        ì´ˆê¸° ìŠ¤ìº”ì€ AdminClientë¡œ ìˆ˜í–‰í•˜ì—¬ ì¦‰ì‹œ í† í”½ ëª©ë¡ì„ í™•ì¸í•˜ê³ ,
        ì‹¤ì œ êµ¬ë…ì€ Kafka ë„¤ì´í‹°ë¸Œ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ì—…ë°ì´íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.

        Args:
            topics: êµ¬ë…í•  í† í”½ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì™€ì¼ë“œì¹´ë“œê°€ í¬í•¨ëœ í† í”½ì´ ìˆëŠ”ì§€ í™•ì¸
            pattern_topics = []
            direct_topics = []

            for topic in topics:
                if "*" in topic or "?" in topic:
                    # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì€ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥ (Kafkaê°€ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬)
                    pattern_topics.append(topic)
                else:
                    direct_topics.append(topic)

            # íŒ¨í„´ì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì‚¬ìš©
            if pattern_topics:
                # ğŸ” 1ë‹¨ê³„: AdminClientë¡œ ì´ˆê¸° í† í”½ ìŠ¤ìº” (ë””ë²„ê¹… ë° ë¡œê¹…ìš©)
                admin_client = None
                initial_matched_topics = []

                try:
                    admin_client = KafkaAdminClient(
                        bootstrap_servers=self.bootstrap_servers,
                        client_id=f"{self.group_id}-admin",
                    )
                    # ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ
                    all_topics = admin_client.list_topics()

                    # ê° íŒ¨í„´ì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í† í”½ ì°¾ê¸°
                    for pattern_str in pattern_topics:
                        # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì„ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜
                        # * -> .*, ? -> ., . -> \.
                        pattern_regex = (
                            pattern_str.replace(".", r"\.")
                            .replace("*", ".*")
                            .replace("?", ".")
                        )
                        compiled_pattern = re.compile(f"^{pattern_regex}$")

                        for topic in all_topics:
                            if compiled_pattern.match(topic):
                                if topic not in initial_matched_topics:
                                    initial_matched_topics.append(topic)

                    self.logger.info(
                        f"ğŸ” Initial pattern matching: {pattern_topics} -> "
                        f"{len(initial_matched_topics)} topics found: {initial_matched_topics}"
                    )

                    if not initial_matched_topics:
                        self.logger.warning(
                            f"No topics matched pattern(s): {pattern_topics}. "
                            f"Available topics: {sorted(all_topics)[:10]}... "
                            f"(Will auto-subscribe when topics are created)"
                        )

                except Exception as e:
                    self.logger.warning(
                        f"Failed to list topics for initial scan: {e}. "
                        f"Continuing with pattern subscription..."
                    )
                finally:
                    if admin_client:
                        try:
                            admin_client.close()
                        except:
                            pass

                # ğŸš€ 2ë‹¨ê³„: Consumer ìƒì„± ë° Kafka ë„¤ì´í‹°ë¸Œ íŒ¨í„´ êµ¬ë…
                # consumer_timeout_msë¥¼ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì • (ë¬´í•œ ëŒ€ê¸° íš¨ê³¼, Python 3.14 í˜¸í™˜)
                self.consumer = KafkaConsumer(
                    bootstrap_servers=self.bootstrap_servers,
                    group_id=self.group_id,
                    auto_offset_reset=self.auto_offset_reset,
                    enable_auto_commit=self.enable_auto_commit,
                    value_deserializer=self.value_deserializer,
                    key_deserializer=self.key_deserializer,
                    consumer_timeout_ms=2147483647,  # ë¬´í•œ ëŒ€ê¸° (Python 3.14 í˜¸í™˜)
                )

                # Kafka ë„¤ì´í‹°ë¸Œ íŒ¨í„´ êµ¬ë… (ìë™ ì—…ë°ì´íŠ¸ í™œì„±í™”)
                pattern_str = pattern_topics[0]
                # ì™€ì¼ë“œì¹´ë“œë¥¼ Java ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜
                kafka_pattern = f"^{pattern_str.replace('.', r'\\.').replace('*', '.*').replace('?', '.')}$"

                try:
                    self.consumer.subscribe(pattern=kafka_pattern)
                    self.logger.info(
                        f"ğŸ¯ Kafka Consumer subscribed with pattern: {kafka_pattern}, "
                        f"group_id={self.group_id}, mode=AUTO-UPDATE"
                    )
                except Exception as e:
                    self.logger.error(
                        f"Failed to subscribe with pattern {kafka_pattern}: {e}"
                    )
                    return False

                if len(pattern_topics) > 1:
                    self.logger.warning(
                        f"Multiple patterns provided, using first pattern: {pattern_str}"
                    )

                # êµ¬ë… í™•ì¸ (poll ì „ì—ëŠ” ë¹ˆ setì¼ ìˆ˜ ìˆìŒ)
                subscription = self.consumer.subscription()
                self.logger.info(
                    f"âœ… Kafka Consumer subscription confirmed: {subscription} "
                    f"(will auto-update as new topics are created)"
                )
            elif direct_topics:
                # ì§ì ‘ í† í”½ êµ¬ë…
                # consumer_timeout_msë¥¼ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì • (ë¬´í•œ ëŒ€ê¸° íš¨ê³¼, Python 3.14 í˜¸í™˜)
                # 2147483647 = 2^31 - 1 (ì•½ 24ì¼)
                self.consumer = KafkaConsumer(
                    *direct_topics,
                    bootstrap_servers=self.bootstrap_servers,
                    group_id=self.group_id,
                    auto_offset_reset=self.auto_offset_reset,
                    enable_auto_commit=self.enable_auto_commit,
                    value_deserializer=self.value_deserializer,
                    key_deserializer=self.key_deserializer,
                    consumer_timeout_ms=2147483647,  # ë¬´í•œ ëŒ€ê¸° (Python 3.14 í˜¸í™˜)
                )
                # êµ¬ë… í™•ì¸
                subscription = self.consumer.subscription()
                self.logger.info(
                    f"Kafka Consumer connected to {self._get_servers_str()}, "
                    f"topics={direct_topics}, group_id={self.group_id}, subscription={subscription}"
                )
            else:
                # í† í”½ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
                self.logger.error("No topics or patterns provided")
                return False

            return True
        except Exception as e:
            self.logger.error(f"Failed to connect Kafka Consumer: {e}", exc_info=True)
            raise  # ì¬ì‹œë„ ë¡œì§ì„ ìœ„í•´ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´

    def consume(self, callback=None, max_messages: Optional[int] = None):
        """
        ë©”ì‹œì§€ ì†Œë¹„

        Args:
            callback: ë©”ì‹œì§€ ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ (message -> None)
            max_messages: ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (Noneì´ë©´ ë¬´ì œí•œ)
        """
        if not self.consumer:
            self.logger.error("Consumer not connected")
            return

        message_count = 0
        try:
            self.logger.info("Starting message consumption loop...")
            for message in self.consumer:
                if callback:
                    callback(message)
                else:
                    self.logger.info(
                        f"Received message: topic={message.topic}, "
                        f"partition={message.partition}, "
                        f"offset={message.offset}, "
                        f"key={message.key}, "
                        f"value={message.value}"
                    )

                message_count += 1
                if max_messages and message_count >= max_messages:
                    self.logger.info(f"Reached max_messages limit: {max_messages}")
                    break

            self.logger.info(
                f"Message consumption loop ended. Total messages: {message_count}"
            )

        except KeyboardInterrupt:
            self.logger.info("Consumer interrupted by user")
        except Exception as e:
            self.logger.error(f"Error consuming messages: {e}", exc_info=True)

    def get_consumer_groups(self) -> Dict[str, Any]:
        """
        Consumer Groups ìƒíƒœ ì¡°íšŒ

        Returns:
            Consumer Groups ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.consumer:
            return {"error": "Consumer not connected"}

        try:
            # Consumerì˜ ê·¸ë£¹ IDì™€ êµ¬ë… ì •ë³´
            subscription = self.consumer.subscription()
            assignment = self.consumer.assignment()

            return {
                "group_id": self.group_id,
                "subscription": list(subscription) if subscription else [],
                "assignment": (
                    [
                        {
                            "topic": tp.topic,
                            "partition": tp.partition,
                        }
                        for tp in assignment
                    ]
                    if assignment
                    else []
                ),
                "num_partitions": len(assignment) if assignment else 0,
            }
        except Exception as e:
            self.logger.error(f"Failed to get consumer groups: {e}")
            return {"error": str(e)}

    def close(self):
        """Consumer ì¢…ë£Œ"""
        if self.consumer:
            self.consumer.close()
            self.logger.info("Kafka Consumer closed")
