"""
Kafka í´ë¼ì´ì–¸íŠ¸ ìœ í‹¸ë¦¬í‹°
Producerì™€ Consumerë¥¼ ìœ„í•œ ê³µí†µ í´ë¼ì´ì–¸íŠ¸
"""

import json
import logging
import re
from typing import Optional, List, Dict, Any
from pathlib import Path
from kafka import KafkaProducer, KafkaConsumer, KafkaAdminClient
from kafka.errors import KafkaError
from loguru import logger

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
from shared.path_utils import get_cointicker_root

cointicker_root = get_cointicker_root()
log_file = cointicker_root / "logs" / "kafka_client.log"
log_file.parent.mkdir(parents=True, exist_ok=True)

# loguru íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€
logger.add(
    str(log_file),
    rotation="10 MB",  # 10MBë§ˆë‹¤ ë¡œê·¸ íŒŒì¼ íšŒì „
    retention="7 days",  # 7ì¼ í›„ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ
    encoding="utf-8",
    level="INFO",
)


class KafkaClient:
    """Kafka í´ë¼ì´ì–¸íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
    ):
        """
        Kafka í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        """
        self.bootstrap_servers = bootstrap_servers or ["localhost:9092"]
        self.timeout = timeout
        self.logger = logger

    def _get_servers_str(self) -> str:
        """ë¸Œë¡œì»¤ ì„œë²„ ì£¼ì†Œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜"""
        return ",".join(self.bootstrap_servers)


class KafkaProducerClient(KafkaClient):
    """Kafka Producer í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
        value_serializer=None,
        key_serializer=None,
        acks: str = "all",
        retries: int = 3,
        compression_type: str = "gzip",
        linger_ms: int = 100,
    ):
        """
        Kafka Producer ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            value_serializer: ê°’ ì§ë ¬í™” í•¨ìˆ˜
            key_serializer: í‚¤ ì§ë ¬í™” í•¨ìˆ˜
            acks: ACK ì„¤ì • ("all", "1", "0")
            retries: ì¬ì‹œë„ íšŸìˆ˜
            compression_type: ì••ì¶• íƒ€ì… ("gzip", "snappy", "lz4", "zstd", None)
            linger_ms: ë°°ì¹˜ ì „ì†¡ ì „ ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        """
        super().__init__(bootstrap_servers, timeout)

        # ê¸°ë³¸ ì§ë ¬í™” í•¨ìˆ˜
        if value_serializer is None:
            value_serializer = lambda v: json.dumps(v, ensure_ascii=False).encode(
                "utf-8"
            )
        if key_serializer is None:
            key_serializer = lambda k: k.encode("utf-8") if k else None

        self.producer = None
        self.value_serializer = value_serializer
        self.key_serializer = key_serializer
        self.acks = acks
        self.retries = retries
        self.compression_type = compression_type
        self.linger_ms = linger_ms

    def connect(self) -> bool:
        """Producer ì—°ê²°"""
        try:
            producer_config = {
                "bootstrap_servers": self.bootstrap_servers,
                "value_serializer": self.value_serializer,
                "key_serializer": self.key_serializer,
                "acks": self.acks,
                "retries": self.retries,
                "request_timeout_ms": self.timeout * 1000,
            }

            # ê³ ê¸‰ ì„¤ì • ì¶”ê°€ (kafka_projectì˜ producer.properties ì°¸ê³ )
            if self.compression_type:
                producer_config["compression_type"] = self.compression_type
            if self.linger_ms:
                producer_config["linger_ms"] = self.linger_ms

            self.producer = KafkaProducer(**producer_config)
            self.logger.info(
                f"Kafka Producer connected to {self._get_servers_str()} "
                f"(compression={self.compression_type}, linger_ms={self.linger_ms})"
            )
            return True
        except Exception as e:
            self.logger.error(f"Failed to connect Kafka Producer: {e}")
            return False

    def send(
        self,
        topic: str,
        value: Any,
        key: Optional[str] = None,
        partition: Optional[int] = None,
    ) -> bool:
        """
        ë©”ì‹œì§€ ì „ì†¡

        Args:
            topic: í† í”½ ì´ë¦„
            value: ë©”ì‹œì§€ ê°’
            key: ë©”ì‹œì§€ í‚¤ (ì„ íƒ)
            partition: íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì„ íƒ)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        if not self.producer:
            if not self.connect():
                return False

        try:
            future = self.producer.send(
                topic,
                value=value,
                key=key,
                partition=partition,
            )
            record_metadata = future.get(timeout=self.timeout)
            self.logger.debug(
                f"Message sent to topic={topic}, "
                f"partition={record_metadata.partition}, "
                f"offset={record_metadata.offset}"
            )
            return True
        except KafkaError as e:
            self.logger.error(f"Kafka send error: {e}")
            return False
        except Exception as e:
            self.logger.error(f"Unexpected error sending message: {e}")
            return False

    def send_batch(self, topic: str, messages: List[Dict[str, Any]]) -> int:
        """
        ë°°ì¹˜ ë©”ì‹œì§€ ì „ì†¡

        Args:
            topic: í† í”½ ì´ë¦„
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ (ê° ë©”ì‹œì§€ëŠ” {"key": ..., "value": ...} í˜•ì‹)

        Returns:
            ì„±ê³µì ìœ¼ë¡œ ì „ì†¡ëœ ë©”ì‹œì§€ ìˆ˜
        """
        if not self.producer:
            if not self.connect():
                return 0

        success_count = 0
        for msg in messages:
            key = msg.get("key")
            value = msg.get("value")
            if self.send(topic, value, key):
                success_count += 1

        return success_count

    def send_with_callback(
        self,
        topic: str,
        value: Any,
        key: Optional[str] = None,
        partition: Optional[int] = None,
        callback=None,
    ):
        """
        Callbackì„ ì‚¬ìš©í•œ ë¹„ë™ê¸° ë©”ì‹œì§€ ì „ì†¡ (kafka_projectì˜ CallbackProducer ì°¸ê³ )

        Args:
            topic: í† í”½ ì´ë¦„
            value: ë©”ì‹œì§€ ê°’
            key: ë©”ì‹œì§€ í‚¤ (ì„ íƒ)
            partition: íŒŒí‹°ì…˜ ë²ˆí˜¸ (ì„ íƒ)
            callback: ì½œë°± í•¨ìˆ˜ (metadata, exception) -> None

        Returns:
            Future ê°ì²´ (ì„ íƒì )
        """
        if not self.producer:
            if not self.connect():
                return None

        try:
            future = self.producer.send(
                topic,
                value=value,
                key=key,
                partition=partition,
            )

            # Callbackì´ ì œê³µë˜ë©´ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ (kafka-pythonì˜ FutureëŠ” add_callback/add_errback ì‚¬ìš©)
            if callback:

                def on_success(record_metadata):
                    """ì„±ê³µ ì‹œ ì½œë°±"""
                    try:
                        callback(record_metadata, None)
                    except Exception as e:
                        self.logger.error(f"Error in callback: {e}")

                def on_error(exception):
                    """ì‹¤íŒ¨ ì‹œ ì½œë°±"""
                    try:
                        callback(None, exception)
                    except Exception as e:
                        self.logger.error(f"Error in error callback: {e}")

                future.add_callback(on_success)
                future.add_errback(on_error)
                return None  # Callbackì´ ìˆìœ¼ë©´ Futureë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ
            else:
                return future  # Callbackì´ ì—†ìœ¼ë©´ Future ë°˜í™˜
        except Exception as e:
            self.logger.error(f"Error sending message with callback: {e}")
            if callback:
                callback(None, e)
            return None

    def flush(self):
        """Producer ë²„í¼ í”ŒëŸ¬ì‹œ"""
        if self.producer:
            self.producer.flush()

    def close(self):
        """Producer ì¢…ë£Œ"""
        if self.producer:
            self.producer.close()
            self.logger.info("Kafka Producer closed")


class KafkaConsumerClient(KafkaClient):
    """Kafka Consumer í´ë¼ì´ì–¸íŠ¸"""

    def __init__(
        self,
        bootstrap_servers: List[str] = None,
        timeout: int = 10,
        group_id: str = "cointicker-consumer",
        auto_offset_reset: str = "earliest",
        enable_auto_commit: bool = True,
        value_deserializer=None,
        key_deserializer=None,
    ):
        """
        Kafka Consumer ì´ˆê¸°í™”

        Args:
            bootstrap_servers: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            group_id: Consumer Group ID
            auto_offset_reset: ì˜¤í”„ì…‹ ë¦¬ì…‹ ë°©ì‹ ("earliest", "latest")
            enable_auto_commit: ìë™ ì»¤ë°‹ ì—¬ë¶€
            value_deserializer: ê°’ ì—­ì§ë ¬í™” í•¨ìˆ˜
            key_deserializer: í‚¤ ì—­ì§ë ¬í™” í•¨ìˆ˜
        """
        super().__init__(bootstrap_servers, timeout)

        # ê¸°ë³¸ ì—­ì§ë ¬í™” í•¨ìˆ˜
        if value_deserializer is None:
            value_deserializer = lambda v: json.loads(v.decode("utf-8")) if v else None
        if key_deserializer is None:
            key_deserializer = lambda k: k.decode("utf-8") if k else None

        self.consumer = None
        self.group_id = group_id
        self.auto_offset_reset = auto_offset_reset
        self.enable_auto_commit = enable_auto_commit
        self.value_deserializer = value_deserializer
        self.key_deserializer = key_deserializer

    def connect(
        self, topics: List[str], max_retries: int = 3, retry_delay: float = 2.0
    ) -> bool:
        """
        Consumer ì—°ê²° (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

        Args:
            topics: êµ¬ë…í•  í† í”½ ë¦¬ìŠ¤íŠ¸ (ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ ì§€ì›, ì˜ˆ: "cointicker.raw.*")
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 3)
            retry_delay: ì¬ì‹œë„ ì§€ì—° ì‹œê°„ (ì´ˆ, ê¸°ë³¸: 2.0)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        from gui.core.retry_utils import execute_with_retry

        def _connect_attempt():
            return self._connect_internal(topics)

        try:
            return execute_with_retry(
                _connect_attempt,
                max_retries=max_retries,
                delay=retry_delay,
                backoff_factor=2.0,
                exceptions=(Exception,),
                on_retry=lambda attempt, e: self.logger.warning(
                    f"Kafka Consumer ì—°ê²° ì‹¤íŒ¨ (ì‹œë„ {attempt}/{max_retries}): {e}. ì¬ì‹œë„ ì¤‘..."
                ),
            )
        except Exception as e:
            self.logger.error(f"Kafka Consumer ì—°ê²° ìµœì¢… ì‹¤íŒ¨: {e}")
            return False

    def _connect_internal(self, topics: List[str]) -> bool:
        """
        Consumer ì—°ê²° ë‚´ë¶€ êµ¬í˜„ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)

        ì´ˆê¸° ìŠ¤ìº”ì€ AdminClientë¡œ ìˆ˜í–‰í•˜ì—¬ ì¦‰ì‹œ í† í”½ ëª©ë¡ì„ í™•ì¸í•˜ê³ ,
        ì‹¤ì œ êµ¬ë…ì€ Kafka ë„¤ì´í‹°ë¸Œ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ì—…ë°ì´íŠ¸ë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.

        Args:
            topics: êµ¬ë…í•  í† í”½ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì™€ì¼ë“œì¹´ë“œê°€ í¬í•¨ëœ í† í”½ì´ ìˆëŠ”ì§€ í™•ì¸
            pattern_topics = []
            direct_topics = []

            for topic in topics:
                if "*" in topic or "?" in topic:
                    # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì€ ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥ (Kafkaê°€ ìì²´ì ìœ¼ë¡œ ì²˜ë¦¬)
                    pattern_topics.append(topic)
                else:
                    direct_topics.append(topic)

            # íŒ¨í„´ì´ ìˆìœ¼ë©´ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ì‚¬ìš© (Rebalance ë°©ì§€)
            if pattern_topics:
                # ğŸ” 1ë‹¨ê³„: AdminClientë¡œ íŒ¨í„´ ë§¤ì¹­ í† í”½ ì°¾ê¸°
                admin_client = None
                matched_topics = []

                try:
                    admin_client = KafkaAdminClient(
                        bootstrap_servers=self.bootstrap_servers,
                        client_id=f"{self.group_id}-admin",
                    )
                    # ëª¨ë“  í† í”½ ëª©ë¡ ì¡°íšŒ
                    all_topics = admin_client.list_topics()

                    # ê° íŒ¨í„´ì— ëŒ€í•´ ë§¤ì¹­ë˜ëŠ” í† í”½ ì°¾ê¸°
                    for pattern_str in pattern_topics:
                        # ì™€ì¼ë“œì¹´ë“œ íŒ¨í„´ì„ ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜
                        # * -> .*, ? -> ., . -> \.
                        pattern_regex = (
                            pattern_str.replace(".", r"\.")
                            .replace("*", ".*")
                            .replace("?", ".")
                        )
                        compiled_pattern = re.compile(f"^{pattern_regex}$")

                        for topic in all_topics:
                            if compiled_pattern.match(topic):
                                if topic not in matched_topics:
                                    matched_topics.append(topic)

                    self.logger.info(
                        f"ğŸ” Pattern matching: {pattern_topics} -> "
                        f"{len(matched_topics)} topics found: {matched_topics}"
                    )

                except Exception as e:
                    self.logger.warning(
                        f"Failed to list topics: {e}. "
                        f"Falling back to pattern subscription..."
                    )
                finally:
                    if admin_client:
                        try:
                            admin_client.close()
                        except:
                            pass

                # ğŸš€ 2ë‹¨ê³„: Consumer ìƒì„±
                # consumer_timeout_msë¥¼ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì • (ë¬´í•œ ëŒ€ê¸° íš¨ê³¼, Python 3.14 í˜¸í™˜)
                self.consumer = KafkaConsumer(
                    bootstrap_servers=self.bootstrap_servers,
                    group_id=self.group_id,
                    auto_offset_reset=self.auto_offset_reset,
                    enable_auto_commit=self.enable_auto_commit,
                    value_deserializer=self.value_deserializer,
                    key_deserializer=self.key_deserializer,
                    consumer_timeout_ms=2147483647,  # ë¬´í•œ ëŒ€ê¸° (Python 3.14 í˜¸í™˜)
                )

                # ğŸ¯ 3ë‹¨ê³„: ë§¤ì¹­ëœ í† í”½ì´ ìˆìœ¼ë©´ ì§ì ‘ êµ¬ë… (Rebalance ë°©ì§€)
                # ì—†ìœ¼ë©´ íŒ¨í„´ êµ¬ë… (ìƒˆ í† í”½ ìë™ ê°ì§€)
                pattern_str = pattern_topics[0]

                if matched_topics:
                    # ì§ì ‘ í† í”½ êµ¬ë… (ë” ì•ˆì •ì , Rebalance ìµœì†Œí™”)
                    self.consumer.subscribe(topics=matched_topics)
                    self.logger.info(
                        f"âœ… Kafka Consumer subscribed to topics: {matched_topics}, "
                        f"group_id={self.group_id} (direct subscription to prevent rebalance)"
                    )
                else:
                    # íŒ¨í„´ êµ¬ë… (ìƒˆ í† í”½ ìë™ ê°ì§€)
                    # ì™€ì¼ë“œì¹´ë“œë¥¼ Java ì •ê·œì‹ìœ¼ë¡œ ë³€í™˜
                    kafka_pattern = f"^{pattern_str.replace('.', r'\\.').replace('*', '.*').replace('?', '.')}$"
                    try:
                        self.consumer.subscribe(pattern=kafka_pattern)
                        self.logger.info(
                            f"ğŸ¯ Kafka Consumer subscribed with pattern: {kafka_pattern}, "
                            f"group_id={self.group_id}, mode=AUTO-UPDATE"
                        )
                    except Exception as e:
                        self.logger.error(
                            f"Failed to subscribe with pattern {kafka_pattern}: {e}"
                        )
                        return False

                if len(pattern_topics) > 1:
                    self.logger.warning(
                        f"Multiple patterns provided, using first pattern: {pattern_str}"
                    )

                # ğŸ”„ 4ë‹¨ê³„: ì²« poll()ì„ ì‹¤í–‰í•˜ì—¬ í† í”½ í• ë‹¹ í™•ì •
                self.logger.info(
                    "Triggering initial poll() to finalize topic assignment..."
                )
                try:
                    # ì§§ì€ íƒ€ì„ì•„ì›ƒìœ¼ë¡œ poll í˜¸ì¶œ (í† í”½ í• ë‹¹ì„ ìœ„í•´)
                    self.consumer.poll(timeout_ms=5000)

                    # poll í›„ assignment í™•ì¸
                    assignment = self.consumer.assignment()
                    subscription = self.consumer.subscription()

                    if assignment:
                        assigned_topics = set(tp.topic for tp in assignment)
                        self.logger.info(
                            f"âœ… Kafka Consumer topics assigned after poll: {sorted(assigned_topics)}, "
                            f"partitions={len(assignment)}"
                        )
                    else:
                        # í• ë‹¹ëœ íŒŒí‹°ì…˜ì´ ì—†ì–´ë„ ìƒˆ í† í”½ì´ ìƒì„±ë˜ë©´ ìë™ êµ¬ë…ë¨
                        self.logger.warning(
                            f"âš ï¸ No partitions assigned yet. "
                            f"Topics will be assigned when matching topics have data."
                        )
                except Exception as poll_error:
                    self.logger.warning(
                        f"Initial poll failed (non-critical): {poll_error}"
                    )

                subscription = self.consumer.subscription()
                self.logger.info(
                    f"âœ… Kafka Consumer subscription confirmed: {subscription}"
                )
            elif direct_topics:
                # ì§ì ‘ í† í”½ êµ¬ë…
                # consumer_timeout_msë¥¼ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì„¤ì • (ë¬´í•œ ëŒ€ê¸° íš¨ê³¼, Python 3.14 í˜¸í™˜)
                # 2147483647 = 2^31 - 1 (ì•½ 24ì¼)
                self.consumer = KafkaConsumer(
                    *direct_topics,
                    bootstrap_servers=self.bootstrap_servers,
                    group_id=self.group_id,
                    auto_offset_reset=self.auto_offset_reset,
                    enable_auto_commit=self.enable_auto_commit,
                    value_deserializer=self.value_deserializer,
                    key_deserializer=self.key_deserializer,
                    consumer_timeout_ms=2147483647,  # ë¬´í•œ ëŒ€ê¸° (Python 3.14 í˜¸í™˜)
                )
                # êµ¬ë… í™•ì¸
                subscription = self.consumer.subscription()
                self.logger.info(
                    f"Kafka Consumer connected to {self._get_servers_str()}, "
                    f"topics={direct_topics}, group_id={self.group_id}, subscription={subscription}"
                )
            else:
                # í† í”½ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
                self.logger.error("No topics or patterns provided")
                return False

            return True
        except Exception as e:
            self.logger.error(f"Failed to connect Kafka Consumer: {e}", exc_info=True)
            raise  # ì¬ì‹œë„ ë¡œì§ì„ ìœ„í•´ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´

    def consume(self, callback=None, max_messages: Optional[int] = None):
        """
        ë©”ì‹œì§€ ì†Œë¹„

        Args:
            callback: ë©”ì‹œì§€ ì²˜ë¦¬ ì½œë°± í•¨ìˆ˜ (message -> None)
            max_messages: ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜ (Noneì´ë©´ ë¬´ì œí•œ)
        """
        if not self.consumer:
            self.logger.error("Consumer not connected")
            return

        message_count = 0
        poll_timeout_ms = 1000  # 1ì´ˆ íƒ€ì„ì•„ì›ƒ
        no_assignment_warnings = 0
        max_no_assignment_warnings = 10  # 10ë²ˆ ê²½ê³  í›„ ë¡œê·¸ ë ˆë²¨ ë³€ê²½

        try:
            self.logger.info("Starting message consumption loop...")

            # íŒŒí‹°ì…˜ í• ë‹¹ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ, Rebalance ë°©ì§€)
            assignment_wait_time = 0
            max_assignment_wait = 10
            while assignment_wait_time < max_assignment_wait:
                assignment = self.consumer.assignment()
                if assignment:
                    assigned_topics = set(tp.topic for tp in assignment)
                    self.logger.info(
                        f"âœ… Partitions assigned: {sorted(assigned_topics)}, "
                        f"partitions={len(assignment)}"
                    )
                    break
                else:
                    # íŒŒí‹°ì…˜ í• ë‹¹ ëŒ€ê¸°
                    self.consumer.poll(timeout_ms=1000)
                    assignment_wait_time += 1
                    if assignment_wait_time % 5 == 0:
                        self.logger.debug(
                            f"Waiting for partition assignment... ({assignment_wait_time}s/{max_assignment_wait}s)"
                        )

            if not self.consumer.assignment():
                self.logger.warning(
                    f"âš ï¸ No partitions assigned after {max_assignment_wait}s. "
                    f"Consumer will continue polling for new topics..."
                )

            # ë©”ì‹œì§€ ì†Œë¹„ ë£¨í”„
            while True:
                try:
                    # poll()ì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
                    message_batch = self.consumer.poll(timeout_ms=poll_timeout_ms)

                    if not message_batch:
                        # ë©”ì‹œì§€ê°€ ì—†ì„ ë•Œ íŒŒí‹°ì…˜ í• ë‹¹ ìƒíƒœ í™•ì¸
                        assignment = self.consumer.assignment()
                        if not assignment:
                            no_assignment_warnings += 1
                            if no_assignment_warnings <= max_no_assignment_warnings:
                                self.logger.debug(
                                    f"No messages and no partitions assigned yet. "
                                    f"Waiting for topic assignment... ({no_assignment_warnings}/{max_no_assignment_warnings})"
                                )
                        continue

                    # íŒŒí‹°ì…˜ì´ í• ë‹¹ë˜ì—ˆìœ¼ë©´ ê²½ê³  ì¹´ìš´í„° ë¦¬ì…‹
                    if assignment:
                        no_assignment_warnings = 0

                    # ë°°ì¹˜ì˜ ê° ë©”ì‹œì§€ ì²˜ë¦¬
                    for topic_partition, messages in message_batch.items():
                        for message in messages:
                            if callback:
                                callback(message)
                            else:
                                self.logger.info(
                                    f"Received message: topic={message.topic}, "
                                    f"partition={message.partition}, "
                                    f"offset={message.offset}, "
                                    f"key={message.key}, "
                                    f"value={message.value}"
                                )

                            message_count += 1
                            if max_messages and message_count >= max_messages:
                                self.logger.info(
                                    f"Reached max_messages limit: {max_messages}"
                                )
                                return

                except Exception as poll_error:
                    self.logger.error(f"Error during poll: {poll_error}", exc_info=True)
                    # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ë£¨í”„ ê³„ì† ì§„í–‰
                    continue

        except KeyboardInterrupt:
            self.logger.info("Consumer interrupted by user")
        except Exception as e:
            self.logger.error(f"Error consuming messages: {e}", exc_info=True)
        finally:
            self.logger.info(
                f"Message consumption loop ended. Total messages: {message_count}"
            )

    def get_consumer_groups(self) -> Dict[str, Any]:
        """
        Consumer Groups ìƒíƒœ ì¡°íšŒ

        Returns:
            Consumer Groups ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        if not self.consumer:
            return {"error": "Consumer not connected"}

        try:
            # Consumerì˜ ê·¸ë£¹ IDì™€ êµ¬ë… ì •ë³´
            subscription = self.consumer.subscription()
            assignment = self.consumer.assignment()

            return {
                "group_id": self.group_id,
                "subscription": list(subscription) if subscription else [],
                "assignment": (
                    [
                        {
                            "topic": tp.topic,
                            "partition": tp.partition,
                        }
                        for tp in assignment
                    ]
                    if assignment
                    else []
                ),
                "num_partitions": len(assignment) if assignment else 0,
            }
        except Exception as e:
            self.logger.error(f"Failed to get consumer groups: {e}")
            return {"error": str(e)}

    def close(self):
        """Consumer ì¢…ë£Œ"""
        if self.consumer:
            self.consumer.close()
            self.logger.info("Kafka Consumer closed")
