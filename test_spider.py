#!/usr/bin/env python3
"""
response.follow() ë°©ì‹ì„ ì‚¬ìš©í•œ Scrapy ìŠ¤íŒŒì´ë” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import scrapy
from scrapy.crawler import CrawlerProcess
import json
import os


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = ["http://quotes.toscrape.com"]

    def __init__(self):
        self.collected_items = []
        self.page_count = 0

    def parse(self, response):
        self.page_count += 1
        print(f"\nğŸ“„ ëª…ì–¸ í˜ì´ì§€ {self.page_count} ì²˜ë¦¬ ì¤‘: {response.url}")
        print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status}")

        quotes = response.css("div.quote")
        print(f"ğŸ” ë°œê²¬ëœ ëª…ì–¸ ìˆ˜: {len(quotes)}ê°œ")

        for i, q in enumerate(quotes, 1):
            item = {
                "text": q.css("span.text::text").get(),
                "author": q.css("small.author::text").get(),
                "tags": q.css("div.tags a.tag::text").getall(),
            }
            self.collected_items.append(item)
            print(f"  ğŸ’¬ ëª…ì–¸ {i}: {item['author']} - {item['text'][:50]}...")
            yield item

        # response.follow() ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•œ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜
        pager_links = response.css("ul.pager a")
        print(f"ğŸ”— ë°œê²¬ëœ í˜ì´ì§€ ë§í¬: {len(pager_links)}ê°œ")
        for a in pager_links:
            link_text = a.css("::text").get()
            link_href = a.css("::attr(href)").get()
            print(f"  â¡ï¸ ë§í¬ ë”°ë¼ê°€ê¸°: '{link_text}' -> {link_href}")
            yield response.follow(a, callback=self.parse)

    def closed(self, reason):
        print(f"\nâœ… ëª…ì–¸ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ëª…ì–¸: {len(self.collected_items)}ê°œ")
        print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {self.page_count}ê°œ")
        print(f"ğŸ”š ì¢…ë£Œ ì´ìœ : {reason}")


class AuthorSpider(scrapy.Spider):
    name = "author"
    start_urls = ["http://quotes.toscrape.com/"]

    def __init__(self):
        self.collected_authors = []
        self.page_count = 0

    def parse(self, response):
        self.page_count += 1
        print(f"\nğŸ‘¤ ì‘ê°€ í˜ì´ì§€ {self.page_count} ì²˜ë¦¬ ì¤‘: {response.url}")
        print(f"ğŸ“Š ì‘ë‹µ ìƒíƒœ: {response.status}")

        # ì‘ê°€ ë§í¬ ì°¾ê¸° ë° ë”°ë¼ê°€ê¸°
        author_links = response.css(".author + a")
        print(f"ğŸ” ë°œê²¬ëœ ì‘ê°€ ë§í¬: {len(author_links)}ê°œ")
        for author_link in author_links:
            author_name = author_link.css("::text").get()
            print(f"  ğŸ‘¤ ì‘ê°€ ë§í¬ ë”°ë¼ê°€ê¸°: {author_name}")
            yield response.follow(author_link, callback=self.parse_author)

        # í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜
        page_links = response.css("li.next a")
        for page_link in page_links:
            print(f"  â¡ï¸ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™")
            yield response.follow(page_link, callback=self.parse)

    def parse_author(self, response):
        print(f"\nğŸ“ ì‘ê°€ ìƒì„¸ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘: {response.url}")

        author_data = {
            "name": response.css(".author-title::text").get(),
            "birthdate": response.css(".author-born-date::text").get(),
            "birthplace": response.css(".author-born-location::text").get(),
            "bio": response.css(".author-description::text").get(),
        }

        # None ê°’ë“¤ì„ ì •ë¦¬
        for key, value in author_data.items():
            if value:
                author_data[key] = value.strip()

        self.collected_authors.append(author_data)
        print(f"  âœ¨ ì‘ê°€ ì •ë³´ ìˆ˜ì§‘: {author_data['name']}")
        print(f"    ğŸ“… ìƒë…„ì›”ì¼: {author_data['birthdate']}")
        print(f"    ğŸ“ ì¶œìƒì§€: {author_data['birthplace']}")

        yield author_data

    def closed(self, reason):
        print(f"\nâœ… ì‘ê°€ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ‘¤ ì´ ìˆ˜ì§‘ëœ ì‘ê°€: {len(self.collected_authors)}ëª…")
        print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€: {self.page_count}ê°œ")
        print(f"ğŸ”š ì¢…ë£Œ ì´ìœ : {reason}")


def run_quotes_spider():
    print("\nğŸš€ ëª…ì–¸ ìŠ¤íŒŒì´ë” ì‹œì‘!")
    process = CrawlerProcess(
        {
            "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "ROBOTSTXT_OBEY": False,
            "DOWNLOAD_DELAY": 0.5,
            "FEEDS": {
                "quotes_follow_output.json": {
                    "format": "json",
                    "encoding": "utf8",
                    "store_empty": False,
                    "overwrite": True,
                },
            },
        }
    )
    process.crawl(QuotesSpider)
    process.start()


def run_author_spider():
    print("\nğŸš€ ì‘ê°€ ìŠ¤íŒŒì´ë” ì‹œì‘!")
    process = CrawlerProcess(
        {
            "USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "ROBOTSTXT_OBEY": False,
            "DOWNLOAD_DELAY": 0.5,
            "FEEDS": {
                "authors_output.json": {
                    "format": "json",
                    "encoding": "utf8",
                    "store_empty": False,
                    "overwrite": True,
                },
            },
        }
    )
    process.crawl(AuthorSpider)
    process.start()


if __name__ == "__main__":
    print("ğŸš€ response.follow() ë°©ì‹ Scrapy ìŠ¤íŒŒì´ë” í…ŒìŠ¤íŠ¸!")
    print("ğŸ¯ ëŒ€ìƒ ì‚¬ì´íŠ¸: http://quotes.toscrape.com")
    print("\nì–´ë–¤ ìŠ¤íŒŒì´ë”ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("1. ëª…ì–¸ ìŠ¤íŒŒì´ë” (QuotesSpider)")
    print("2. ì‘ê°€ ìŠ¤íŒŒì´ë” (AuthorSpider)")

    choice = input("\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()

    if choice == "1":
        run_quotes_spider()
        print("\nğŸ‰ ëª…ì–¸ í¬ë¡¤ë§ ì™„ë£Œ! quotes_follow_output.json íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    elif choice == "2":
        run_author_spider()
        print("\nğŸ‰ ì‘ê°€ í¬ë¡¤ë§ ì™„ë£Œ! authors_output.json íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
