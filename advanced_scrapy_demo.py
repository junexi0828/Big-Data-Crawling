#!/usr/bin/env python3
"""
ê³ ê¸‰ Scrapy ê¸°ëŠ¥ ë°ëª¨: response.follow_all, Spider Arguments, Complex Spiders
"""
import scrapy
from scrapy.crawler import CrawlerProcess
import subprocess
import os


def demo_spider_arguments():
    """Spider Arguments ë°ëª¨"""
    print("ğŸ¯ Spider Arguments ë°ëª¨")
    print("=" * 50)

    # 1. íƒœê·¸ ì—†ì´ ì‹¤í–‰
    print("\n1ï¸âƒ£ ê¸°ë³¸ ì‹¤í–‰ (ëª¨ë“  ëª…ì–¸)")
    cmd = "cd /Users/juns/bigdata && source scrapy_env/bin/activate && cd tutorial && scrapy crawl quotes -s DOWNLOAD_DELAY=1 -s LOG_LEVEL=INFO -L INFO -o quotes_all.json"
    print(f"ì‹¤í–‰ ëª…ë ¹ì–´: {cmd.split('&&')[-1].strip()}")

    # 2. humor íƒœê·¸ë¡œ í•„í„°ë§
    print("\n2ï¸âƒ£ íƒœê·¸ í•„í„°ë§ ì‹¤í–‰ (humor íƒœê·¸ë§Œ)")
    cmd2 = "cd /Users/juns/bigdata && source scrapy_env/bin/activate && cd tutorial && scrapy crawl quotes -a tag=humor -s DOWNLOAD_DELAY=1 -s LOG_LEVEL=INFO -L INFO -o quotes_humor.json"
    print(f"ì‹¤í–‰ ëª…ë ¹ì–´: {cmd2.split('&&')[-1].strip()}")

    # 3. love íƒœê·¸ë¡œ í•„í„°ë§
    print("\n3ï¸âƒ£ íƒœê·¸ í•„í„°ë§ ì‹¤í–‰ (love íƒœê·¸ë§Œ)")
    cmd3 = "cd /Users/juns/bigdata && source scrapy_env/bin/activate && cd tutorial && scrapy crawl quotes -a tag=love -s DOWNLOAD_DELAY=1 -s LOG_LEVEL=INFO -L INFO -o quotes_love.json"
    print(f"ì‹¤í–‰ ëª…ë ¹ì–´: {cmd3.split('&&')[-1].strip()}")

    print("\nğŸ“ ì‹¤í–‰ ì˜ˆì‹œ:")
    print("â€¢ scrapy crawl quotes                    # ëª¨ë“  ëª…ì–¸")
    print("â€¢ scrapy crawl quotes -a tag=humor       # humor íƒœê·¸ë§Œ")
    print("â€¢ scrapy crawl quotes -a tag=love        # love íƒœê·¸ë§Œ")
    print("â€¢ scrapy crawl quotes -a tag=inspirational # inspirational íƒœê·¸ë§Œ")


def demo_response_follow_all():
    """response.follow_all ë°ëª¨"""
    print("\nğŸ”— response.follow_all ê¸°ëŠ¥ ì„¤ëª…")
    print("=" * 50)

    print(
        """
ğŸ“š ê¸°ë³¸ ë°©ì‹ vs response.follow_all ë¹„êµ:

ğŸ”¸ ê¸°ë³¸ ë°©ì‹ (ë°˜ë³µë¬¸ ì‚¬ìš©):
    for author_link in response.css('.author + a'):
        yield response.follow(author_link, callback=self.parse_author)

    for page_link in response.css('li.next a'):
        yield response.follow(page_link, callback=self.parse)

ğŸ”¸ response.follow_all ë°©ì‹ (ê°„ê²°í•¨):
    yield from response.follow_all(response.css('.author + a'), self.parse_author)
    yield from response.follow_all(response.css('li.next a'), callback=self.parse)

âœ… ì¥ì :
â€¢ ì½”ë“œê°€ ë” ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ì›€
â€¢ ì—¬ëŸ¬ ë§í¬ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
â€¢ ìë™ìœ¼ë¡œ ë¹„ë™ê¸° ìŠ¤ì¼€ì¤„ë§ë¨
â€¢ ìƒëŒ€ URL ìë™ ì²˜ë¦¬
"""
    )


def demo_complex_spider():
    """ComplexQuotesSpider ë°ëª¨"""
    print("\nğŸ—ï¸ ComplexQuotesSpider íŠ¹ì§•")
    print("=" * 50)

    print(
        """
ğŸ¯ ComplexQuotesSpiderì˜ íŠ¹ì§•:

1ï¸âƒ£ ë™ì‹œ ë°ì´í„° ìˆ˜ì§‘:
   â€¢ ëª…ì–¸ ì •ë³´ (text, author, tags)
   â€¢ ì‘ê°€ ìƒì„¸ ì •ë³´ (name, birthdate, birthplace, bio)

2ï¸âƒ£ ë¹„ë™ê¸° ì²˜ë¦¬:
   â€¢ ëª…ì–¸ê³¼ ì‘ê°€ í˜ì´ì§€ê°€ ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ë¨
   â€¢ ê°œë³„ ì‘ê°€ í˜ì´ì§€ê°€ í•´ë‹¹ ëª…ì–¸ê³¼ ë™ê¸°í™”ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

3ï¸âƒ£ response.follow_all í™œìš©:
   â€¢ ëª¨ë“  ì‘ê°€ ë§í¬ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
   â€¢ ëª¨ë“  í˜ì´ì§€ë„¤ì´ì…˜ì„ í•œ ë²ˆì— ì²˜ë¦¬

4ï¸âƒ£ ì‹¤í–‰ ëª…ë ¹ì–´:
   scrapy crawl complex_quotes -o complex_output.json
"""
    )


def show_scrapy_shell_examples():
    """Scrapy Shell ì‚¬ìš© ì˜ˆì‹œ"""
    print("\nğŸš Scrapy Shell í™œìš©ë²•")
    print("=" * 50)

    print(
        """
ğŸ” CSS ì„ íƒì í…ŒìŠ¤íŠ¸:

1ï¸âƒ£ Shell ì‹œì‘:
   scrapy shell "http://quotes.toscrape.com"

2ï¸âƒ£ ì‘ê°€ ë§í¬ í™•ì¸:
   response.css('.author + a::attr(href)').get()
   response.css('.author + a::attr(href)').getall()

3ï¸âƒ£ ì—¬ëŸ¬ ìš”ì†Œ ì„ íƒ:
   anchors = response.css('.author + a')
   [a.css('::attr(href)').get() for a in anchors]

4ï¸âƒ£ response.follow_all í…ŒìŠ¤íŠ¸:
   requests = list(response.follow_all(response.css('.author + a')))
   len(requests)  # ìƒì„±ëœ ìš”ì²­ ê°œìˆ˜ í™•ì¸

5ï¸âƒ£ ë³µí•© ì„ íƒì:
   response.css('li.next a::attr(href)').get()
   response.css('div.quote .author::text').getall()
"""
    )


def demo_dupefilter_class():
    """DUPEFILTER_CLASS ì„¤ëª…"""
    print("\nğŸ”„ DUPEFILTER_CLASS ë™ì‘ ì›ë¦¬")
    print("=" * 50)

    print(
        """
ğŸ›¡ï¸ Scrapyì˜ ì¤‘ë³µ í•„í„°ë§:

ğŸ“Œ ê¸°ë³¸ ë™ì‘:
â€¢ ScrapyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ ë°©ë¬¸í•œ URLì— ëŒ€í•œ ìš”ì²­ì„ í•„í„°ë§
â€¢ ë™ì¼í•œ URLë¡œ ì—¬ëŸ¬ ë²ˆ ìš”ì²­í•´ë„ í•œ ë²ˆë§Œ ì²˜ë¦¬ë¨

ğŸ“Œ ì„¤ì • ì˜µì…˜:
â€¢ DUPEFILTER_CLASS = 'scrapy.dupefilters.RFPDupeFilter'  (ê¸°ë³¸ê°’)
â€¢ DUPEFILTER_CLASS = 'scrapy.dupefilters.BaseDupeFilter'  (ë¹„í™œì„±í™”)

ğŸ“Œ ì‹¤ì œ ì˜ˆì‹œ:
ê°™ì€ ì‘ê°€ í˜ì´ì§€ê°€ ì—¬ëŸ¬ ëª…ì–¸ì—ì„œ ë§í¬ë˜ì–´ë„ í•œ ë²ˆë§Œ í¬ë¡¤ë§ë¨

ğŸ“Œ ë””ë²„ê¹…:
â€¢ DUPEFILTER_DEBUG = True ì„¤ì •ìœ¼ë¡œ ë¡œê·¸ í™•ì¸ ê°€ëŠ¥
â€¢ ë¡œê·¸ì—ì„œ "Filtered duplicate request" ë©”ì‹œì§€ í™•ì¸
"""
    )


if __name__ == "__main__":
    print("ğŸš€ ê³ ê¸‰ Scrapy ê¸°ëŠ¥ ë°ëª¨")
    print("ğŸ¯ response.follow_all, Spider Arguments, Complex Spiders")
    print()

    # ëª¨ë“  ë°ëª¨ ì‹¤í–‰
    demo_spider_arguments()
    demo_response_follow_all()
    demo_complex_spider()
    show_scrapy_shell_examples()
    demo_dupefilter_class()

    print("\nğŸ‰ ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ“š ìœ„ì˜ ëª…ë ¹ì–´ë“¤ì„ ì§ì ‘ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
